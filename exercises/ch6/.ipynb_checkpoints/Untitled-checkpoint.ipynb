{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b279cb3-a4cc-4a41-8c9c-7dfe0802cd3a",
   "metadata": {},
   "source": [
    "# 1. Even if there is no need to deploy trained models to a different device, what are the practical benefits of storing model parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440b44d0-adf3-4e0b-aef0-6cdcc3ecb9d8",
   "metadata": {},
   "source": [
    "Storing model parameters has several practical benefits even if there is no immediate need to deploy trained models to different devices. Here are some reasons why storing model parameters is valuable:\n",
    "\n",
    "1. **Faster Model Initialization:** When you load pretrained model parameters, you avoid the need to retrain the model from scratch. This can significantly speed up the process of initializing your model for further training or evaluation.\n",
    "\n",
    "2. **Experiment Reproducibility:** Storing model parameters allows you to reproduce experimental results consistently. Researchers and practitioners often share pretrained models along with their parameters to ensure that others can replicate their findings.\n",
    "\n",
    "3. **Resource Savings:** Training deep learning models can be computationally intensive and time-consuming. By storing pretrained parameters, you save computational resources and time by not needing to retrain the model every time you want to use it.\n",
    "\n",
    "4. **Fine-Tuning:** If you have a pretrained model, you can fine-tune it on a specific task by adjusting a smaller set of parameters while keeping the bulk of the model parameters fixed. This can help you achieve better performance with limited training data.\n",
    "\n",
    "5. **Model Versioning:** Storing model parameters allows you to version your models, making it easier to keep track of changes and improvements over time. This is important for model maintenance and updates.\n",
    "\n",
    "6. **Knowledge Transfer:** Model parameters can carry learned knowledge from one task to another, even if the tasks are related but not identical. Transfer learning leverages pretrained models to improve performance on new tasks.\n",
    "\n",
    "7. **Sharing and Collaboration:** Sharing pretrained models, along with their parameters, encourages collaboration and knowledge exchange among researchers and practitioners.\n",
    "\n",
    "8. **Efficient Storage:** Model parameters are typically much smaller in size compared to storing entire model architectures. This makes it feasible to store multiple versions of models for different purposes without consuming excessive storage space.\n",
    "\n",
    "9. **Offline Inference:** If you need to make predictions on new data without access to the original training infrastructure, storing model parameters allows you to perform inference offline.\n",
    "\n",
    "Overall, storing model parameters is a common practice in deep learning due to the efficiency, reproducibility, and versatility it provides, even if immediate deployment to different devices is not a requirement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf25266-b03c-42f8-85a7-90db1ac06591",
   "metadata": {},
   "source": [
    "# 2. Assume that we want to reuse only parts of a network to be incorporated into a network having a different architecture. How would you go about using, say the first two layers from a previous network in a new network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16b229ef-0986-4d46-975a-e6160a1521c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import warnings\n",
    "sys.path.append('/home/jovyan/work/d2l_solutions/notebooks/exercises/d2l_utils/')\n",
    "import d2l\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class ReusedMLP(d2l.MulMLP):\n",
    "     def __init__(self, num_outputs, num_hiddens, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "hparams = {'num_hiddens':[256,128,64,32],'num_outputs':10,'lr':0.1}\n",
    "model = d2l.MulMLP(**hparams)\n",
    "X = torch.randn(size=(2, 20))\n",
    "Y = model(X)\n",
    "torch.save(model.state_dict(), 'mlp.params')\n",
    "clone = d2l.MulMLP(**hparams)\n",
    "clone.load_state_dict(torch.load('mlp.params'))\n",
    "reused_layer = clone.net[:2]\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b38f71a-85bb-4432-8240-4bbea26dec31",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85a76246-6fdf-4157-acde-ff893bed08b3",
   "metadata": {},
   "source": [
    "# 3. How would you go about saving the network architecture and parameters? What restrictions would you impose on the architecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bec915-bab3-49f5-9770-42dc572bec70",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "765e9d78-bf68-4805-b26b-0f08b29e85ca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33a60d09-9ed1-4df1-8a92-f73a05e6aa7c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:d2l]",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
