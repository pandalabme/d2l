{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe04830b-c762-4003-933e-230b58481cfb",
   "metadata": {},
   "source": [
    "# 1. What happens if you specify the input dimensions to the first layer but not to subsequent layers? Do you get immediate initialization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e04b633f-5b68-42c5-a2d8-c63563eca891",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0042, -0.0041,  0.0132,  ...,  0.0145, -0.0068, -0.0133],\n",
       "        [-0.0151,  0.0175,  0.0106,  ...,  0.0133, -0.0010,  0.0254],\n",
       "        [-0.0012,  0.0225,  0.0020,  ..., -0.0270,  0.0126, -0.0066],\n",
       "        ...,\n",
       "        [-0.0037,  0.0327,  0.0219,  ..., -0.0305,  0.0265, -0.0318],\n",
       "        [-0.0354,  0.0245,  0.0181,  ...,  0.0314, -0.0357, -0.0207],\n",
       "        [-0.0222,  0.0320,  0.0309,  ...,  0.0284,  0.0331,  0.0284]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import warnings\n",
    "sys.path.append('/home/jovyan/work/d2l_solutions/notebooks/exercises/d2l_utils/')\n",
    "import d2l\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "net = nn.Sequential(nn.Linear(28*28,256), nn.ReLU(), nn.LazyLinear(10))\n",
    "net[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a89145-a6cc-409a-afd0-007df34a9618",
   "metadata": {},
   "source": [
    "# 2. What happens if you specify mismatching dimensions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fbe72e8-f9d8-4984-94d3-fc36052ea9f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x256 and 128x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m net \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m28\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m256\u001b[39m), nn\u001b[38;5;241m.\u001b[39mReLU(), nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m28\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m28\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x256 and 128x10)"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(28*28,256), nn.ReLU(), nn.Linear(128,10))\n",
    "x = torch.randn(1,28*28)\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79a171a-17ea-4601-97d3-c1447260f6ef",
   "metadata": {},
   "source": [
    "# 3. What would you need to do if you have input of varying dimensionality? Hint: look at the parameter tying."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8bd2db-0fc1-4e87-a21f-bdb00aa3a883",
   "metadata": {},
   "source": [
    "The `TyingParamTeterMLP` class implements a linear layer with shared weights across inputs of varying dimensions. The key idea is that the same set of weights is used for different input sizes, allowing parameter tying to create shared representations.\n",
    "\n",
    "Please note that while this example demonstrates parameter tying, it might not be the most practical approach for handling varying input dimensionality in complex networks. More often, architectural techniques like pooling, padding, and dynamic computation are used to handle varying input sizes effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b6041806-d4ac-4c74-8251-6ffe3dc92e3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TyingParamTeterMLP(d2l.Classifier):\n",
    "    def __init__(self, num_outputs, num_hiddens, lr, dropouts, k):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        layers = [] # nn.Flatten()\n",
    "        self.flat = nn.Flatten()\n",
    "        self.shared = nn.LazyLinear(num_hiddens[0])\n",
    "        for i in range(1,len(num_hiddens)):\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropouts[i]))\n",
    "        layers.append(nn.LazyLinear(num_outputs))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.flat(X)\n",
    "        r = X.shape[-1] % self.k\n",
    "        if r != 0:\n",
    "            pad = torch.zeros(list(X.shape[:-1])+[r])\n",
    "            X = torch.cat((X, pad), dim=-1)\n",
    "        n = X.shape[-1] // self.k\n",
    "        chunks = torch.chunk(X, n, dim=-1)\n",
    "        tying_X = self.shared(chunks[0])\n",
    "        for i in range(1,len(chunks)):\n",
    "            tying_X += self.shared(chunks[i])\n",
    "        return self.net(tying_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d95710a3-5ef6-4a38-ab2e-c976656ba69f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2834, -0.6606, -1.0245,  0.9141, -0.8389, -0.3940, -0.7341, -1.2385,\n",
      "          0.5124,  1.2274]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 5.5602, -4.5217, -6.7503,  4.9315, -4.9578, -5.1872, -5.0341, -8.8645,\n",
      "          0.9688,  6.2508]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hparams = {'num_outputs':10,'num_hiddens':[8,4,2],\n",
    "           'dropouts':[0]*3,'lr':0.1,'k':16}\n",
    "model = TyingParameterMLP(**hparams)\n",
    "x1 = torch.randn(1,28*28)\n",
    "x2 = torch.randn(1,32*32)\n",
    "print(model(x1))\n",
    "print(model(x2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:d2l]",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
