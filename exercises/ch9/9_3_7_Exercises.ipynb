{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9cd6bf1-784d-4309-b487-9008d07442fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import warnings\n",
    "import re\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append('/home/jovyan/work/d2l_solutions/notebooks/exercises/d2l_utils/')\n",
    "import d2l\n",
    "from torchsummary import summary\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class TimeMachine(d2l.DataModule): #@save\n",
    "    \"\"\"The Time Machine dataset.\"\"\"\n",
    "    def _download(self):\n",
    "        fname = d2l.download(d2l.DATA_URL + 'timemachine.txt', self.root,\n",
    "                             '090b5e7e70c295757f55df93cb0a180b9691891a')\n",
    "        with open(fname) as f:\n",
    "            return f.read()\n",
    "\n",
    "    def _preprocess(self, text):\n",
    "        return re.sub('[^A-Za-z]+', ' ', text).lower()\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        return list(text)\n",
    "    \n",
    "    def build(self, raw_text, vocab=None):\n",
    "        tokens = self._tokenize(self._preprocess(raw_text))\n",
    "        if vocab is None: vocab = Vocab(tokens)\n",
    "        corpus = [vocab[token] for token in tokens]\n",
    "        return corpus, vocab\n",
    "    \n",
    "class Vocab:\n",
    "    \"\"\"Vocabulary for text.\"\"\"\n",
    "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
    "        # Flatten a 2D list if needed\n",
    "        if tokens and isinstance(tokens[0], list):\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "        # Count token frequencies\n",
    "        counter = collections.Counter(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                  reverse=True)\n",
    "        # The list of unique tokens\n",
    "        self.idx_to_token = list(sorted(set(['<unk>'] + reserved_tokens + [\n",
    "            token for token, freq in self.token_freqs if freq >= min_freq])))\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
    "            return [self.idx_to_token[int(index)] for index in indices]\n",
    "        return self.idx_to_token[indices]\n",
    "\n",
    "    def unk(self):  # Index for the unknown token\n",
    "        return self.token_to_idx['<unk>']\n",
    "    \n",
    "class FreqData(d2l.SyntheticRegressionData):\n",
    "    def __init__(self, X, y, num_train=1000, batch_size=32):\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "def estimate_exp(words):\n",
    "    vocab = Vocab(words)\n",
    "    freqs = torch.log(torch.tensor([freq for w, freq in vocab.token_freqs])).reshape(-1,1)\n",
    "    rank = torch.log(torch.range(1, len(freqs))).reshape(-1,1)\n",
    "    data = FreqData(X=rank, y=freqs)\n",
    "    model = d2l.LinearRegression(1)\n",
    "    trainer = d2l.Trainer(max_epochs=5)\n",
    "    trainer.fit(model, data)\n",
    "    w = model.net.weight.item()\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.scatter(rank.detach().numpy(), freqs.detach().numpy(), label=\"Data\")\n",
    "    plt.plot(rank.detach().numpy(), model(rank).detach().numpy(), 'r-', label=\"Fitted Power Law\")\n",
    "    plt.xlabel(\"Rank\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"Estimated Exponent (s): {w:.2f}\")\n",
    "    plt.legend()\n",
    "    # plt.grid(True)\n",
    "    plt.show()\n",
    "    print(f\"Estimated Exponent (s): {w:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "198db7aa-dc62-4208-950d-809561b42aa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = TimeMachine()\n",
    "raw_text = data._download()\n",
    "text = data._preprocess(raw_text)\n",
    "words = text.split()\n",
    "bigram_tokens = ['--'.join(pair) for pair in zip(words[:-1], words[1:])]\n",
    "trigram_tokens = ['--'.join(pair) for pair in zip(words[:-2], words[1:], words[2:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f595f0a-808d-4f0d-917d-62f2c9a6be94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# estimate_exp(trigram_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:d2l]",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
