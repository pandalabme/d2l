{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b3615d6-bbf4-4eff-a05d-a25c6af26eae",
   "metadata": {},
   "source": [
    "# 1. Does the implemented language model predict the next token based on all the past tokens up to the very first token in The Time Machine?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034b8307-259d-4b16-b24a-daea4fb75f75",
   "metadata": {},
   "source": [
    "The prediction of the next token is influenced by a fixed-length context window, which is controled by the hyperparameter of prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f88eb3-4f7e-45ef-bec1-aeba1327eb90",
   "metadata": {},
   "source": [
    "# 2. Which hyperparameter controls the length of history used for prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e308b3d-a558-4b9c-9985-d9406be85bdc",
   "metadata": {},
   "source": [
    "prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec87cea-09d2-4df7-b0d5-6b7d522c5419",
   "metadata": {},
   "source": [
    "# 3. Show that one-hot encoding is equivalent to picking a different embedding for each object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4fbada-41df-4e10-b480-20601be4a019",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Create a tensor of indices representing the categories A, B, C, and D\n",
    "indices = torch.tensor([0, 1, 2, 3])\n",
    "\n",
    "# Create a one-hot encoded tensor from the indices\n",
    "one_hot = F.one_hot(indices)\n",
    "\n",
    "# Create an embedding matrix of size 4 x 3\n",
    "embedding = torch.tensor([[0.2, -0.5, 0.7],\n",
    "                          [-0.1, 0.3, -0.6],\n",
    "                          [0.4, -0.2, 0.1],\n",
    "                          [-0.3, 0.6, -0.4]])\n",
    "\n",
    "result = torch.matmul(one_hot[2].float(), embedding)\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263dfb0e-e499-4dbd-b759-da0d7c831a27",
   "metadata": {},
   "source": [
    "# 4. Adjust the hyperparameters (e.g., number of epochs, number of hidden units, number of time steps in a minibatch, and learning rate) to improve the perplexity. How low can you go while sticking with this simple architecture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a0008a4-2048-44c1-9d12-44360234d4d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import warnings\n",
    "sys.path.append('/home/jovyan/work/d2l_solutions/notebooks/exercises/d2l_utils/')\n",
    "import d2l\n",
    "from torchsummary import summary\n",
    "from torch.nn import functional as F\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class RNNScratch(d2l.Module):  #@save\n",
    "    \"\"\"The RNN model implemented from scratch.\"\"\"\n",
    "    def __init__(self, num_inputs, num_hiddens, sigma=0.01, plot_flag=True):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.W_xh = nn.Parameter(\n",
    "            torch.randn(num_inputs, num_hiddens) * sigma)\n",
    "        self.W_hh = nn.Parameter(\n",
    "            torch.randn(num_hiddens, num_hiddens) * sigma)\n",
    "        self.b_h = nn.Parameter(torch.zeros(num_hiddens))\n",
    "        \n",
    "    def forward(self, inputs, state=None):\n",
    "        if state is None:\n",
    "            # Initial state with shape: (batch_size, num_hiddens)\n",
    "            state = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
    "                              device=inputs.device)\n",
    "        else:\n",
    "            state, = state\n",
    "        outputs = []\n",
    "        for X in inputs:  # Shape of inputs: (num_steps, batch_size, num_inputs)\n",
    "            state = torch.tanh(torch.matmul(X, self.W_xh) +\n",
    "                             torch.matmul(state, self.W_hh) + self.b_h)\n",
    "            outputs.append(state)\n",
    "        return outputs, state\n",
    "    \n",
    "class RNNLMScratch(d2l.Classifier):  #@save\n",
    "    \"\"\"The RNN-based language model implemented from scratch.\"\"\"\n",
    "    def __init__(self, rnn, vocab_size, lr=0.01, plot_flag=True):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        self.W_hq = nn.Parameter(\n",
    "            torch.randn(\n",
    "                self.rnn.num_hiddens, self.vocab_size) * self.rnn.sigma)\n",
    "        self.b_q = nn.Parameter(torch.zeros(self.vocab_size))\n",
    "\n",
    "    def training_step(self, batch, plot_flag=True):\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        self.plot('ppl', torch.exp(l), train=True)\n",
    "        return l\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        self.plot('ppl', torch.exp(l), train=False)\n",
    "        \n",
    "    def one_hot(self, X):\n",
    "        # Output shape: (num_steps, batch_size, vocab_size)\n",
    "        return F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
    "    \n",
    "    def output_layer(self, rnn_outputs):\n",
    "        outputs = [torch.matmul(H, self.W_hq) + self.b_q for H in rnn_outputs]\n",
    "        return torch.stack(outputs, 1)\n",
    "\n",
    "    def forward(self, X, state=None):\n",
    "        embs = self.one_hot(X)\n",
    "        rnn_outputs, _ = self.rnn(embs, state)\n",
    "        return self.output_layer(rnn_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dd4e0dd-e476-4e49-b67a-c39aa8e71631",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'F' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m RNNLMScratch(rnn, vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mvocab), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m trainer \u001b[38;5;241m=\u001b[39m d2l\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, gradient_clip_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#, num_gpus=1\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/d2l_solutions/notebooks/exercises/d2l_utils/d2l.py:207\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, data)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_batch_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_epochs):\n\u001b[0;32m--> 207\u001b[0m     train_loss, valid_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_loss, valid_loss\n",
      "File \u001b[0;32m~/work/d2l_solutions/notebooks/exercises/d2l_utils/d2l.py:223\u001b[0m, in \u001b[0;36mTrainer.fit_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    219\u001b[0m train_loss, valid_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# if len(batch[0]) != 32:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#     print(len(batch[0]))\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mplot_flag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_flag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# print(f'step train loss:{loss}, T:{self.model.T}')\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Cell \u001b[0;32mIn[8], line 49\u001b[0m, in \u001b[0;36mRNNLMScratch.training_step\u001b[0;34m(self, batch, plot_flag)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, plot_flag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 49\u001b[0m     l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, batch[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mppl\u001b[39m\u001b[38;5;124m'\u001b[39m, torch\u001b[38;5;241m.\u001b[39mexp(l), train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m l\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[8], line 66\u001b[0m, in \u001b[0;36mRNNLMScratch.forward\u001b[0;34m(self, X, state)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 66\u001b[0m     embs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     rnn_outputs, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(embs, state)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(rnn_outputs)\n",
      "Cell \u001b[0;32mIn[8], line 59\u001b[0m, in \u001b[0;36mRNNLMScratch.one_hot\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_hot\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# Output shape: (num_steps, batch_size, vocab_size)\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241m.\u001b[39mone_hot(X\u001b[38;5;241m.\u001b[39mT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'F' is not defined"
     ]
    }
   ],
   "source": [
    "data = d2l.TimeMachine(batch_size=1024, num_steps=32)\n",
    "rnn = RNNScratch(num_inputs=len(data.vocab), num_hiddens=32)\n",
    "model = RNNLMScratch(rnn, vocab_size=len(data.vocab), lr=1)\n",
    "trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1) #, num_gpus=1\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4968fb-7a23-4282-8109-e9c84960ffc0",
   "metadata": {},
   "source": [
    "# 5. Replace one-hot encoding with learnable embeddings. Does this lead to better performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25df62e1-3ce4-48a0-abec-30e9842831ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d31d825a-eb6b-4f95-a831-0d29379c4202",
   "metadata": {},
   "source": [
    "# 6. Conduct an experiment to determine how well this language model trained on The Time Machine works on other books by H. G. Wells, e.g., The War of the Worlds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b31bf9a-8bbb-4127-b620-bcb3f2b34c67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b0ddbff-784f-4482-8653-2c8b6c6455f0",
   "metadata": {},
   "source": [
    "# 7. Conduct another experiment to evaluate the perplexity of this model on books written by other authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db12da78-7798-4afe-91a9-88e1f319508f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e8e514e-0be7-4084-8c96-6a2907e3df31",
   "metadata": {},
   "source": [
    "# 8. Modify the prediction method so as to use sampling rather than picking the most likely next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4f2d61-e927-4508-8227-44d28df9f314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f1af15e-f2eb-4ef6-96b0-3c04f0371524",
   "metadata": {},
   "source": [
    "## 8.1 What happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6731a230-f9f5-42b8-9c6b-1990b2bf67d1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e23abb36-8118-48c5-b76f-aebaf25bc6f9",
   "metadata": {},
   "source": [
    "## 8.2 Bias the model towards more likely outputs, e.g., by sampling from $q(x_{t}|x_{t-1},\\dots,x_1) \\propto P(x_{t}|x_{t-1},\\dots,x_1)^\\alpha$ for $\\alpha>1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7f801b-efb6-4cce-885a-97f4ba9ff574",
   "metadata": {},
   "source": [
    "# 9. Run the code in this section without clipping the gradient. What happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e793f2b7-6740-4d8d-88ee-3484ca6bb0ea",
   "metadata": {},
   "source": [
    "# 10. Replace the activation function used in this section with ReLU and repeat the experiments in this section. Do we still need gradient clipping? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c680c250-b12c-4ac7-ae00-864f6ceb7d51",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90cad5e3-c31b-479b-b0e0-4b2aec6163b5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9867bde-4295-4db2-a6fd-75344ce3a513",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e894e79-13bc-4ac1-93aa-a3b0d02a27be",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7a55423-5e59-412c-b433-5c95a9746c8d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fb7a4a6-72c8-49bf-81cb-4f23f6ddb85f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72519807-8bc8-4ca8-aea8-2db6c3e12a83",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d329f049-89d0-40b3-a011-923be6d94372",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82f68ce4-965d-464c-a269-845550e686de",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "240a76b4-f884-4d62-8d7e-dadb5fd4e2c3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce901b79-eb27-47d4-a060-7d4b8cffa711",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a700b78c-a6ab-4ec6-b5c2-7c22bbda2d9d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "890aa6a0-78cb-466c-8ea2-725f3e111e6c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8177fc92-7828-4ef1-9f25-3204c131c92a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c130d0fc-f996-4002-b128-52353fd71511",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:d2l]",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
