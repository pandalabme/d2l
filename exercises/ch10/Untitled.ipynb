{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a36e05a8-a88c-4f04-889d-c8346b3c5657",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "sys.path.append('/home/jovyan/work/d2l_solutions/notebooks/exercises/d2l_utils/')\n",
    "import d2l\n",
    "from torchsummary import summary\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class Seq2SeqEncoder(d2l.Encoder):  #@save\n",
    "    \"\"\"The RNN encoder for sequence-to-sequence learning.\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = d2l.GRU(embed_size, num_hiddens, num_layers, dropout)\n",
    "        self.apply(init_seq2seq)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        # X shape: (batch_size, num_steps)\n",
    "        embs = self.embedding(X.t().type(torch.int64))\n",
    "        # embs shape: (num_steps, batch_size, embed_size)\n",
    "        outputs, state = self.rnn(embs)\n",
    "        # outputs shape: (num_steps, batch_size, num_hiddens)\n",
    "        # state shape: (num_layers, batch_size, num_hiddens)\n",
    "        return outputs, state\n",
    "    \n",
    "class Seq2SeqDecoder(d2l.Decoder):\n",
    "    \"\"\"The RNN decoder for sequence to sequence learning.\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = d2l.GRU(embed_size+num_hiddens, num_hiddens,\n",
    "                           num_layers, dropout)\n",
    "        self.dense = nn.LazyLinear(vocab_size)\n",
    "        self.apply(init_seq2seq)\n",
    "\n",
    "    def init_state(self, enc_all_outputs, *args):\n",
    "        return enc_all_outputs\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # X shape: (batch_size, num_steps)\n",
    "        # embs shape: (num_steps, batch_size, embed_size)\n",
    "        embs = self.embedding(X.t().type(torch.int32))\n",
    "        enc_output, hidden_state = state\n",
    "        # context shape: (batch_size, num_hiddens)\n",
    "        context = enc_output[-1]\n",
    "        # Broadcast context to (num_steps, batch_size, num_hiddens)\n",
    "        context = context.repeat(embs.shape[0], 1, 1)\n",
    "        # Concat at the feature dimension\n",
    "        embs_and_context = torch.cat((embs, context), -1)\n",
    "        outputs, hidden_state = self.rnn(embs_and_context, hidden_state)\n",
    "        outputs = self.dense(outputs).swapaxes(0, 1)\n",
    "        # outputs shape: (batch_size, num_steps, vocab_size)\n",
    "        # hidden_state shape: (num_layers, batch_size, num_hiddens)\n",
    "        return outputs, [enc_output, hidden_state]\n",
    "    \n",
    "class Seq2Seq(d2l.EncoderDecoder):  #@save\n",
    "    \"\"\"The RNN encoder--decoder for sequence to sequence learning.\"\"\"\n",
    "    def __init__(self, encoder, decoder, tgt_pad, lr):\n",
    "        super().__init__(encoder, decoder)\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "    def loss(self, Y_hat, Y):\n",
    "        l = super(Seq2Seq, self).loss(Y_hat, Y, averaged=False)\n",
    "        mask = (Y.reshape(-1) != self.tgt_pad).type(torch.float32)\n",
    "        return (l * mask).sum() / mask.sum()\n",
    "\n",
    "    def validation_step(self, batch, plot_flag=True):\n",
    "        Y_hat = self(*batch[:-1])\n",
    "        l = self.loss(Y_hat, batch[-1])\n",
    "        if plot_flag:\n",
    "            self.plot('loss', l, train=False)\n",
    "        return l\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Adam optimizer is used here\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "    \n",
    "def init_seq2seq(module):  #@save\n",
    "    \"\"\"Initialize weights for sequence-to-sequence learning.\"\"\"\n",
    "    if type(module) == nn.Linear:\n",
    "         nn.init.xavier_uniform_(module.weight)\n",
    "    if type(module) == nn.GRU:\n",
    "        for param in module._flat_weights_names:\n",
    "            if \"weight\" in param:\n",
    "                nn.init.xavier_uniform_(module._parameters[param])\n",
    "\n",
    "def stat_val(model, data):\n",
    "    ppls = []\n",
    "    for batch in iter(data.get_dataloader(False)):\n",
    "        ppls.append(model.validation_step(batch, plot_flag=False).detach().numpy())\n",
    "    return np.exp(np.mean(ppls))\n",
    "\n",
    "def experiment(model, data):\n",
    "    trainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1, num_gpus=0)\n",
    "    trainer.fit(model, data)\n",
    "    return stat_val(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd4a82ef-6166-490e-b935-03acfd3d1db5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = d2l.MTFraEng(batch_size=128)\n",
    "embed_size, num_hiddens, num_layers, dropout = 256, 256, 3, 0.2\n",
    "encoder = Seq2SeqEncoder(\n",
    "    len(data.src_vocab), embed_size, num_hiddens, num_layers, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c587d72-38e9-4f05-b1af-2fe8bbc345ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f292466d-12f2-4832-b3f5-6b2baadb43b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 128, 256])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e74270a4-877b-4677-99fc-a66eb266b37e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 512])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src, tgt, src_valid_len, label = next(iter(data.get_dataloader(False)))\n",
    "y = encoder(src)\n",
    "dec_hiddens, dec_layers = 512, 2\n",
    "model = nn.LazyLinear(dec_hiddens*dec_layers)\n",
    "H = y[1].swapaxes(0, 1)\n",
    "H = H.reshape(H.shape[0],-1)\n",
    "S = model(H)\n",
    "S = S.reshape(S.shape[0],-1,dec_hiddens)\n",
    "S = S.swapaxes(0, 1)\n",
    "S.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:d2l]",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
