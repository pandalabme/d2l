{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58c8d368-ba90-48df-8025-72393efc74e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. If the different directions use a different number of hidden units, how will the shape of $H_t$ change?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41779fd3-5f17-4ba8-93de-2f11fc355ca1",
   "metadata": {},
   "source": [
    "$H_t.shape[-1] = \\overrightarrow{H_t}.shape[-1] + \\overleftarrow{H_t}.shape[-1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22e3bde3-8077-4bb3-a705-279aade7a4ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/d2l_solutions/notebooks/exercises/d2l_utils/d2l.py:129: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(self, 'net'), 'Neural network is defined'\n",
      "/home/jovyan/work/d2l_solutions/notebooks/exercises/d2l_utils/d2l.py:133: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(self, 'trainer'), 'trainer is not inited'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import warnings\n",
    "sys.path.append('/home/jovyan/work/d2l_solutions/notebooks/exercises/d2l_utils/')\n",
    "import d2l\n",
    "from torchsummary import summary\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "class BiRNNScratch(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.f_rnn = d2l.RNNScratch(num_inputs, num_hiddens[0], sigma)\n",
    "        self.b_rnn = d2l.RNNScratch(num_inputs, num_hiddens[1], sigma)\n",
    "        self.num_hiddens = sum(num_hiddens)  # The output dimension will be doubled\n",
    "        \n",
    "    def forward(self, inputs, Hs=None):\n",
    "        f_H, b_H = Hs if Hs is not None else (None, None)\n",
    "        f_outputs, f_H = self.f_rnn(inputs, f_H)\n",
    "        b_outputs, b_H = self.b_rnn(reversed(inputs), b_H)\n",
    "        outputs = [torch.cat((f, b), -1) for f, b in zip(\n",
    "            f_outputs, reversed(b_outputs))]\n",
    "        return outputs, (f_H, b_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69b2eb54-e50e-49e8-819f-f15b990c4df3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 24])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_rnn = BiRNNScratch(num_inputs=4, num_hiddens=[8,16])\n",
    "X = torch.randn(1,4)\n",
    "outputs, (f_H, b_H) = bi_rnn(X)\n",
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025d46ab-5048-4795-b8f9-d7fdcde83935",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Design a bidirectional RNN with multiple hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c302702d-71f9-4b91-886c-fde84fa80ceb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BiRNN(d2l.RNN):\n",
    "    def __init__(self, num_inputs, num_hiddens, num_layers):\n",
    "        d2l.Module.__init__(self)\n",
    "        self.save_hyperparameters()\n",
    "        self.rnn = nn.RNN(num_inputs, num_hiddens, num_layers=num_layers, bidirectional=True)\n",
    "        self.num_hiddens *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "754be800-a22f-40b5-b9fa-27cabc73e77e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiRNN(\n",
       "  (rnn): RNN(4, 8, num_layers=2, bidirectional=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_rnn = BiRNN(num_inputs=4, num_hiddens=8,num_layers=2)\n",
    "bi_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe2147b-6bf3-4d56-801c-0dea21e48e23",
   "metadata": {},
   "source": [
    "# 3. Polysemy is common in natural languages. For example, the word “bank” has different meanings in contexts “i went to the bank to deposit cash” and “i went to the bank to sit down”. How can we design a neural network model such that given a context sequence and a word, a vector representation of the word in the correct context will be returned? What type of neural architectures is preferred for handling polysemy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d95db83-c5b9-45f7-895f-68109e9b0aa5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.3454, -0.0369, -0.1882,  0.1576],\n",
       "        [ 0.3329, -0.1928, -0.1029, -0.0484],\n",
       "        [-0.2265,  0.1040,  0.1375,  0.0568],\n",
       "        [ 0.2175,  0.1918,  0.3387, -0.0371],\n",
       "        [-0.1609, -0.0286, -0.1931, -0.3293],\n",
       "        [ 0.1150, -0.1777, -0.0100,  0.3497],\n",
       "        [ 0.3511,  0.3004,  0.2287, -0.3489],\n",
       "        [-0.2413, -0.2574,  0.0273, -0.3384]], requires_grad=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_rnn.rnn.weight_ih_l0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e51e23-5a7c-4d55-8e7e-b351b663cbcc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:d2l]",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
