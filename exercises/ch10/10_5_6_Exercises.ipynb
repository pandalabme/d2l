{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1e8f781-0095-4c66-8255-7316585692fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Try different values of the max_examples argument in the _tokenize method. How does this affect the vocabulary sizes of the source language and the target language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76fcfc70-aed7-4358-a9d5-bc02af6272f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import warnings\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "sys.path.append('/home/jovyan/work/d2l_solutions/notebooks/exercises/d2l_utils/')\n",
    "import d2l\n",
    "from torchsummary import summary\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class MTFraEng(d2l.DataModule):  #@save\n",
    "    \"\"\"The English-French dataset.\"\"\"\n",
    "    def _download(self):\n",
    "        d2l.extract(d2l.download(\n",
    "            d2l.DATA_URL+'fra-eng.zip', self.root,\n",
    "            '94646ad1522d915e7b0f9296181140edcf86a4f5'))\n",
    "        with open(self.root + '/fra-eng/fra.txt', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "        \n",
    "    def _preprocess(self, text):\n",
    "        # Replace non-breaking space with space\n",
    "        text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n",
    "        # Insert space between words and punctuation marks\n",
    "        no_space = lambda char, prev_char: char in ',.!?' and prev_char != ' '\n",
    "        out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n",
    "               for i, char in enumerate(text.lower())]\n",
    "        return ''.join(out)\n",
    "    \n",
    "    def _tokenize(self, text, max_examples=None):\n",
    "        src, tgt = [], []\n",
    "        for i, line in enumerate(text.split('\\n')):\n",
    "            if max_examples and i > max_examples: break\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                # Skip empty tokens\n",
    "                src.append([t for t in f'{parts[0]} <eos>'.split(' ') if t])\n",
    "                tgt.append([t for t in f'{parts[1]} <eos>'.split(' ') if t])\n",
    "        return src, tgt\n",
    "    \n",
    "    def __init__(self, batch_size, num_steps=9, num_train=512, num_val=128):\n",
    "        super(MTFraEng, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.arrays, self.src_vocab, self.tgt_vocab = self._build_arrays(\n",
    "            self._download())\n",
    "\n",
    "    def _build_arrays(self, raw_text, src_vocab=None, tgt_vocab=None):\n",
    "        def _build_array(sentences, vocab, is_tgt=False):\n",
    "            pad_or_trim = lambda seq, t: (\n",
    "                seq[:t] if len(seq) > t else seq + ['<pad>'] * (t - len(seq)))\n",
    "            sentences = [pad_or_trim(s, self.num_steps) for s in sentences]\n",
    "            if is_tgt:\n",
    "                sentences = [['<bos>'] + s for s in sentences]\n",
    "            if vocab is None:\n",
    "                vocab = d2l.Vocab(sentences, min_freq=2)\n",
    "            array = torch.tensor([vocab.to_tokens(s) for s in sentences])\n",
    "            valid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)\n",
    "            return array, vocab, valid_len\n",
    "        src, tgt = self._tokenize(self._preprocess(raw_text),\n",
    "                                  self.num_train + self.num_val)\n",
    "        src_array, src_vocab, src_valid_len = _build_array(src, src_vocab)\n",
    "        tgt_array, tgt_vocab, _ = _build_array(tgt, tgt_vocab, True)\n",
    "        return ((src_array, tgt_array[:,:-1], src_valid_len, tgt_array[:,1:]),\n",
    "                src_vocab, tgt_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05433e4a-e146-41ec-b7d4-3674a02f7d98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'go'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mMTFraEng\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m src, tgt, src_valid_len, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(data\u001b[38;5;241m.\u001b[39mtrain_dataloader()))\n",
      "Cell \u001b[0;32mIn[9], line 43\u001b[0m, in \u001b[0;36mMTFraEng.__init__\u001b[0;34m(self, batch_size, num_steps, num_train, num_val)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28msuper\u001b[39m(MTFraEng, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_hyperparameters()\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marrays, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_vocab, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtgt_vocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 60\u001b[0m, in \u001b[0;36mMTFraEng._build_arrays\u001b[0;34m(self, raw_text, src_vocab, tgt_vocab)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m array, vocab, valid_len\n\u001b[1;32m     58\u001b[0m src, tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess(raw_text),\n\u001b[1;32m     59\u001b[0m                           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_val)\n\u001b[0;32m---> 60\u001b[0m src_array, src_vocab, src_valid_len \u001b[38;5;241m=\u001b[39m \u001b[43m_build_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_vocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m tgt_array, tgt_vocab, _ \u001b[38;5;241m=\u001b[39m _build_array(tgt, tgt_vocab, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ((src_array, tgt_array[:,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], src_valid_len, tgt_array[:,\u001b[38;5;241m1\u001b[39m:]),\n\u001b[1;32m     63\u001b[0m         src_vocab, tgt_vocab)\n",
      "Cell \u001b[0;32mIn[9], line 55\u001b[0m, in \u001b[0;36mMTFraEng._build_arrays.<locals>._build_array\u001b[0;34m(sentences, vocab, is_tgt)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     vocab \u001b[38;5;241m=\u001b[39m d2l\u001b[38;5;241m.\u001b[39mVocab(sentences, min_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m array \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43m[\u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     56\u001b[0m valid_len \u001b[38;5;241m=\u001b[39m (array \u001b[38;5;241m!=\u001b[39m vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array, vocab, valid_len\n",
      "Cell \u001b[0;32mIn[9], line 55\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     vocab \u001b[38;5;241m=\u001b[39m d2l\u001b[38;5;241m.\u001b[39mVocab(sentences, min_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m array \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sentences])\n\u001b[1;32m     56\u001b[0m valid_len \u001b[38;5;241m=\u001b[39m (array \u001b[38;5;241m!=\u001b[39m vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array, vocab, valid_len\n",
      "File \u001b[0;32m~/work/d2l_solutions/notebooks/exercises/d2l_utils/d2l.py:588\u001b[0m, in \u001b[0;36mVocab.to_tokens\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_tokens\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices):\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(indices, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__len__\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indices) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 588\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midx_to_token\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midx_to_token[indices]\n",
      "File \u001b[0;32m~/work/d2l_solutions/notebooks/exercises/d2l_utils/d2l.py:588\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_tokens\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices):\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(indices, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__len__\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indices) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 588\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midx_to_token[\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m indices]\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midx_to_token[indices]\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'go'"
     ]
    }
   ],
   "source": [
    "data = MTFraEng(batch_size=3)\n",
    "src, tgt, src_valid_len, label = next(iter(data.train_dataloader()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f36073-9664-433e-97df-505a6711d8f2",
   "metadata": {},
   "source": [
    "# 2. Text in some languages such as Chinese and Japanese does not have word boundary indicators (e.g., space). Is word-level tokenization still a good idea for such cases? Why or why not?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:d2l]",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
