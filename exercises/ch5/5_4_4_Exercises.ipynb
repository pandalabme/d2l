{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64025ae0-e4d8-4993-94a8-d99142d1abb0",
   "metadata": {},
   "source": [
    "# 1. Can you design other cases where a neural network might exhibit symmetry that needs breaking, besides the permutation symmetry in an MLPâ€™s layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2357a7b-4c71-44d3-b45e-385035fe508c",
   "metadata": {},
   "source": [
    "Certainly, there are several other scenarios in which neural networks might exhibit symmetry that needs to be broken. Symmetry in neural networks can lead to difficulties in learning, convergence, and generalization. Here are a few cases where symmetry breaking is important:\n",
    "\n",
    "1. **Weight Symmetry in Convolutional Layers:**\n",
    "   In convolutional neural networks (CNNs), weight sharing across different channels or filters can lead to symmetry in feature detection. Breaking this symmetry is important to ensure that different filters specialize in detecting different features, improving the network's representational capacity.\n",
    "\n",
    "2. **Initializations for Recurrent Neural Networks (RNNs):**\n",
    "   In RNNs, all the recurrent units are updated with the same weights in each time step. This can lead to symmetry in how information is processed across time. Proper weight initialization and techniques like orthogonal initialization or identity initialization are used to break this symmetry and allow the network to learn meaningful temporal dependencies.\n",
    "\n",
    "3. **Symmetric Activation Functions:**\n",
    "   If you use activation functions that are symmetric around the origin, such as the hyperbolic tangent (tanh) or the sine function, the network's hidden units might exhibit symmetry in their responses. This can lead to slow convergence or stuck gradients. Using non-symmetric activation functions like ReLU or Leaky ReLU can help break this symmetry.\n",
    "\n",
    "4. **Shared Weights in Autoencoders:**\n",
    "   In autoencoders, symmetric weight sharing between the encoder and decoder can result in learning a trivial identity mapping. By introducing constraints or regularization techniques, you can break this symmetry and force the network to learn meaningful representations.\n",
    "\n",
    "5. **Permutation Symmetry in Graph Neural Networks (GNNs):**\n",
    "   Graph Neural Networks often operate on graphs, where the order of nodes doesn't matter. However, if the network treats nodes symmetrically, it might miss out on important structural information. Techniques like node shuffling or adding positional embeddings can break this symmetry.\n",
    "\n",
    "6. **Symmetry in Attention Mechanisms:**\n",
    "   In attention mechanisms, if all query-key pairs are treated symmetrically, the mechanism might not learn to differentiate important relationships from less important ones. Attention masks or position-dependent weights can break this symmetry.\n",
    "\n",
    "7. **Symmetric Pooling Layers:**\n",
    "   Symmetric pooling operations (e.g., average pooling) might not effectively capture hierarchical features. Using max pooling or adaptive pooling can help break this symmetry and focus on more salient features.\n",
    "\n",
    "In general, symmetry breaking techniques aim to encourage the network to learn diverse, meaningful representations and relationships within the data. This leads to improved learning, faster convergence, and better generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d79c90-c1b5-4094-b953-0a66df714605",
   "metadata": {},
   "source": [
    "# 2. Can we initialize all weight parameters in linear regression or in softmax regression to the same value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19633eb4-837f-4cbe-b40d-67d423e8a4fb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea889958-553a-47fb-b037-5212217d8dc1",
   "metadata": {},
   "source": [
    "# 3. Look up analytic bounds on the eigenvalues of the product of two matrices. What does this tell you about ensuring that gradients are well conditioned?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd9385f-f990-48f8-afe9-fea1cbfc3cae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "470f77c7-a7d6-4535-b9d7-df5e16142d08",
   "metadata": {},
   "source": [
    "# 4. If we know that some terms diverge, can we fix this after the fact? Look at the paper on layerwise adaptive rate scaling for inspiration (You et al., 2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58821e72-eba0-48be-a001-b643a7adb0a5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:d2l]",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
