{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "667ed0e0-2d30-44d2-b068-d68a37ce338a",
   "metadata": {},
   "source": [
    "# 6. Compare the speed of the framework and the from-scratch implementation for a challenging problem. How does it change with the complexity of the network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fb47cf-b27f-4e6c-a7c3-633f59c4143c",
   "metadata": {},
   "source": [
    "Comparing the speed of a deep learning framework like PyTorch with a from-scratch implementation for a challenging problem can be insightful, as it provides an understanding of the performance benefits and trade-offs of each approach. The speed comparison can be affected by various factors, including the problem complexity, network architecture, data size, and hardware resources.\n",
    "\n",
    "Here's how the speed comparison might change with the complexity of the network:\n",
    "\n",
    "1. **Simple Network:**\n",
    "   For a relatively simple network architecture with a small number of layers and parameters, the speed difference between a framework and a from-scratch implementation might not be as significant. The overhead introduced by the framework's abstractions and optimizations could be more noticeable relative to the problem's complexity.\n",
    "\n",
    "2. **Moderate Complexity:**\n",
    "   As the network complexity increases, with more layers, units, and parameters, the deep learning framework's optimizations can become more valuable. Frameworks often leverage GPU acceleration, optimized tensor operations, and parallelism, resulting in significant speed improvements compared to a purely from-scratch implementation.\n",
    "\n",
    "3. **Complex Network:**\n",
    "   In the case of complex architectures like deep convolutional networks or large-scale recurrent networks, the gap in speed between the framework and from-scratch implementation can be substantial. The deep learning framework's low-level optimizations, automatic differentiation, and GPU acceleration can provide a significant advantage, enabling faster training and convergence.\n",
    "\n",
    "4. **Batch Processing:**\n",
    "   The deep learning framework's ability to efficiently process mini-batches of data further contributes to its speed advantage. Frameworks can take advantage of vectorized operations and parallelism to process multiple data points simultaneously, leading to faster updates of model parameters.\n",
    "\n",
    "5. **Hardware Utilization:**\n",
    "   The use of specialized hardware, such as GPUs or TPUs, can greatly accelerate training in a framework. These hardware devices are optimized for tensor operations and can significantly outperform CPUs in terms of both computation and memory bandwidth.\n",
    "\n",
    "6. **Custom Implementation Control:**\n",
    "   While a from-scratch implementation might provide more control over every aspect of the process, including initialization methods, optimization algorithms, and convergence criteria, it usually comes at the cost of development time and potentially slower execution.\n",
    "\n",
    "In summary, deep learning frameworks like PyTorch are designed to optimize training efficiency and provide a balance between performance and flexibility. They leverage hardware acceleration, automatic differentiation, and optimizations to significantly speed up the training process, especially for complex network architectures. A from-scratch implementation, on the other hand, might offer more customization but can be significantly slower, especially for challenging problems involving complex networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5d9561-4c71-428f-970f-6df73a2e577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import cProfile\n",
    "sys.path.append('/home/jovyan/work/d2l_solutions/notebooks/exercises/d2l_utils/')\n",
    "import d2l\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class MulMLPScratch(d2l.Classifier):\n",
    "    def __init__(self, num_inputs, num_outputs, num_hiddens, lr, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        bef = num_inputs\n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        for num_hidden in num_hiddens:\n",
    "            self.W.append(nn.Parameter(torch.randn(bef, num_hidden)*sigma))\n",
    "            self.b.append(nn.Parameter(torch.zeros(num_hidden)))\n",
    "            bef = num_hidden\n",
    "        self.W.append(nn.Parameter(torch.randn(bef, num_outputs)*sigma))\n",
    "        self.b.append(nn.Parameter(torch.zeros(num_outputs)))\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        H = X.reshape(-1, self.num_inputs)\n",
    "        for i in range(len(self.W)-1):\n",
    "            H = relu(torch.matmul(H, self.W[i]) + self.b[i])\n",
    "        return torch.matmul(H, self.W[-1]) + self.b[-1]\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return d2l.SGD([*self.W, *self.b], self.lr)\n",
    "\n",
    "def stat_time(model, data):\n",
    "    t0 = time.time()\n",
    "    trainer = d2l.Trainer(max_epochs=10, plot_flag=False)\n",
    "    trainer.fit(model, data)\n",
    "    return time.time() - t0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a396885-3d55-4ad6-8c2e-4659bbcc4303",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens=[256,128,64,32,16]\n",
    "ts = []\n",
    "ts_strach = []\n",
    "for i in tqdm(range(1,len(num_hiddens)+1)):\n",
    "    model = MulMLP(num_outputs=10, num_hiddens=num_hiddens[:i], lr=0.1)\n",
    "    model_scratch = MulMLPScratch(num_inputs=784, num_outputs=10, num_hiddens=num_hiddens[:i], lr=0.1)\n",
    "    ts_strach.append(stat_time(model_scratch, data))\n",
    "    ts.append(stat_time(model, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fbecbc-eeda-443a-baea-38be012f6eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.plot(list(range(1,len(num_hiddens)+1)),[ts,ts_strach],legend=['framework','scratch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86730e9-8be5-4063-8213-e29a9acfef2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 7. Measure the speed of tensorâ€“matrix multiplications for well-aligned and misaligned matrices. For instance, test for matrices with dimension 1024, 1025, 1026, 1028, and 1032.\n",
    "- How does this change between GPUs and CPUs?\n",
    "- Determine the memory bus width of your CPU and GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d4cbc4-bbab-46d2-a45b-e7bae709b3eb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bd6c6e9-e490-47b3-ba86-ee14fcee689d",
   "metadata": {},
   "source": [
    "# 8. Try out different activation functions. Which one works best?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4967fb6-a827-4682-8555-82372dda8258",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c22287-e160-4b0c-9a1d-10848bab0ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ActMLP(d2l.Classifier):\n",
    "    def __init__(self, num_outputs, num_hiddens, lr, act=act):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(nn.Flatten(), nn.LazyLinear(num_hiddens),\n",
    "                                 nn.ReLU(), nn.LazyLinear(num_outputs))\n",
    "        \n",
    "def stat_act(act, data):\n",
    "    model = ActMLP(num_outputs=10, num_hiddens=256, lr=0.1, act=act)\n",
    "    trainer = d2l.Trainer(max_epochs=10, plot_flag=False)\n",
    "    trainer.fit(model, data)\n",
    "    y_hat = model(data.val.data.type(torch.float32))\n",
    "    return model.accuracy(y_hat,data.val.targets).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf324c1-5aba-4aaa-8e3c-7f5440f1845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "acts = [nn.ReLU(),nn.Sigmoid(), nn.Tanh(),nn.LeakyReLU(negative_slope=0.01),nn.PReLU(num_parameters=1)]\n",
    "accs = []\n",
    "for act in tqdm(acts):\n",
    "    accs.append(stat_act(act, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba3ed54-9e6e-4eff-8bfb-e0ea10dc073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.plot(range(len(acts)),accs[-len(lrs):],'activation','acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347f1004-88a2-47a5-870b-a183012fa70f",
   "metadata": {},
   "source": [
    "# 9. Is there a difference between weight initializations of the network? Does it matter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afed105-3384-4375-99c2-202bfb78d50d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76b0ecde-516e-45d1-adcd-7fa89a193b27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(d2l.Classifier):\n",
    "    def __init__(self, num_outputs, num_hiddens, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(nn.Flatten(), nn.Linear(num_hiddens),\n",
    "                                 nn.ReLU(), nn.Linear(num_outputs))\n",
    "        \n",
    "\n",
    "def init_xavier(module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        init.xavier_uniform(module.weight)\n",
    "        if module.bias is not None:\n",
    "            init.constant_(module.bias, 0)\n",
    "            \n",
    "def init_uniform(module):\n",
    "    if isinstance(module, nn.LazyLinear):\n",
    "        init.uniform_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            init.constant_(module.bias, 0)\n",
    "            \n",
    "def init_normal(module):\n",
    "    if isinstance(module, nn.LazyLinear):\n",
    "        init.normal_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            init.constant_(module.bias, 0)\n",
    "            \n",
    "        \n",
    "def stat_init(init_f, data):\n",
    "    model = MLP(num_outputs=10, num_hiddens=256, lr=0.1)\n",
    "    model.apply(init_f)\n",
    "    trainer = d2l.Trainer(max_epochs=10, plot_flag=True)\n",
    "    trainer.fit(model, data)\n",
    "    y_hat = model(data.val.data.type(torch.float32))\n",
    "    return model.accuracy(y_hat,data.val.targets).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e19cd79-5995-46d0-a431-231d546f370e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Linear.__init__() missing 1 required positional argument: 'out_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m accs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(inits):\n\u001b[0;32m----> 5\u001b[0m     accs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mstat_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[13], line 29\u001b[0m, in \u001b[0;36mstat_init\u001b[0;34m(init_f, data)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstat_init\u001b[39m(init_f, data):\n\u001b[0;32m---> 29\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_hiddens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     model\u001b[38;5;241m.\u001b[39mapply(init_f)\n\u001b[1;32m     31\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m d2l\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, plot_flag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m, in \u001b[0;36mMLP.__init__\u001b[0;34m(self, num_outputs, num_hiddens, lr)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_hyperparameters()\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(nn\u001b[38;5;241m.\u001b[39mFlatten(), \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_hiddens\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      6\u001b[0m                          nn\u001b[38;5;241m.\u001b[39mReLU(), nn\u001b[38;5;241m.\u001b[39mLinear(num_outputs))\n",
      "\u001b[0;31mTypeError\u001b[0m: Linear.__init__() missing 1 required positional argument: 'out_features'"
     ]
    }
   ],
   "source": [
    "data = d2l.FashionMNIST(batch_size=256)\n",
    "inits = [init_xavier,init_uniform,init_normal]\n",
    "accs = []\n",
    "for i in tqdm(inits):\n",
    "    accs.append(stat_init(i, data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:d2l]",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
