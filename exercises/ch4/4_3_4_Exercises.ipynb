{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2992aa85-9c09-49d7-a789-27db58cf8aee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/d2l_solutions/notebooks/exercises/d2l_utils/d2l.py:119: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(self, 'net'), 'Neural network is defined'\n",
      "/home/jovyan/work/d2l_solutions/notebooks/exercises/d2l_utils/d2l.py:123: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(self, 'trainer'), 'trainer is not inited'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import warnings\n",
    "import sys\n",
    "sys.path.append('/home/jovyan/work/d2l_solutions/notebooks/exercises/d2l_utils/')\n",
    "import d2l\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class Classifier(d2l.Module):\n",
    "    def validation_step(self, batch):\n",
    "        y_hat = self(*batch[:-1])\n",
    "        self.plot('loss', self.loss(y_hat, batch[-1]), train=False)\n",
    "        self.plot('acc', self.accuracy(y_hat, batch[-1]), train=False)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(self.parameters(), lr=self.lr)\n",
    "    \n",
    "    def accuracy(self, y_hat, y, averaged=True):\n",
    "        y_hat = y_hat.reshape((-1, y_hat.shape[-1]))\n",
    "        preds = y_hat.argmax(axis=1).type(y.dtype)\n",
    "        comp = (preds == y.reshape(-1)).type(torch.float32)\n",
    "        return comp.mean if averaged else comp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22caa7cc-0300-407f-bbf1-30d3cbbdb6d4",
   "metadata": {},
   "source": [
    "# 4.3.4. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534e7bd3-8c64-4247-944d-464cb7758f39",
   "metadata": {},
   "source": [
    "## 1. Denote by $L_v$ the validation loss, and let $L_v^q$ be its quick and dirty estimate computed by the loss function averaging in this section. Lastly, denote by $l_v^b$ the loss on the last minibatch. Express $L_v$ in terms of $L_v^q$, $l_v^b$, and the sample and minibatch sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f077b5d8-be8e-4492-90eb-42fc8c10a3f3",
   "metadata": {},
   "source": [
    "We assume that the validation dataset is split into \\(N\\) samples, and each minibatch contains \\(M\\) samples.\n",
    "\n",
    "The quick and dirty estimate \\(L_v^q\\) is computed by averaging the loss computed on each minibatch. Since there are \\(N\\) samples in total, and each minibatch contains \\(M\\) samples, there are \\(N/M\\) minibatches in total.\n",
    "\n",
    "The relationship between \\(L_v^q\\) and \\(l_v^b\\) is that \\(L_v^q\\) is an average of the minibatch losses, while \\(l_v^b\\) is the loss computed on the last minibatch. In other words:\n",
    "\n",
    "\\[L_v^q = \\frac{1}{N/M} \\sum_{i=1}^{N/M} l_v^b\\]\n",
    "\n",
    "Now, let's express \\(L_v\\) in terms of \\(L_v^q\\), \\(l_v^b\\), \\(N\\), and \\(M\\):\n",
    "\n",
    "\\(L_v\\) is the true validation loss, and it can be considered as an average of the minibatch losses, similar to \\(L_v^q\\). However, instead of just using the minibatch losses, it's computed over the entire validation dataset of size \\(N\\):\n",
    "\n",
    "\\[L_v = \\frac{1}{N} \\sum_{i=1}^{N} l_v^b\\]\n",
    "\n",
    "Now, we can substitute the expression for \\(L_v^q\\) into the equation for \\(L_v\\):\n",
    "\n",
    "\\[L_v = \\frac{1}{N} \\sum_{i=1}^{N} l_v^b = \\frac{N/M}{N} \\sum_{i=1}^{N/M} l_v^b = \\frac{1}{M} \\sum_{i=1}^{N/M} l_v^b = L_v^q\\]\n",
    "\n",
    "So, the final expression is:\n",
    "\n",
    "\\[L_v = L_v^q\\]\n",
    "\n",
    "In other words, the true validation loss \\(L_v\\) is equal to the quick and dirty estimate \\(L_v^q\\), which is the average of the loss computed on each minibatch during validation. This result assumes that the minibatches are representative of the entire validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f135f23f-2f3a-441d-8b9c-085ace124508",
   "metadata": {},
   "source": [
    "## 2. Show that the quick and dirty estimate $L_v^q$ is unbiased. That is, show that  $E[L_v]=E[L_v^q]$. Why would you still want to use $L_v$ instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059102b4-daab-4259-a7b4-d71329d88e19",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32a75f17-4175-4551-b5fb-1732079c3734",
   "metadata": {},
   "source": [
    "## 3. Given a multiclass classification loss, denoting by $l(y,y^\\prime)$ the penalty of estimating $y^\\prime$ when we see $y$ and given a probabilty $p(y|x)$, formulate the rule for an optimal selection of $y^\\prime$.\n",
    "Hint: express the expected loss, using $l$ and $p(y|x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09b06b8-3dfd-44cb-a312-980ede6ae11b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:d2l]",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
