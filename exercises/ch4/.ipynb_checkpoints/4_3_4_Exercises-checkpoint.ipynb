{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2992aa85-9c09-49d7-a789-27db58cf8aee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/d2l_solutions/notebooks/exercises/d2l_utils/d2l.py:119: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(self, 'net'), 'Neural network is defined'\n",
      "/home/jovyan/work/d2l_solutions/notebooks/exercises/d2l_utils/d2l.py:123: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(self, 'trainer'), 'trainer is not inited'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import warnings\n",
    "import sys\n",
    "sys.path.append('/home/jovyan/work/d2l_solutions/notebooks/exercises/d2l_utils/')\n",
    "import d2l\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class Classifier(d2l.Module):\n",
    "    def validation_step(self, batch):\n",
    "        y_hat = self(*batch[:-1])\n",
    "        self.plot('loss', self.loss(y_hat, batch[-1]), train=False)\n",
    "        self.plot('acc', self.accuracy(y_hat, batch[-1]), train=False)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(self.parameters(), lr=self.lr)\n",
    "    \n",
    "    def accuracy(self, y_hat, y, averaged=True):\n",
    "        y_hat = y_hat.reshape((-1, y_hat.shape[-1]))\n",
    "        preds = y_hat.argmax(axis=1).type(y.dtype)\n",
    "        comp = (preds == y.reshape(-1)).type(torch.float32)\n",
    "        return comp.mean if averaged else comp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22caa7cc-0300-407f-bbf1-30d3cbbdb6d4",
   "metadata": {},
   "source": [
    "# 4.3.4. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534e7bd3-8c64-4247-944d-464cb7758f39",
   "metadata": {},
   "source": [
    "## 1. Denote by $L_v$ the validation loss, and let $L_v^q$ be its quick and dirty estimate computed by the loss function averaging in this section. Lastly, denote by $l_v^b$ the loss on the last minibatch. Express $L_v$ in terms of $L_v^q$, $l_v^b$, and the sample and minibatch sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f077b5d8-be8e-4492-90eb-42fc8c10a3f3",
   "metadata": {},
   "source": [
    "We assume that the validation dataset is split into $N$ samples, and each minibatch contains $M$ samples.\n",
    "The quick and dirty estimate $L_v^q$ is computed by averaging the loss computed on each minibatch. Since there are $N$ samples in total, and each minibatch contains $M$ samples, there are $N/M$ minibatches in total.\n",
    "Now, let's express $L_v$ in terms of $L_v^q$, $l_v^b$, $N$, and $M$:\n",
    "$L_v$ is the true validation loss, and it can be considered as an average of the batch losses:\n",
    "$$L_v = \\frac{M}{N} \\sum_{i=1}^{N/M}l_v^q$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f135f23f-2f3a-441d-8b9c-085ace124508",
   "metadata": {},
   "source": [
    "## 2. Show that the quick and dirty estimate $L_v^q$ is unbiased. That is, show that $E[L_v]=E[L_v^q]$. Why would you still want to use $L_v$ instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059102b4-daab-4259-a7b4-d71329d88e19",
   "metadata": {},
   "source": [
    "$$E[L_v] = E[\\frac{M}{N} \\sum_{i=1}^{N/M}l_v^q]==\\frac{M}{N}\\sum_{i=1}^{N/M}E[l_v^q]=E[l_v^q]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a75f17-4175-4551-b5fb-1732079c3734",
   "metadata": {},
   "source": [
    "## 3. Given a multiclass classification loss, denoting by $l(y,y^\\prime)$ the penalty of estimating $y^\\prime$ when we see $y$ and given a probabilty $p(y|x)$, formulate the rule for an optimal selection of $y^\\prime$.\n",
    "Hint: express the expected loss, using $l$ and $p(y|x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09b06b8-3dfd-44cb-a312-980ede6ae11b",
   "metadata": {},
   "source": [
    "The optimal selection of $y^\\prime$ in a multiclass classification scenario can be formulated using the concept of expected loss. Given a true class $y$ and a predicted class $y^\\prime$, and assuming that $p(y|x)$ represents the probability of observing class $y$ given input $x$, the expected loss can be used to guide the decision-making process.\n",
    "The expected loss $\\mathbb{E}[l(y, y^\\prime)]$ is the average loss that we expect to incur when predicting $y^\\prime$ while the true class is $y$. To minimize the expected loss, we need to select the $y^\\prime$ that minimizes this average.\n",
    "The optimal selection of $y^\\prime$ can be formulated as follows:\n",
    "$$y^\\prime = \\arg\\min_{\\text{all possible } y^\\prime} \\sum_{y} p(y|x) \\cdot l(y, y^\\prime)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:d2l]",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
