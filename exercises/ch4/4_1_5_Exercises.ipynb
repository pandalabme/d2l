{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ae432e3-29db-4fb4-8428-ef121a7c203a",
   "metadata": {},
   "source": [
    "# 4.1.5. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a390f42-f0b5-4c5a-8a23-ea76e14a0865",
   "metadata": {},
   "source": [
    "## 1. We can explore the connection between exponential families and softmax in some more depth.\n",
    "* Compute the second derivative of the cross-entropy loss $l(y,\\hat{y})$ for softmax.\n",
    "* Compute the variance of the distribution given by $\\text{softmax}(o)$ and show that it matches the second derivative computed above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af39231-5c7e-4d3f-a2d4-79dc8bfeb02e",
   "metadata": {},
   "source": [
    "### Second derivative of the cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e07b28b-6333-43b4-9770-da514de917d8",
   "metadata": {},
   "source": [
    "The cross-entropy loss for softmax is given by:\n",
    "\n",
    "$$l(y, \\hat{y}) = -\\sum_i y_i \\log(\\text{softmax}(o_i))$$\n",
    "\n",
    "where $\\text{softmax}(o)_i = \\frac{e^{o_i}}{\\sum_j e^{o_j}}$.\n",
    "\n",
    "Let's denote the loss as $L = -\\sum_i y_i \\log(\\text{softmax}(o_i))$.\n",
    "\n",
    "**First Derivative:**\n",
    "The first derivative of $L$ with respect to $o_j$ is:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial o_j} = \\text{softmax}(o_j) - y_j$$\n",
    "\n",
    "**Second Derivative:**\n",
    "The second derivative of $L$ with respect to $o_j$ is:\n",
    "\n",
    "$$\\frac{\\partial^2 L}{\\partial o_j^2} = \\frac{\\partial}{\\partial o_j} (\\text{softmax}(o_j) - y_j) = \\frac{\\partial \\text{softmax}(o_j)}{\\partial o_j} = \\text{softmax}(o_j )(1 - \\text{softmax}(o_j))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb44f3c4-01d5-4580-b865-3a9f3e58c451",
   "metadata": {},
   "source": [
    "### Variance of the distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b259ea-a2cd-4dda-90aa-ce2c07d5a775",
   "metadata": {},
   "source": [
    "The distribution given by $softmax(𝑜)$ is actually a Bernoulli distribution with probabilities $p = softmax(o)$, so the variance is:\n",
    "$$Var[X] = E[X^2] - E[X]^2 = \\text{softmax}(o)(1 - \\text{softmax}(o))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563b57e8-2d8f-4cc1-94fe-7067fa0415c0",
   "metadata": {},
   "source": [
    "## 2. Assume that we have three classes which occur with equal probability, i.e., the probability vector is $(\\frac{1}{3},\\frac{1}{3},\\frac{1}{3})$\n",
    "* What is the problem if we try to design a binary code for it?\n",
    "* Can you design a better code? Hint: what happens if we try to encode two independent observations? What if we encode observations jointly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8a4140-5e47-48cf-aa49-6b2e2b33b50d",
   "metadata": {},
   "source": [
    "### 2.1 Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2210b07c-7018-4c3c-b88f-832af9162ca5",
   "metadata": {},
   "source": [
    "If we design a binary code directly for the three classes with equal probabilities, we might assign binary codes 00, 01, and 10 to the classes. However, this approach doesn't utilize the information about the equal probabilities effectively. It assigns different lengths to the codes, which could lead to suboptimal performance in certain scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecce63c9-cc0e-4961-9bce-e31affcb3653",
   "metadata": {},
   "source": [
    "### 2.2 Better code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56253ba-0ac5-4e08-8862-467e8072f856",
   "metadata": {},
   "source": [
    "We can use one-hot encode to express the jointly observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f451971-adba-42fb-9bef-6847759f270b",
   "metadata": {},
   "source": [
    "## 3. When encoding signals transmitted over a physical wire, engineers do not always use binary codes. For instance, PAM-3 uses three signal levels $\\{-1,0,1\\}$ as opposed to two levels $\\{0,1\\}$. How many ternary units do you need to transmit an integer in the range $\\{0,...,7\\}$? Why might this be a better idea in terms of electronics?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f21f5d-fd89-48c1-b756-c74341a99d56",
   "metadata": {
    "tags": []
   },
   "source": [
    "The number of ternary units needed can be found using the formula:\n",
    "$ \\text{Number of ternary units} = \\log_3 (\\text{Range}) + 1$\n",
    "\n",
    "So in this case:\n",
    "$\\text{Number of ternary units} = \\log_3 (7+1)  + 1 = \\log_3 8 + 1 = 2$\n",
    "\n",
    "This means that you would need just two ternary digit to represent integers in the range \\(\\{0, \\ldots, 7\\}\\) using the \\(-1, 0, 1\\) encoding.\n",
    "\n",
    "Using ternary encoding can be a better idea in terms of electronics for a few reasons:\n",
    "\n",
    "1. **Increased Information Density:** Ternary encoding allows you to represent more information in a single symbol compared to binary encoding. This means that you can transmit more data in the same amount of time.\n",
    "\n",
    "2. **Reduced Transmission Errors:** Ternary encoding with three signal levels (\\(-1, 0, 1\\)) can provide better noise immunity compared to binary encoding (\\(0, 1\\)). The presence of a zero level allows for better differentiation between signal states, reducing the likelihood of errors due to noise.\n",
    "\n",
    "3. **Simpler Hardware:** Ternary encoding can sometimes simplify hardware design. For instance, in differential signaling systems, where the difference between signal levels matters more than the absolute values, ternary encoding can offer benefits.\n",
    "4. **Easy Accomplish:** There are distinctive conditions: positive voltage, negative voltage, zero voltage in a physical wire which can be used as ternary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03b039e-e1af-44a1-9c91-2913a1edd69c",
   "metadata": {},
   "source": [
    "## 4. The Bradley–Terry model uses a logistic model to capture preferences. For a user to choose between apples and oranges one assumes scores $o_{apple}$ and $o_{orange}$. Our requirements are that larger scores should lead to a higher likelihood in choosing the associated item and that the item with the largest score is the most likely one to be chosen (Bradley and Terry, 1952).\n",
    "* Prove that softmax satisfies this requirement.\n",
    "* What happens if you want to allow for a default option of choosing neither apples nor oranges? Hint: now the user has three choices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bc290e-cb4f-41f0-8175-91e76be17f50",
   "metadata": {},
   "source": [
    "### 4.1 Prove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf46005-f6d0-4281-bcd6-d05185d2232c",
   "metadata": {},
   "source": [
    "In the Bradley-Terry model, the probability of item $i$ being chosen over item $j$ is given by:\n",
    "$$P(i > j) = \\frac{p_i}{p_i+p_j}$$\n",
    "as $p_i=\\text{softmax}(o_i) = \\frac{e^{o_i}}{\\sum_i e^{o_i}}$ and $p_i+p_j=1$, this can be simplified to:\n",
    "$$P(i>j)=p_i=softmax(o_i)= \\frac{e^{o_i}}{\\sum_i e^{o_i}}\\propto{e^{o_i}}\\propto{o_i}$$\n",
    "This show that larger scores lead to higher likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c227f68b-a7e7-4bc6-aca4-75986bd915d4",
   "metadata": {},
   "source": [
    "### 4.2 Three choices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03791989-372e-4fe3-8130-722280df5540",
   "metadata": {},
   "source": [
    "No matter how many choices we have, the probability of choosing item i is:\n",
    "$$p_i=softmax(o_i)= \\frac{e^{o_i}}{\\sum_i e^{o_i}}\\propto{e^{o_i}}\\propto{o_i}$$\n",
    "Item with the largest score is the most likely one to be chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6964c72b-372e-442c-8661-779679ba22e4",
   "metadata": {},
   "source": [
    "## 5. Softmax gets its name from the following mapping: $RealSoftMax(a,b)=\\log(\\exp(a)+\\exp(b))$. \n",
    "* Prove that $RealSoftMax(a,b)\\gt\\max(a,b)$\n",
    "* How small can you make the difference between both functions? Hint: without loss of generality you can set $b=0$ and $a\\geq{b}$\n",
    "* Prove that this holds for $\\lambda^{-1}RealSoftMax(\\lambda{a},\\lambda{b})$, provided that $\\lambda \\ge{0}$.\n",
    "* Show that for $\\lambda\\to\\infty$ we have $\\lambda^{-1}RealSoftMax(\\lambda{a},\\lambda{b})\\to\\max(a,b)$.\n",
    "* Construct an analogous softmin function.\n",
    "* Extend this to more than two numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ed3006-f299-4a04-b6bd-ebfc6c2fcda9",
   "metadata": {},
   "source": [
    "### 5.1 Proving $RealSoftMax(a,b) > \\max(a,b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e30a7e-66d4-4881-b181-42900e5147db",
   "metadata": {},
   "source": [
    "As:\n",
    "$$\\exp(a) + \\exp(b) > \\exp(\\max(a,b)) > 0$$\n",
    "and log is a monotonically increasing function, we can get:\n",
    "$$RealSoftMax(a,b)=log(\\exp(a) + \\exp(b))>\\max(a,b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0be5625-1590-4c65-aad2-0482df21f88f",
   "metadata": {},
   "source": [
    "## 5.2 Minimizing the Difference Between Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567d1e50-1c38-4180-8900-a65b4ecf3850",
   "metadata": {},
   "source": [
    "If we set $b = 0$ and \\(a \\geq b\\), the functions become:\n",
    "$$RealSoftMax(a, 0) = \\log(\\exp(a) + 1) $$\n",
    "$$\\max(a, 0) = a$$\n",
    "So the difference between the two functions is:\n",
    "$$\\text{diff(a)}=\\log(1+\\exp(-a))$$\n",
    "As $a$ increases, $\\text{diff}$ gets smaller, but it's never exactly zero because $\\exp(-a)$ will always be slightly larger than $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac37190-65cd-42d4-92df-0560ad6d4fd9",
   "metadata": {},
   "source": [
    "### 5.3 Proving the Property Holds for Scaled Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5600dca8-bbb5-4223-8b73-4fe0e8ae22ab",
   "metadata": {},
   "source": [
    "As $\\lambda\\gt0$:\n",
    "$$\\exp(\\lambda{a}) + \\exp(\\lambda{b}) > \\exp(\\lambda\\max(a,b)) > 0$$\n",
    "and log is a monotonically increasing function, we can get:\n",
    "$$RealSoftMax(a,b)=log(\\exp(\\lambda{a}) + \\exp(\\lambda{b}))>\\lambda\\max(a,b)$$\n",
    "so:\n",
    "$$\\lambda^{-1}RealSoftMax(a,b)>\\max(a,b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b4de82-0af8-43ed-9ae9-69bafd129ab0",
   "metadata": {},
   "source": [
    "### 5.4 Limit as $\\lambda\\to\\infty$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d31671-c4be-461d-8c01-75f505a55e54",
   "metadata": {},
   "source": [
    "If we set $b = 0$ and \\(a \\geq b\\), the functions become:\n",
    "$$\\lambda^{-1}RealSoftMax(\\lambda{a}, 0) = \\log(\\exp(\\lambda{a}) + 1) $$\n",
    "$$\\max(a, 0) = a$$\n",
    "So the difference between the two functions is:\n",
    "$$\\text{diff}(\\lambda)=\\lambda^{-1}\\log(1+\\exp(-\\lambda{a}))$$\n",
    "Using Lagrange's theorem:\n",
    "$$\\lim_{\\lambda\\to\\infty}\\text{diff}(\\lambda)=\\lim_{\\lambda\\to\\infty}\\frac{-1}{1+\\exp(\\lambda{a})}=0$$\n",
    "So we have $\\lambda^{-1}RealSoftMax(\\lambda{a},\\lambda{b})\\to\\max(a,b)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f00f72b-8e55-42d0-b16a-c779c1da6662",
   "metadata": {},
   "source": [
    "### 5.5. Analogous Softmin Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756a6438-4319-4f1b-bad8-04ff1cb771ed",
   "metadata": {},
   "source": [
    "An analogous function to the $RealSoftMax$ function can be defined as follows:\n",
    "$$RealSoftMin(a, b) = -\\log(\\exp(-a) + \\exp(-b)) $$\n",
    "\n",
    "This function captures the \"soft\" version of the minimum operation, where the logarithmic function is used to create a smooth transition between the two values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afc015b-c99a-4e8d-92da-081b85eeff15",
   "metadata": {},
   "source": [
    "### 5.6. Extension to More Than Two Numbers\n",
    "The concept can be extended to more than two numbers using a similar approach. Given numbers $a_1, a_2, \\ldots, a_n$, you can define the $RealSoftMax$ as:\n",
    "$$ RealSoftMax(a_1, a_2, \\ldots, a_n) = \\log(\\sum_{i=1}^{n} \\exp(a_i))$$\n",
    "\n",
    "This function smoothens the maximum operation over multiple numbers. An analogous function, \\(RealSoftMin\\), can also be defined for the \"soft\" version of the minimum operation over more than two numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591d7cfd-121f-4ca6-b103-8e5e44680937",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. The function $g(x) \\overset{\\mathrm{def}}{=}\\log\\sum{\\exp{x_i}}  $ is sometimes also referred to as the log-partition function.\n",
    "* Prove that the function is convex. Hint: to do so, use the fact that the first derivative amounts to the probabilities from the softmax function and show that the second derivative is the variance.\n",
    "* Show that $g$ is translation invariant, i.e.$g(x+b)=g(x)$.\n",
    "* What happens if some of the coordinates $x_i$ are very large? What happens if they’re all very small?\n",
    "* Show that if we choose $b=\\max_i{x_i}$ we end up with a numerically stable implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479e5a48-5c38-4e64-8d9a-b2e71d373c22",
   "metadata": {},
   "source": [
    "## 7. Assume that we have some probability distribution $P$. Suppose we pick another distribution $Q$ with $Q(i)\\propto P(i)^\\alpha$ for $\\alpha\\gt0$.\n",
    "* Which choice of $\\alpha$ corresponds to doubling the temperature? Which choice corresponds to halving it?\n",
    "* What happens if we let the temperature approach 0?\n",
    "* What happens if we let the temperature approach $\\infty$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9095a9b-2679-4a14-afe7-56c8e8f28e49",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:d2l]",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
