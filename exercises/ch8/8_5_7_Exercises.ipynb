{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92209e16-b6e9-406d-ab56-6f621ef32770",
   "metadata": {},
   "source": [
    "# 1. Should we remove the bias parameter from the fully connected layer or the convolutional layer before the batch normalization? Why?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c695e0b6-9e1b-45dd-b260-095222b81a34",
   "metadata": {},
   "source": [
    "Batch normalization is a technique that helps stabilize and accelerate the training of deep neural networks. It normalizes the activations within a layer by subtracting the mean and dividing by the standard deviation of the mini-batch. The normalized activations are then scaled and shifted using learnable parameters: the scale (gamma) and shift (beta) parameters.\n",
    "\n",
    "When using batch normalization, it's generally recommended to remove the bias parameter from the fully connected (linear) layer before applying batch normalization. This recommendation comes from the idea that the normalization process already includes shifting the activations with the beta parameter of batch normalization. Adding an additional bias term can introduce redundancy and might negatively impact the learning process.\n",
    "\n",
    "For convolutional layers, the use of bias parameters is less clear-cut. Whether to use bias parameters before batch normalization in convolutional layers depends on your specific use case and the design choices you are making.\n",
    "\n",
    "Here's a general guideline for both cases:\n",
    "\n",
    "1. **Fully Connected (Linear) Layer**:\n",
    "   - **Remove Bias**: It's recommended to remove the bias parameter from the fully connected layer before applying batch normalization. This helps avoid the potential redundancy between the bias and beta parameters of batch normalization.\n",
    "   \n",
    "2. **Convolutional Layer**:\n",
    "   - **With Bias**: Some architectures and setups use bias parameters in convolutional layers before batch normalization. The bias parameter can still provide flexibility in modeling, especially in the early stages of the network.\n",
    "   - **Without Bias**: If you decide to remove the bias parameter from convolutional layers before batch normalization, you're essentially letting batch normalization handle both the shifting and scaling of the activations.\n",
    "\n",
    "Remember that the effectiveness of these choices can also depend on the specific architecture, dataset, and optimization process you're using. It's often a good idea to experiment with different configurations to find the best setup for your particular use case.\n",
    "\n",
    "In summary, for fully connected layers, it's generally recommended to remove the bias parameter before applying batch normalization. For convolutional layers, you have some flexibility and can choose to include or exclude bias parameters based on your design choices and performance considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbed085e-e0fd-4b28-a5d6-5e13ca3c2cda",
   "metadata": {},
   "source": [
    "# 2. Compare the learning rates for LeNet with and without batch normalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97838c26-f8ad-4da4-9f0a-2bbda31a940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import warnings\n",
    "sys.path.append('/home/jovyan/work/d2l_solutions/notebooks/exercises/d2l_utils/')\n",
    "import d2l\n",
    "from torchsummary import summary\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class LeNet(d2l.Classifier):  #@save\n",
    "    \"\"\"The LeNet-5 model.\"\"\"\n",
    "    def __init__(self, lr=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(6, kernel_size=5, padding=2), nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(16, kernel_size=5), nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(120), nn.Sigmoid(),\n",
    "            nn.LazyLinear(84), nn.Sigmoid(),\n",
    "            nn.LazyLinear(num_classes))\n",
    "        \n",
    "class BNLeNet(d2l.Classifier):\n",
    "    def __init__(self, lr=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(6, kernel_size=5), nn.LazyBatchNorm2d(),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(16, kernel_size=5), nn.LazyBatchNorm2d(),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(), nn.LazyLinear(120), nn.LazyBatchNorm1d(),\n",
    "            nn.Sigmoid(), nn.LazyLinear(84), nn.LazyBatchNorm1d(),\n",
    "            nn.Sigmoid(), nn.LazyLinear(num_classes))\n",
    "        \n",
    "def stat_model_acc(model, data, plot_flag):\n",
    "    model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
    "    trainer = d2l.Trainer(max_epochs=10, num_gpus=1,plot_flag=plot_flag)\n",
    "    trainer.fit(model, data)\n",
    "    X,y = next(iter(data.get_dataloader(False)))\n",
    "    X = X.to('cuda')\n",
    "    y = y.to('cuda')\n",
    "    y_hat = model(X) \n",
    "    return model.accuracy(y_hat,y).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164ec23d-4c73-421b-9a8f-5c783d7b6ede",
   "metadata": {},
   "source": [
    "## 2.1 Plot the increase in validation accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4a52b56-ae32-4a0e-9c8c-a892920e4d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = d2l.FashionMNIST(batch_size=128)\n",
    "lr_list = [0.001,0.01,0.03,0.1,0.3]\n",
    "le_accs= []\n",
    "ble_accs = []\n",
    "for lr in lr_list[:1]:\n",
    "    le = LeNet(lr=lr)\n",
    "    ble = BNLeNet(lr=lr)\n",
    "    le_acc.append(stat_model_acc(le, data, False))\n",
    "    ble_acc.append(stat_model_acc(ble, data, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506c9e3f-59fa-4761-b3dc-d95088bac9eb",
   "metadata": {},
   "source": [
    "## 2.2 How large can you make the learning rate before the optimization fails in both cases?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebadf49-150e-4505-a426-c753d08492bf",
   "metadata": {},
   "source": [
    "# 3. Do we need batch normalization in every layer? Experiment with it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188a8e46-16a0-4438-9eb8-512a0ce32ea7",
   "metadata": {},
   "source": [
    "Whether to apply batch normalization in every layer of a neural network is not a strict rule but a design choice that depends on the specific problem, architecture, and training dynamics. The decision can impact the model's convergence, performance, and training stability. Here are some considerations to help you decide:\n",
    "\n",
    "**Advantages of Batch Normalization in Every Layer:**\n",
    "\n",
    "1. **Stabilized Training**: Applying batch normalization in every layer helps stabilize training by normalizing activations and reducing internal covariate shifts, which can lead to faster convergence and more stable gradient propagation.\n",
    "\n",
    "2. **Regularization**: Batch normalization has an inherent regularization effect, which can help prevent overfitting. Applying it in every layer might provide consistent regularization throughout the network.\n",
    "\n",
    "3. **Deeper Architectures**: For very deep networks, applying batch normalization in every layer can help mitigate gradient vanishing/exploding problems, enabling the training of even deeper models.\n",
    "\n",
    "4. **Less Sensitive to Initialization**: Batch normalization can reduce the sensitivity to weight initialization, allowing you to use larger learning rates and more aggressive optimization techniques.\n",
    "\n",
    "**Considerations Against Batch Normalization in Every Layer:**\n",
    "\n",
    "1. **Reduced Model Capacity**: Batch normalization can suppress the network's capacity to fit the training data. Applying it too frequently might lead to underfitting, especially in smaller models.\n",
    "\n",
    "2. **Slower Training**: Adding batch normalization to every layer increases computational overhead, which might slow down training, especially on hardware with limited resources.\n",
    "\n",
    "3. **Loss of Expressiveness**: Excessive normalization can remove useful information from activations, potentially limiting the model's expressiveness. It can also hinder the model's ability to memorize certain patterns, which could be desirable in some scenarios.\n",
    "\n",
    "4. **Unstable for Very Small Batches**: Batch normalization relies on batch statistics, which can be unstable for very small batches. In such cases, using batch normalization in every layer might lead to poor performance.\n",
    "\n",
    "**Guidelines and Best Practices:**\n",
    "\n",
    "1. **Experiment**: It's recommended to experiment with different configurations, including applying batch normalization selectively or in every layer. Test the impact on validation performance, convergence speed, and generalization.\n",
    "\n",
    "2. **Network Depth**: Deeper networks tend to benefit more from batch normalization in every layer due to the vanishing gradient problem. For shallower networks, you might achieve good results with selective application.\n",
    "\n",
    "3. **Use Validation**: Monitor validation performance during training to detect potential overfitting caused by excessive batch normalization.\n",
    "\n",
    "4. **Small Datasets**: For small datasets, you might need to be more cautious with normalization. Experiment with validation performance to find the right balance.\n",
    "\n",
    "5. **Different Architectures**: Different architectures might respond differently to batch normalization. What works for one architecture might not work optimally for another.\n",
    "\n",
    "In conclusion, while applying batch normalization in every layer can have benefits, it's important to consider the trade-offs and experiment with different configurations. The choice depends on your specific use case, the architecture of your model, the dataset, and computational constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb905a6-8c63-422e-9fe3-7bc224629b9a",
   "metadata": {},
   "source": [
    "# 4. Implement a “lite” version of batch normalization that only removes the mean, or alternatively one that only removes the variance. How does it behave?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc911b24-542b-485b-a3fd-265344481eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lite_batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum, mean_flag):\n",
    "    # Use is_grad_enabled to determine whether we are in training mode\n",
    "    if not torch.is_grad_enabled():\n",
    "        # In prediction mode, use mean and variance obtained by moving average\n",
    "        if mean_flag:\n",
    "            X_hat = X - moving_mean\n",
    "        else:\n",
    "            X_hat = X / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        if len(X.shape) == 2:\n",
    "            # When using a fully connected layer, calculate the mean and\n",
    "            # variance on the feature dimension\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean) ** 2).mean(dim=0)\n",
    "        else:\n",
    "            # When using a two-dimensional convolutional layer, calculate the\n",
    "            # mean and variance on the channel dimension (axis=1). Here we\n",
    "            # need to maintain the shape of X, so that the broadcasting\n",
    "            # operation can be carried out later\n",
    "            mean = X.mean(dim=(0, 2, 3), keepdim=True)\n",
    "            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
    "        # In training mode, the current mean and variance are used\n",
    "        if mean_flag:\n",
    "            X_hat = X - mean\n",
    "        else:\n",
    "            X_hat = X / torch.sqrt(moving_var + eps)\n",
    "        # Update the mean and variance using moving average\n",
    "        moving_mean = (1.0 - momentum) * moving_mean + momentum * mean\n",
    "        moving_var = (1.0 - momentum) * moving_var + momentum * var\n",
    "    Y = gamma * X_hat + beta  # Scale and shift\n",
    "    return Y, moving_mean.data, moving_var.data\n",
    "\n",
    "class LiteBatchNorm(nn.Module):\n",
    "    # num_features: the number of outputs for a fully connected layer or the\n",
    "    # number of output channels for a convolutional layer. num_dims: 2 for a\n",
    "    # fully connected layer and 4 for a convolutional layer\n",
    "    def __init__(self, num_features, num_dims, mean_flag=True):\n",
    "        super().__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # The scale parameter and the shift parameter (model parameters) are\n",
    "        # initialized to 1 and 0, respectively\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        # The variables that are not model parameters are initialized to 0 and\n",
    "        # 1\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.ones(shape)\n",
    "        self.mean_flag = mean_flag\n",
    "\n",
    "    def forward(self, X):\n",
    "        # If X is not on the main memory, copy moving_mean and moving_var to\n",
    "        # the device where X is located\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        # Save the updated moving_mean and moving_var\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
    "            X, self.gamma, self.beta, self.moving_mean,\n",
    "            self.moving_var, eps=1e-5, momentum=0.1, mean_flag=self.mean_flag)\n",
    "        return Y\n",
    "\n",
    "    \n",
    "class LiteBNLeNetScratch(d2l.Classifier):\n",
    "    def __init__(self, lr=0.1, num_classes=10, mean_flag=True):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(6, kernel_size=5), LiteBatchNorm(6, num_dims=4, mean_flag=mean_flag),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(16, kernel_size=5), LiteBatchNorm(16, num_dims=4, mean_flag=mean_flag),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(), nn.LazyLinear(120),\n",
    "            LiteBatchNorm(120, num_dims=2, mean_flag=mean_flag), nn.Sigmoid(), nn.LazyLinear(84),\n",
    "            LiteBatchNorm(84, num_dims=2, mean_flag=mean_flag), nn.Sigmoid(),\n",
    "            nn.LazyLinear(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ad894-af0e-4c70-bcbc-34a5005ddbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LiteBNLeNetScratch(lr=0.1)\n",
    "stat_model_acc(model, data, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898ee607-70a7-4b52-96e9-b70bf648dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LiteBNLeNetScratch(lr=0.1,mean_flag=False)\n",
    "stat_model_acc(model, data, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b638098-4683-4e2b-a8e5-919a26de04a5",
   "metadata": {},
   "source": [
    "# 5. Fix the parameters beta and gamma. Observe and analyze the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9289790-bbfc-4ecf-918d-99cc0c3f2d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedBatchNorm(nn.Module):\n",
    "    # num_features: the number of outputs for a fully connected layer or the\n",
    "    # number of output channels for a convolutional layer. num_dims: 2 for a\n",
    "    # fully connected layer and 4 for a convolutional layer\n",
    "    def __init__(self, num_features, num_dims, beta=None, gamma=None):\n",
    "        super().__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # The scale parameter and the shift parameter (model parameters) are\n",
    "        # initialized to 1 and 0, respectively\n",
    "        self.gamma = torch.ones(shape) if gamma is None else gamma\n",
    "        self.beta = torch.zeros(shape) if beta is None else beta\n",
    "        # The variables that are not model parameters are initialized to 0 and\n",
    "        # 1\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.ones(shape)\n",
    "        self.mean_flag = mean_flag\n",
    "\n",
    "    def forward(self, X):\n",
    "        # If X is not on the main memory, copy moving_mean and moving_var to\n",
    "        # the device where X is located\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        # Save the updated moving_mean and moving_var\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
    "            X, self.gamma, self.beta, self.moving_mean,\n",
    "            self.moving_var, eps=1e-5, momentum=0.1, mean_flag=self.mean_flag)\n",
    "        return Y\n",
    "\n",
    "    \n",
    "class FixedBNLeNetScratch(d2l.Classifier):\n",
    "    def __init__(self, lr=0.1, num_classes=10, beta=None, gamma=None):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(6, kernel_size=5), FixedBatchNorm(6, num_dims=4, beta=beta, gamma=gamma),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(16, kernel_size=5), FixedBatchNorm(16, num_dims=4, beta=beta, gamma=gamma),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(), nn.LazyLinear(120),\n",
    "            FixedBatchNorm(120, num_dims=2, beta=beta, gamma=gamma), nn.Sigmoid(), nn.LazyLinear(84),\n",
    "            FixedBatchNorm(84, num_dims=2, beta=beta, gamma=gamma), nn.Sigmoid(),\n",
    "            nn.LazyLinear(num_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1612e1ff-dba8-466d-b2f5-3b6164d9c8da",
   "metadata": {},
   "source": [
    "# 6. Can you replace dropout by batch normalization? How does the behavior change?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6d56d7-ccff-444f-9639-ae44d26c8a0e",
   "metadata": {},
   "source": [
    "Dropout and batch normalization are two different techniques used for regularization in neural networks. While they both aim to prevent overfitting, they operate in distinct ways. Dropout involves randomly dropping out units (neurons) during training, while batch normalization normalizes activations in each layer. They serve different purposes, and replacing one with the other may not yield the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39949507-0b5c-4797-85b2-9a93a687cf1b",
   "metadata": {},
   "source": [
    "# 7. Research ideas: think of other normalization transforms that you can apply:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c71969-715d-44b5-9548-56f842af7093",
   "metadata": {},
   "source": [
    "## 7.1 Can you apply the probability integral transform?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ac6c4516-3c67-4287-8568-31fdd8a184ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_sort_cdf(data):\n",
    "    sorts = []\n",
    "    cdfs = []\n",
    "    for i in range(data.shape[1]):\n",
    "        sort = np.sort(data[:, i])\n",
    "        cdf = np.arange(1, len(sort) + 1) / len(sort)\n",
    "        sorts.append(torch.tensor(sort).reshape(-1,1))\n",
    "        cdfs.append(torch.tensor(cdf).reshape(-1,1))\n",
    "    return torch.cat(sorts, dim=1), torch.cat(cdfs, dim=1)\n",
    "\n",
    "def pit_col(sorted_data, cdf_values, data):\n",
    "    # sorted_data = np.sort(org_data)\n",
    "    # cdf_values = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
    "    transformed_data = np.interp(data, sorted_data, cdf_values)\n",
    "    return \n",
    "\n",
    "def pit(sorted_data, cdf_values, data):\n",
    "    return torch.cat([pit_col(sorted_data[:,i], cdf_values[:,i], data[:, i]) for i in range(data.shape[1])], dim=1)\n",
    "\n",
    "def batch_pit_norm(X, gamma, beta, moving_sorted, moving_cdf, momentum):\n",
    "    # Use is_grad_enabled to determine whether we are in training mode\n",
    "    assert len(X.shape) in (2, 4)\n",
    "    shape  = X.shape\n",
    "    if len(shape) == 4:\n",
    "        X = torch.transpose(X,0,1).reshape(shape[1],-1)\n",
    "    if not torch.is_grad_enabled():\n",
    "        # In prediction mode, use mean and variance obtained by moving average\n",
    "        X_hat = pit(moving_sorted, moving_cdf, cdfs, X)\n",
    "    else:\n",
    "        sorts, cdfs = gen_sort_cdf(data)\n",
    "        X_hat = pit(sorts, cdfs, X)\n",
    "        moving_sorted = (1.0 - momentum) * moving_sorted + momentum * sorts\n",
    "        moving_cdf = (1.0 - momentum) * moving_cdf + momentum * cdfs\n",
    "    X_hat = X.reshape(shape)\n",
    "        # Update the mean and variance using moving average\n",
    "    Y = gamma * X_hat + beta  # Scale and shift\n",
    "    return Y, moving_sorted, moving_cdf\n",
    "\n",
    "class PitBatchNorm(nn.Module):\n",
    "    # num_features: the number of outputs for a fully connected layer or the\n",
    "    # number of output channels for a convolutional layer. num_dims: 2 for a\n",
    "    # fully connected layer and 4 for a convolutional layer\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super().__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # The scale parameter and the shift parameter (model parameters) are\n",
    "        # initialized to 1 and 0, respectively\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        # The variables that are not model parameters are initialized to 0 \n",
    "        self.moving_sorted = torch.zeros(shape)\n",
    "        self.moving_cdf = torch.zeros(shape)\n",
    "       \n",
    "    def forward(self, X):\n",
    "        # If X is not on the main memory, copy moving_mean and moving_var to\n",
    "        # the device where X is located\n",
    "        if self.moving_sorted.device != X.device:\n",
    "            self.moving_sorted = self.moving_sorted.to(X.device)\n",
    "            self.moving_cdf = self.moving_cdf.to(X.device)\n",
    "        # Save the updated moving_mean and moving_var\n",
    "        Y, self.moving_sorted, self.moving_cdf = batch_pit_norm(\n",
    "            X, self.gamma, self.beta, self.moving_sorted,\n",
    "            self.moving_cdf, momentum=0.1)\n",
    "        return Y\n",
    "\n",
    "    \n",
    "class PitBNLeNetScratch(d2l.Classifier):\n",
    "    def __init__(self, lr=0.1, num_classes=10, mean_flag=True):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(6, kernel_size=5), PitBatchNorm(6, num_dims=4),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(16, kernel_size=5), PitBatchNorm(6, num_dims=4),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(), nn.LazyLinear(120),\n",
    "            PitBatchNorm(6, num_dims=4), nn.Sigmoid(), nn.LazyLinear(84),\n",
    "            PitBatchNorm(6, num_dims=4), nn.Sigmoid(),\n",
    "            nn.LazyLinear(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cdaedec0-88ec-4c62-ba33-687d4dfb9fbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = PitBNLeNetScratch(lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310c4354-6212-4ce0-9335-c757c3a60be5",
   "metadata": {},
   "source": [
    "## 7.2 Can you use a full-rank covariance estimate? Why should you probably not do that?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8a2a4c5a-fb98-4de8-ad52-674ff0a035b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_frcov_norm(X, gamma, beta, moving_cov_matrix, momentum):\n",
    "    # Use is_grad_enabled to determine whether we are in training mode\n",
    "    assert len(X.shape) in (2, 4)\n",
    "    shape  = X.shape\n",
    "    if len(shape) == 4:\n",
    "        X = torch.transpose(X,0,1).reshape(shape[1],-1)\n",
    "    if not torch.is_grad_enabled():\n",
    "        # In prediction mode, use mean and variance obtained by moving average\n",
    "        eigenvalues, eigenvectors = torch.linalg.eig(moving_cov_matrix)\n",
    "        X_hat = X @ eigenvectors.type(torch.float32)\n",
    "    else:\n",
    "        centered_data = X - X.mean(dim=0)\n",
    "        cov_matrix = (centered_data.conj().T @ centered_data) / (X.shape[0] - 1)\n",
    "        eigenvalues, eigenvectors = torch.linalg.eig(cov_matrix)\n",
    "        X_hat = X @ eigenvectors.type(torch.float32)\n",
    "        moving_cov_matrix = (1.0 - momentum) * moving_cov_matrix + momentum * cov_matrix\n",
    "    X_hat = X.reshape(shape)\n",
    "        # Update the mean and variance using moving average\n",
    "    Y = gamma * X_hat + beta  # Scale and shift\n",
    "    return Y, moving_cov_matrix\n",
    "\n",
    "class FrcovBatchNorm(nn.Module):\n",
    "    # num_features: the number of outputs for a fully connected layer or the\n",
    "    # number of output channels for a convolutional layer. num_dims: 2 for a\n",
    "    # fully connected layer and 4 for a convolutional layer\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super().__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # The scale parameter and the shift parameter (model parameters) are\n",
    "        # initialized to 1 and 0, respectively\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        # The variables that are not model parameters are initialized to 0 \n",
    "        self.moving_cov_matrix = torch.zeros(shape)\n",
    "       \n",
    "    def forward(self, X):\n",
    "        # If X is not on the main memory, copy moving_mean and moving_var to\n",
    "        # the device where X is located\n",
    "        if self.moving_cov_matrix.device != X.device:\n",
    "            self.moving_cov_matrix = self.moving_cov_matrix.to(X.device)\n",
    "        # Save the updated moving_mean and moving_var\n",
    "        Y, self.moving_cov_matrix = batch_frcov_norm(\n",
    "            X, self.gamma, self.beta, self.moving_cov_matrix,\n",
    "            momentum=0.1)\n",
    "        return Y\n",
    "\n",
    "    \n",
    "class FrcovBNLeNetScratch(d2l.Classifier):\n",
    "    def __init__(self, lr=0.1, num_classes=10, mean_flag=True):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(6, kernel_size=5), FrcovBatchNorm(6, num_dims=4),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(16, kernel_size=5), FrcovBatchNorm(6, num_dims=4),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(), nn.LazyLinear(120),\n",
    "            FrcovBatchNorm(6, num_dims=4), nn.Sigmoid(), nn.LazyLinear(84),\n",
    "            FrcovBatchNorm(6, num_dims=4), nn.Sigmoid(),\n",
    "            nn.LazyLinear(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "bab3ac85-f5a9-46ce-9a19-8c44c662c077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = FrcovBNLeNetScratch(lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863afaaf-edeb-474d-85d8-1a182c6ffc8b",
   "metadata": {},
   "source": [
    "## 7.3 Can you use other compact matrix variants (block-diagonal, low-displacement rank, Monarch, etc.)?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51101ab-6202-49a6-ae2f-23eaa7838a89",
   "metadata": {},
   "source": [
    "## 7.4 Does a sparsification compression act as a regularizer?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061b6474-499f-4214-8b52-f901375c701a",
   "metadata": {},
   "source": [
    "## 7.5 Are there other projections (e.g., convex cone, symmetry group-specific transforms) that you can use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b49aad-9e8d-4c9a-9eba-ddebaebbccd2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:d2l]",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
