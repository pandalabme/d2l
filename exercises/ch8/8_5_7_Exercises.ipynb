{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92209e16-b6e9-406d-ab56-6f621ef32770",
   "metadata": {},
   "source": [
    "# 1. Should we remove the bias parameter from the fully connected layer or the convolutional layer before the batch normalization? Why?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbed085e-e0fd-4b28-a5d6-5e13ca3c2cda",
   "metadata": {},
   "source": [
    "# 2. Compare the learning rates for LeNet with and without batch normalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97838c26-f8ad-4da4-9f0a-2bbda31a940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(d2l.Classifier):  #@save\n",
    "    \"\"\"The LeNet-5 model.\"\"\"\n",
    "    def __init__(self, lr=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(6, kernel_size=5, padding=2), nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(16, kernel_size=5), nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(120), nn.Sigmoid(),\n",
    "            nn.LazyLinear(84), nn.Sigmoid(),\n",
    "            nn.LazyLinear(num_classes))\n",
    "        \n",
    "class BNLeNet(d2l.Classifier):\n",
    "    def __init__(self, lr=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(6, kernel_size=5), nn.LazyBatchNorm2d(),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(16, kernel_size=5), nn.LazyBatchNorm2d(),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(), nn.LazyLinear(120), nn.LazyBatchNorm1d(),\n",
    "            nn.Sigmoid(), nn.LazyLinear(84), nn.LazyBatchNorm1d(),\n",
    "            nn.Sigmoid(), nn.LazyLinear(num_classes))\n",
    "        \n",
    "def stat_model_acc(model, data, trainer):\n",
    "    model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
    "    trainer.fit(model, data)\n",
    "    X,y = next(iter(data.get_dataloader(False)))\n",
    "    X = X.to('cuda')\n",
    "    y = y.to('cuda')\n",
    "    y_hat = model(X) \n",
    "    return model.accuracy(y_hat,y).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a52b56-ae32-4a0e-9c8c-a892920e4d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = d2l.FashionMNIST(batch_size=128)\n",
    "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "lr_list = [0.001,0.01,0.03,0.1,0.3]\n",
    "model_list = [BNLeNet,LeNet]\n",
    "le_acs= []\n",
    "for lr in lr_list:\n",
    "    for model_class in model_list:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164ec23d-4c73-421b-9a8f-5c783d7b6ede",
   "metadata": {},
   "source": [
    "## 2.1 Plot the increase in validation accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506c9e3f-59fa-4761-b3dc-d95088bac9eb",
   "metadata": {},
   "source": [
    "## 2.2 How large can you make the learning rate before the optimization fails in both cases?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebadf49-150e-4505-a426-c753d08492bf",
   "metadata": {},
   "source": [
    "# 3. Do we need batch normalization in every layer? Experiment with it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb905a6-8c63-422e-9fe3-7bc224629b9a",
   "metadata": {},
   "source": [
    "# 4. Implement a “lite” version of batch normalization that only removes the mean, or alternatively one that only removes the variance. How does it behave?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b638098-4683-4e2b-a8e5-919a26de04a5",
   "metadata": {},
   "source": [
    "# 5. Fix the parameters beta and gamma. Observe and analyze the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1612e1ff-dba8-466d-b2f5-3b6164d9c8da",
   "metadata": {},
   "source": [
    "# 6. Can you replace dropout by batch normalization? How does the behavior change?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39949507-0b5c-4797-85b2-9a93a687cf1b",
   "metadata": {},
   "source": [
    "# 7. Research ideas: think of other normalization transforms that you can apply:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c71969-715d-44b5-9548-56f842af7093",
   "metadata": {},
   "source": [
    "## 7.1 Can you apply the probability integral transform?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310c4354-6212-4ce0-9335-c757c3a60be5",
   "metadata": {},
   "source": [
    "## 7.2 Can you use a full-rank covariance estimate? Why should you probably not do that?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863afaaf-edeb-474d-85d8-1a182c6ffc8b",
   "metadata": {},
   "source": [
    "## 7.3 Can you use other compact matrix variants (block-diagonal, low-displacement rank, Monarch, etc.)?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51101ab-6202-49a6-ae2f-23eaa7838a89",
   "metadata": {},
   "source": [
    "## 7.4 Does a sparsification compression act as a regularizer?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061b6474-499f-4214-8b52-f901375c701a",
   "metadata": {},
   "source": [
    "## 7.5 Are there other projections (e.g., convex cone, symmetry group-specific transforms) that you can use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b49aad-9e8d-4c9a-9eba-ddebaebbccd2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:d2l]",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
