{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92209e16-b6e9-406d-ab56-6f621ef32770",
   "metadata": {},
   "source": [
    "# 1. Should we remove the bias parameter from the fully connected layer or the convolutional layer before the batch normalization? Why?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c695e0b6-9e1b-45dd-b260-095222b81a34",
   "metadata": {},
   "source": [
    "Batch normalization is a technique that helps stabilize and accelerate the training of deep neural networks. It normalizes the activations within a layer by subtracting the mean and dividing by the standard deviation of the mini-batch. The normalized activations are then scaled and shifted using learnable parameters: the scale (gamma) and shift (beta) parameters.\n",
    "\n",
    "When using batch normalization, it's generally recommended to remove the bias parameter from the fully connected (linear) layer before applying batch normalization. This recommendation comes from the idea that the normalization process already includes shifting the activations with the beta parameter of batch normalization. Adding an additional bias term can introduce redundancy and might negatively impact the learning process.\n",
    "\n",
    "For convolutional layers, the use of bias parameters is less clear-cut. Whether to use bias parameters before batch normalization in convolutional layers depends on your specific use case and the design choices you are making.\n",
    "\n",
    "Here's a general guideline for both cases:\n",
    "\n",
    "1. **Fully Connected (Linear) Layer**:\n",
    "   - **Remove Bias**: It's recommended to remove the bias parameter from the fully connected layer before applying batch normalization. This helps avoid the potential redundancy between the bias and beta parameters of batch normalization.\n",
    "   \n",
    "2. **Convolutional Layer**:\n",
    "   - **With Bias**: Some architectures and setups use bias parameters in convolutional layers before batch normalization. The bias parameter can still provide flexibility in modeling, especially in the early stages of the network.\n",
    "   - **Without Bias**: If you decide to remove the bias parameter from convolutional layers before batch normalization, you're essentially letting batch normalization handle both the shifting and scaling of the activations.\n",
    "\n",
    "Remember that the effectiveness of these choices can also depend on the specific architecture, dataset, and optimization process you're using. It's often a good idea to experiment with different configurations to find the best setup for your particular use case.\n",
    "\n",
    "In summary, for fully connected layers, it's generally recommended to remove the bias parameter before applying batch normalization. For convolutional layers, you have some flexibility and can choose to include or exclude bias parameters based on your design choices and performance considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbed085e-e0fd-4b28-a5d6-5e13ca3c2cda",
   "metadata": {},
   "source": [
    "# 2. Compare the learning rates for LeNet with and without batch normalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97838c26-f8ad-4da4-9f0a-2bbda31a940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import warnings\n",
    "sys.path.append('/home/jovyan/work/d2l_solutions/notebooks/exercises/d2l_utils/')\n",
    "import d2l\n",
    "from torchsummary import summary\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class LeNet(d2l.Classifier):  #@save\n",
    "    \"\"\"The LeNet-5 model.\"\"\"\n",
    "    def __init__(self, lr=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(6, kernel_size=5, padding=2), nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(16, kernel_size=5), nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(120), nn.Sigmoid(),\n",
    "            nn.LazyLinear(84), nn.Sigmoid(),\n",
    "            nn.LazyLinear(num_classes))\n",
    "        \n",
    "class BNLeNet(d2l.Classifier):\n",
    "    def __init__(self, lr=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(6, kernel_size=5), nn.LazyBatchNorm2d(),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(16, kernel_size=5), nn.LazyBatchNorm2d(),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(), nn.LazyLinear(120), nn.LazyBatchNorm1d(),\n",
    "            nn.Sigmoid(), nn.LazyLinear(84), nn.LazyBatchNorm1d(),\n",
    "            nn.Sigmoid(), nn.LazyLinear(num_classes))\n",
    "        \n",
    "def stat_model_acc(model, data, plot_flag):\n",
    "    model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
    "    trainer = d2l.Trainer(max_epochs=10, num_gpus=1,plot_flag=plot_flag)\n",
    "    trainer.fit(model, data)\n",
    "    X,y = next(iter(data.get_dataloader(False)))\n",
    "    X = X.to('cuda')\n",
    "    y = y.to('cuda')\n",
    "    y_hat = model(X) \n",
    "    return model.accuracy(y_hat,y).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164ec23d-4c73-421b-9a8f-5c783d7b6ede",
   "metadata": {},
   "source": [
    "## 2.1 Plot the increase in validation accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b4a52b56-ae32-4a0e-9c8c-a892920e4d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = d2l.FashionMNIST(batch_size=128)\n",
    "lr_list = [0.001,0.01,0.03,0.1,0.3]\n",
    "le_accs= []\n",
    "ble_accs = []\n",
    "for lr in lr_list[:1]:\n",
    "    le = LeNet(lr=lr)\n",
    "    ble = BNLeNet(lr=lr)\n",
    "    le_accs.append(stat_model_acc(le, data, False))\n",
    "    ble_accs.append(stat_model_acc(ble, data, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506c9e3f-59fa-4761-b3dc-d95088bac9eb",
   "metadata": {},
   "source": [
    "## 2.2 How large can you make the learning rate before the optimization fails in both cases?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552ea620-a48f-45c5-ac4c-ee472420df92",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = d2l.FashionMNIST(batch_size=128)\n",
    "lr_list = [1,3,10,30]\n",
    "le_accs= []\n",
    "ble_accs = []\n",
    "for lr in lr_list:\n",
    "\tle = LeNet(lr=lr)\n",
    "\tble = BNLeNet(lr=lr)\n",
    "\tle_accs.append(stat_model_acc(le, data))\n",
    "\tble_accs.append(stat_model_acc(ble, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebadf49-150e-4505-a426-c753d08492bf",
   "metadata": {},
   "source": [
    "# 3. Do we need batch normalization in every layer? Experiment with it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188a8e46-16a0-4438-9eb8-512a0ce32ea7",
   "metadata": {},
   "source": [
    "Whether to apply batch normalization in every layer of a neural network is not a strict rule but a design choice that depends on the specific problem, architecture, and training dynamics. The decision can impact the model's convergence, performance, and training stability. Here are some considerations to help you decide:\n",
    "\n",
    "**Advantages of Batch Normalization in Every Layer:**\n",
    "\n",
    "1. **Stabilized Training**: Applying batch normalization in every layer helps stabilize training by normalizing activations and reducing internal covariate shifts, which can lead to faster convergence and more stable gradient propagation.\n",
    "\n",
    "2. **Regularization**: Batch normalization has an inherent regularization effect, which can help prevent overfitting. Applying it in every layer might provide consistent regularization throughout the network.\n",
    "\n",
    "3. **Deeper Architectures**: For very deep networks, applying batch normalization in every layer can help mitigate gradient vanishing/exploding problems, enabling the training of even deeper models.\n",
    "\n",
    "4. **Less Sensitive to Initialization**: Batch normalization can reduce the sensitivity to weight initialization, allowing you to use larger learning rates and more aggressive optimization techniques.\n",
    "\n",
    "**Considerations Against Batch Normalization in Every Layer:**\n",
    "\n",
    "1. **Reduced Model Capacity**: Batch normalization can suppress the network's capacity to fit the training data. Applying it too frequently might lead to underfitting, especially in smaller models.\n",
    "\n",
    "2. **Slower Training**: Adding batch normalization to every layer increases computational overhead, which might slow down training, especially on hardware with limited resources.\n",
    "\n",
    "3. **Loss of Expressiveness**: Excessive normalization can remove useful information from activations, potentially limiting the model's expressiveness. It can also hinder the model's ability to memorize certain patterns, which could be desirable in some scenarios.\n",
    "\n",
    "4. **Unstable for Very Small Batches**: Batch normalization relies on batch statistics, which can be unstable for very small batches. In such cases, using batch normalization in every layer might lead to poor performance.\n",
    "\n",
    "**Guidelines and Best Practices:**\n",
    "\n",
    "1. **Experiment**: It's recommended to experiment with different configurations, including applying batch normalization selectively or in every layer. Test the impact on validation performance, convergence speed, and generalization.\n",
    "\n",
    "2. **Network Depth**: Deeper networks tend to benefit more from batch normalization in every layer due to the vanishing gradient problem. For shallower networks, you might achieve good results with selective application.\n",
    "\n",
    "3. **Use Validation**: Monitor validation performance during training to detect potential overfitting caused by excessive batch normalization.\n",
    "\n",
    "4. **Small Datasets**: For small datasets, you might need to be more cautious with normalization. Experiment with validation performance to find the right balance.\n",
    "\n",
    "5. **Different Architectures**: Different architectures might respond differently to batch normalization. What works for one architecture might not work optimally for another.\n",
    "\n",
    "In conclusion, while applying batch normalization in every layer can have benefits, it's important to consider the trade-offs and experiment with different configurations. The choice depends on your specific use case, the architecture of your model, the dataset, and computational constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb905a6-8c63-422e-9fe3-7bc224629b9a",
   "metadata": {},
   "source": [
    "# 4. Implement a “lite” version of batch normalization that only removes the mean, or alternatively one that only removes the variance. How does it behave?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc911b24-542b-485b-a3fd-265344481eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lite_batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum, mean_flag):\n",
    "    # Use is_grad_enabled to determine whether we are in training mode\n",
    "    if not torch.is_grad_enabled():\n",
    "        # In prediction mode, use mean and variance obtained by moving average\n",
    "        if mean_flag:\n",
    "            X_hat = X - moving_mean\n",
    "        else:\n",
    "            X_hat = X / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        if len(X.shape) == 2:\n",
    "            # When using a fully connected layer, calculate the mean and\n",
    "            # variance on the feature dimension\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean) ** 2).mean(dim=0)\n",
    "        else:\n",
    "            # When using a two-dimensional convolutional layer, calculate the\n",
    "            # mean and variance on the channel dimension (axis=1). Here we\n",
    "            # need to maintain the shape of X, so that the broadcasting\n",
    "            # operation can be carried out later\n",
    "            mean = X.mean(dim=(0, 2, 3), keepdim=True)\n",
    "            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
    "        # In training mode, the current mean and variance are used\n",
    "        if mean_flag:\n",
    "            X_hat = X - mean\n",
    "        else:\n",
    "            X_hat = X / torch.sqrt(moving_var + eps)\n",
    "        # Update the mean and variance using moving average\n",
    "        moving_mean = (1.0 - momentum) * moving_mean + momentum * mean\n",
    "        moving_var = (1.0 - momentum) * moving_var + momentum * var\n",
    "    Y = gamma * X_hat + beta  # Scale and shift\n",
    "    return Y, moving_mean.data, moving_var.data\n",
    "\n",
    "class LiteBatchNorm(nn.Module):\n",
    "    # num_features: the number of outputs for a fully connected layer or the\n",
    "    # number of output channels for a convolutional layer. num_dims: 2 for a\n",
    "    # fully connected layer and 4 for a convolutional layer\n",
    "    def __init__(self, num_features, num_dims, mean_flag=True):\n",
    "        super().__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # The scale parameter and the shift parameter (model parameters) are\n",
    "        # initialized to 1 and 0, respectively\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        # The variables that are not model parameters are initialized to 0 and\n",
    "        # 1\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.ones(shape)\n",
    "        self.mean_flag = mean_flag\n",
    "\n",
    "    def forward(self, X):\n",
    "        # If X is not on the main memory, copy moving_mean and moving_var to\n",
    "        # the device where X is located\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        # Save the updated moving_mean and moving_var\n",
    "        Y, self.moving_mean, self.moving_var = lite_batch_norm(\n",
    "            X, self.gamma, self.beta, self.moving_mean,\n",
    "            self.moving_var, eps=1e-5, momentum=0.1, mean_flag=self.mean_flag)\n",
    "        return Y\n",
    "\n",
    "    \n",
    "class LiteBNLeNetScratch(d2l.Classifier):\n",
    "    def __init__(self, lr=0.1, num_classes=10, mean_flag=True):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(6, kernel_size=5), LiteBatchNorm(6, num_dims=4, mean_flag=mean_flag),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(16, kernel_size=5), LiteBatchNorm(16, num_dims=4, mean_flag=mean_flag),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(), nn.LazyLinear(120),\n",
    "            LiteBatchNorm(120, num_dims=2, mean_flag=mean_flag), nn.Sigmoid(), nn.LazyLinear(84),\n",
    "            LiteBatchNorm(84, num_dims=2, mean_flag=mean_flag), nn.Sigmoid(),\n",
    "            nn.LazyLinear(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ad894-af0e-4c70-bcbc-34a5005ddbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LiteBNLeNetScratch(lr=0.1)\n",
    "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
    "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "trainer.fit(model, data)\n",
    "X,y = next(iter(data.get_dataloader(False)))\n",
    "X = X.to('cuda')\n",
    "y = y.to('cuda')\n",
    "y_hat = model(X)\n",
    "model.accuracy(y_hat,y).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898ee607-70a7-4b52-96e9-b70bf648dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LiteBNLeNetScratch(lr=0.1,mean_flag=False)\n",
    "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
    "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "trainer.fit(model, data)\n",
    "X,y = next(iter(data.get_dataloader(False)))\n",
    "X = X.to('cuda')\n",
    "y = y.to('cuda')\n",
    "y_hat = model(X)\n",
    "model.accuracy(y_hat,y).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b638098-4683-4e2b-a8e5-919a26de04a5",
   "metadata": {},
   "source": [
    "# 5. Fix the parameters beta and gamma. Observe and analyze the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9289790-bbfc-4ecf-918d-99cc0c3f2d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedBatchNorm(nn.Module):\n",
    "    # num_features: the number of outputs for a fully connected layer or the\n",
    "    # number of output channels for a convolutional layer. num_dims: 2 for a\n",
    "    # fully connected layer and 4 for a convolutional layer\n",
    "    def __init__(self, num_features, num_dims, beta=None, gamma=None):\n",
    "        super().__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # The scale parameter and the shift parameter (model parameters) are\n",
    "        # initialized to 1 and 0, respectively\n",
    "        self.gamma = torch.ones(shape) if gamma is None else gamma\n",
    "        self.beta = torch.zeros(shape) if beta is None else beta\n",
    "        # The variables that are not model parameters are initialized to 0 and\n",
    "        # 1\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.ones(shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # If X is not on the main memory, copy moving_mean and moving_var to\n",
    "        # the device where X is located\n",
    "        # print(X.device)\n",
    "        # X = X.to('cuda')\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        # Save the updated moving_mean and moving_var\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
    "            X, self.gamma, self.beta, self.moving_mean,\n",
    "            self.moving_var, eps=1e-5, momentum=0.1)\n",
    "        return Y\n",
    "\n",
    "    \n",
    "class FixedBNLeNetScratch(d2l.Classifier):\n",
    "    def __init__(self, lr=0.1, num_classes=10, beta=None, gamma=None):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(6, kernel_size=5), FixedBatchNorm(6, num_dims=4, beta=beta, gamma=gamma),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(16, kernel_size=5), FixedBatchNorm(16, num_dims=4, beta=beta, gamma=gamma),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(), nn.LazyLinear(120),\n",
    "            FixedBatchNorm(120, num_dims=2, beta=beta, gamma=gamma), nn.Sigmoid(), nn.LazyLinear(84),\n",
    "            FixedBatchNorm(84, num_dims=2, beta=beta, gamma=gamma), nn.Sigmoid(),\n",
    "            nn.LazyLinear(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ead895a-a326-4235-a76f-54193ee1cd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FixedBNLeNetScratch(lr=0.1)\n",
    "# stat_model_acc(model, data)\n",
    "model = model.to('cuda')\n",
    "model.apply_init([next(iter(data.get_dataloader(True)))[0].to('cuda')], d2l.init_cnn)\n",
    "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "trainer.fit(model, data)\n",
    "X,y = next(iter(data.get_dataloader(False)))\n",
    "X = X.to('cuda')\n",
    "y = y.to('cuda')\n",
    "y_hat = model(X) \n",
    "model.accuracy(y_hat,y).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1612e1ff-dba8-466d-b2f5-3b6164d9c8da",
   "metadata": {},
   "source": [
    "# 6. Can you replace dropout by batch normalization? How does the behavior change?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6d56d7-ccff-444f-9639-ae44d26c8a0e",
   "metadata": {},
   "source": [
    "Dropout and batch normalization are two different techniques used for regularization in neural networks. While they both aim to prevent overfitting, they operate in distinct ways. Dropout involves randomly dropping out units (neurons) during training, while batch normalization normalizes activations in each layer. They serve different purposes, and replacing one with the other may not yield the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39949507-0b5c-4797-85b2-9a93a687cf1b",
   "metadata": {},
   "source": [
    "# 7. Research ideas: think of other normalization transforms that you can apply:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c71969-715d-44b5-9548-56f842af7093",
   "metadata": {},
   "source": [
    "## 7.1 Can you apply the probability integral transform?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ac6c4516-3c67-4287-8568-31fdd8a184ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def linear_interpolation(x, known_x, known_y):\n",
    "    # Find the indices for interpolation\n",
    "    lower_indices = torch.searchsorted(known_x, x, right=True).to('cuda') - 1\n",
    "    upper_indices = lower_indices + 1\n",
    "    max_indices = torch.ones(len(x)).to('cuda')*(len(known_y)-1)\n",
    "    upper_indices = torch.min(upper_indices, max_indices).type(torch.long)\n",
    "    # print('li:',known_y.device)\n",
    "    known_y = known_y.to('cuda')\n",
    "    # print(known_y.device,lower_indices.device)\n",
    "    # Get the corresponding known y values\n",
    "    lower_y = known_y[lower_indices]\n",
    "    upper_y = known_y[upper_indices]\n",
    "    \n",
    "    # Calculate the interpolation weights\n",
    "    weights = (x - known_x[lower_indices]) / (known_x[upper_indices] - known_x[lower_indices])\n",
    "    \n",
    "    # Perform linear interpolation\n",
    "    interpolated_y = lower_y + weights * (upper_y - lower_y)\n",
    "    return interpolated_y\n",
    "\n",
    "def gen_sort_cdf(data):\n",
    "    sorts = []\n",
    "    cdfs = []\n",
    "    for i in range(data.shape[1]):\n",
    "        sort,_ = torch.sort(data[:, i]) #.detach().numpy()\n",
    "        cdf = torch.arange(1, len(sort) + 1) / len(sort)\n",
    "        cdf = cdf.to('cuda')\n",
    "        sorts.append(sort.reshape(-1,1))\n",
    "        cdfs.append(cdf.reshape(-1,1))\n",
    "        # sorts.append(torch.tensor(sort).reshape(-1,1))\n",
    "        # cdfs.append(torch.tensor(cdf).reshape(-1,1))\n",
    "    return torch.cat(sorts, dim=1), torch.cat(cdfs, dim=1)\n",
    "\n",
    "def pit_col(sorted_data, cdf_values, data):\n",
    "    # sorted_data = np.sort(org_data)\n",
    "    # cdf_values = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
    "    transformed_data = linear_interpolation(data, sorted_data, cdf_values)\n",
    "    return transformed_data.reshape(-1,1)\n",
    "\n",
    "def pit(sorted_data, cdf_values, data):\n",
    "  return torch.cat([pit_col(sorted_data[:,i], cdf_values[:,i], data[:, i]) for i in range(data.shape[1])], dim=1)\n",
    "\n",
    "def batch_pit_norm(X, gamma, beta, moving_sorted, moving_cdf, momentum):\n",
    "    # Use is_grad_enabled to determine whether we are in training mode\n",
    "    assert len(X.shape) in (2, 4)\n",
    "    shape  = X.shape\n",
    "    # X = X.to('cuda')\n",
    "    # moving_sorted = moving_sorted.to('cuda')\n",
    "    if len(shape) == 4:\n",
    "        X = torch.transpose(X,0,1).reshape(shape[1],-1)\n",
    "    if not torch.is_grad_enabled():\n",
    "        # In prediction mode, use mean and variance obtained by moving average\n",
    "        X_hat = pit(moving_sorted, moving_cdf, cdfs, X)\n",
    "    else:\n",
    "        sorts, cdfs = gen_sort_cdf(X)\n",
    "        X_hat = pit(sorts, cdfs, X)\n",
    "        # print('sort',moving_sorted.device,sorts.device)\n",
    "        # print('cdf',moving_cdf.device,cdfs.device)\n",
    "        moving_sorted = (1.0 - momentum) * moving_sorted + momentum * sorts\n",
    "        moving_cdf = (1.0 - momentum) * moving_cdf + momentum * cdfs\n",
    "    X_hat = X.reshape(shape)\n",
    "        # Update the mean and variance using moving average\n",
    "    # print(gamma.device,X_hat.device,beta.device)\n",
    "    Y = gamma * X_hat + beta  # Scale and shift\n",
    "    return Y, moving_sorted, moving_cdf\n",
    "\n",
    "class PitBatchNorm(nn.Module):\n",
    "    # num_features: the number of outputs for a fully connected layer or the\n",
    "    # number of output channels for a convolutional layer. num_dims: 2 for a\n",
    "    # fully connected layer and 4 for a convolutional layer\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super().__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # The scale parameter and the shift parameter (model parameters) are\n",
    "        # initialized to 1 and 0, respectively\n",
    "        self.gamma = nn.Parameter(torch.ones(shape)).to('cuda')\n",
    "        self.beta = nn.Parameter(torch.zeros(shape)).to('cuda')\n",
    "        # The variables that are not model parameters are initialized to 0 \n",
    "        self.moving_sorted = torch.zeros(shape)\n",
    "        self.moving_cdf = torch.zeros(shape)\n",
    "       \n",
    "    def forward(self, X):\n",
    "        # If X is not on the main memory, copy moving_mean and moving_var to\n",
    "        # the device where X is located\n",
    "        X = X.to('cuda')\n",
    "        if self.moving_sorted.device != X.device:\n",
    "            self.moving_sorted = self.moving_sorted.to(X.device)\n",
    "            self.moving_cdf = self.moving_cdf.to(X.device)\n",
    "        # print(self.moving_sorted.device,self.moving_cdf.device)\n",
    "        # Save the updated moving_mean and moving_var\n",
    "        Y, self.moving_sorted, self.moving_cdf = batch_pit_norm(\n",
    "            X, self.gamma, self.beta, self.moving_sorted,\n",
    "            self.moving_cdf, momentum=0.1)\n",
    "        return Y\n",
    "\n",
    "    \n",
    "class PitBNLeNetScratch(d2l.Classifier):\n",
    "    def __init__(self, lr=0.1, num_classes=10, mean_flag=True):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(6, kernel_size=5), PitBatchNorm(6, num_dims=4),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(16, kernel_size=5), PitBatchNorm(16, num_dims=4),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(), nn.LazyLinear(120),\n",
    "            # nn.Sigmoid(), nn.LazyLinear(84),\n",
    "            PitBatchNorm(120, num_dims=2), nn.Sigmoid(), nn.LazyLinear(84),\n",
    "            # nn.Sigmoid(),\n",
    "            PitBatchNorm(84, num_dims=2), nn.Sigmoid(),\n",
    "            nn.LazyLinear(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cdaedec0-88ec-4c62-ba33-687d4dfb9fbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = PitBNLeNetScratch(lr=0.1)\n",
    "# stat_model_acc(model, data)\n",
    "model = model\n",
    "model = model.to('cuda')\n",
    "model.apply_init([next(iter(data.get_dataloader(True)))[0].to('cuda')], d2l.init_cnn)\n",
    "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "trainer.fit(model, data)\n",
    "X,y = next(iter(data.get_dataloader(False)))\n",
    "X = X.to('cuda')\n",
    "y = y.to('cuda')\n",
    "y_hat = model(X) \n",
    "model.accuracy(y_hat,y).item()\n",
    "# print(X.shape)\n",
    "# for m in model.net:\n",
    "#   X = m(X)\n",
    "#   print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310c4354-6212-4ce0-9335-c757c3a60be5",
   "metadata": {},
   "source": [
    "## 7.2 Can you use a full-rank covariance estimate? Why should you probably not do that?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477cfe7d-eebf-412b-aa7a-5b99e9689708",
   "metadata": {},
   "source": [
    "Using a full-rank covariance estimate instead of the standard normalization transform (mean and variance) in batch normalization is an interesting idea, but it's not typically recommended for several reasons:\n",
    "\n",
    "1. **Computational Complexity**: Computing the full-rank covariance matrix is more computationally expensive compared to calculating the mean and variance. The covariance matrix has quadratic complexity with respect to the input dimensions, while mean and variance calculations are linear. This added complexity can slow down training significantly.\n",
    "\n",
    "2. **Dimension Mismatch**: Batch normalization is applied independently to each channel (feature) in the input. When computing the covariance matrix, you would typically need to consider interactions between different channels, which could lead to a higher-dimensional covariance matrix. This might not work well for the normalization purposes of batch normalization.\n",
    "\n",
    "3. **Instability**: Computing the full-rank covariance matrix on small batch sizes or when the data has high dimensionality can lead to numerical instability and ill-conditioned covariance matrices. Regularization or other techniques might be needed to ensure numerical stability.\n",
    "\n",
    "4. **Overfitting**: Using a full-rank covariance matrix could introduce additional learnable parameters. This might lead to overfitting, especially if the network is not large enough or the dataset is not sufficiently diverse.\n",
    "\n",
    "5. **Loss of Orthogonality**: One of the benefits of batch normalization is that it maintains the orthogonality between the weight updates and the gradient updates during backpropagation. Introducing a full-rank covariance matrix might break this orthogonality, leading to slower convergence or training instability.\n",
    "\n",
    "6. **Normalization Properties**: Batch normalization is designed to normalize each channel's activations independently. A full-rank covariance estimate might introduce interdependencies between channels, which could disrupt the normalization properties.\n",
    "\n",
    "7. **Lack of Empirical Support**: The standard batch normalization approach using mean and variance has been empirically proven to work well across a wide range of network architectures and tasks. There's less evidence supporting the effectiveness of using a full-rank covariance estimate.\n",
    "\n",
    "In summary, while the idea of using a full-rank covariance matrix in batch normalization is intriguing, it's not commonly used due to the potential drawbacks in terms of computational complexity, instability, and the mismatch between batch normalization's design principles and the properties of a full-rank covariance matrix. The standard normalization transforms (mean and variance) have been well-tested and proven to be effective in stabilizing and accelerating the training of neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8a2a4c5a-fb98-4de8-ad52-674ff0a035b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_frcov_norm(X, gamma, beta, moving_cov_matrix, momentum):\n",
    "    # Use is_grad_enabled to determine whether we are in training mode\n",
    "    assert len(X.shape) in (2, 4)\n",
    "    shape  = X.shape\n",
    "    if len(shape) == 4:\n",
    "        X = torch.transpose(X,0,1).reshape(shape[1],-1)\n",
    "    if not torch.is_grad_enabled():\n",
    "        # In prediction mode, use mean and variance obtained by moving average\n",
    "        eigenvalues, eigenvectors = torch.linalg.eig(moving_cov_matrix)\n",
    "        X_hat = X @ eigenvectors.type(torch.float32)\n",
    "    else:\n",
    "        centered_data = X - X.mean(dim=0)\n",
    "        cov_matrix = (centered_data.conj().T @ centered_data) / (X.shape[0] - 1)\n",
    "        eigenvalues, eigenvectors = torch.linalg.eig(cov_matrix)\n",
    "        X_hat = X @ eigenvectors.type(torch.float32)\n",
    "        moving_cov_matrix = (1.0 - momentum) * moving_cov_matrix + momentum * cov_matrix\n",
    "    X_hat = X.reshape(shape)\n",
    "        # Update the mean and variance using moving average\n",
    "    Y = gamma * X_hat + beta  # Scale and shift\n",
    "    return Y, moving_cov_matrix\n",
    "\n",
    "class FrcovBatchNorm(nn.Module):\n",
    "    # num_features: the number of outputs for a fully connected layer or the\n",
    "    # number of output channels for a convolutional layer. num_dims: 2 for a\n",
    "    # fully connected layer and 4 for a convolutional layer\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super().__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # The scale parameter and the shift parameter (model parameters) are\n",
    "        # initialized to 1 and 0, respectively\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        # The variables that are not model parameters are initialized to 0 \n",
    "        self.moving_cov_matrix = torch.zeros(shape)\n",
    "       \n",
    "    def forward(self, X):\n",
    "        # If X is not on the main memory, copy moving_mean and moving_var to\n",
    "        # the device where X is located\n",
    "        if self.moving_cov_matrix.device != X.device:\n",
    "            self.moving_cov_matrix = self.moving_cov_matrix.to(X.device)\n",
    "        # Save the updated moving_mean and moving_var\n",
    "        Y, self.moving_cov_matrix = batch_frcov_norm(\n",
    "            X, self.gamma, self.beta, self.moving_cov_matrix,\n",
    "            momentum=0.1)\n",
    "        return Y\n",
    "\n",
    "    \n",
    "class FrcovBNLeNetScratch(d2l.Classifier):\n",
    "    def __init__(self, lr=0.1, num_classes=10, mean_flag=True):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(6, kernel_size=5), FrcovBatchNorm(6, num_dims=4),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(16, kernel_size=5), FrcovBatchNorm(6, num_dims=4),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(), nn.LazyLinear(120),\n",
    "            FrcovBatchNorm(6, num_dims=4), nn.Sigmoid(), nn.LazyLinear(84),\n",
    "            FrcovBatchNorm(6, num_dims=4), nn.Sigmoid(),\n",
    "            nn.LazyLinear(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "bab3ac85-f5a9-46ce-9a19-8c44c662c077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = FrcovBNLeNetScratch(lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863afaaf-edeb-474d-85d8-1a182c6ffc8b",
   "metadata": {},
   "source": [
    "## 7.3 Can you use other compact matrix variants (block-diagonal, low-displacement rank, Monarch, etc.)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e63144f2-7642-4559-b57c-09f96fbdaf15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_bdcov_norm(X, gamma, beta, moving_cov_matrix, momentum):\n",
    "    # Use is_grad_enabled to determine whether we are in training mode\n",
    "    assert len(X.shape) in (2, 4)\n",
    "    shape  = X.shape\n",
    "    if len(shape) == 4:\n",
    "        X = torch.transpose(X,0,1).reshape(shape[1],-1)\n",
    "    if not torch.is_grad_enabled():\n",
    "        # In prediction mode, use mean and variance obtained by moving average\n",
    "        diagonal_matrix = torch.diag_embed(moving_cov_matrix)\n",
    "        block_diagonal_matrix = torch.sum(diagonal_matrix, dim=0)\n",
    "        X_hat = X @ block_diagonal_matrix\n",
    "    else:\n",
    "        centered_data = X - X.mean(dim=0)\n",
    "        cov_matrix = (centered_data.conj().T @ centered_data) / (X.shape[0] - 1)\n",
    "        diagonal_matrix = torch.diag_embed(moving_cov_matrix)\n",
    "        block_diagonal_matrix = torch.sum(diagonal_matrix, dim=0)\n",
    "        X_hat = X @ block_diagonal_matrix\n",
    "        moving_cov_matrix = (1.0 - momentum) * moving_cov_matrix + momentum * cov_matrix\n",
    "    X_hat = X.reshape(shape)\n",
    "        # Update the mean and variance using moving average\n",
    "    Y = gamma * X_hat + beta  # Scale and shift\n",
    "    return Y, moving_cov_matrix\n",
    "\n",
    "class BdcovBatchNorm(nn.Module):\n",
    "    # num_features: the number of outputs for a fully connected layer or the\n",
    "    # number of output channels for a convolutional layer. num_dims: 2 for a\n",
    "    # fully connected layer and 4 for a convolutional layer\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super().__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # The scale parameter and the shift parameter (model parameters) are\n",
    "        # initialized to 1 and 0, respectively\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        # The variables that are not model parameters are initialized to 0 \n",
    "        self.moving_cov_matrix = torch.zeros(shape)\n",
    "       \n",
    "    def forward(self, X):\n",
    "        # If X is not on the main memory, copy moving_mean and moving_var to\n",
    "        # the device where X is located\n",
    "        if self.moving_cov_matrix.device != X.device:\n",
    "            self.moving_cov_matrix = self.moving_cov_matrix.to(X.device)\n",
    "        # Save the updated moving_mean and moving_var\n",
    "        Y, self.moving_cov_matrix = batch_bdcov_norm(\n",
    "            X, self.gamma, self.beta, self.moving_cov_matrix,\n",
    "            momentum=0.1)\n",
    "        return Y\n",
    "\n",
    "    \n",
    "class BdcovBNLeNetScratch(d2l.Classifier):\n",
    "    def __init__(self, lr=0.1, num_classes=10, mean_flag=True):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(6, kernel_size=5), BdcovBatchNorm(6, num_dims=4),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(16, kernel_size=5), BdcovBatchNorm(6, num_dims=4),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(), nn.LazyLinear(120),\n",
    "            BdcovBatchNorm(6, num_dims=4), nn.Sigmoid(), nn.LazyLinear(84),\n",
    "            BdcovBatchNorm(6, num_dims=4), nn.Sigmoid(),\n",
    "            nn.LazyLinear(num_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51101ab-6202-49a6-ae2f-23eaa7838a89",
   "metadata": {},
   "source": [
    "## 7.4 Does a sparsification compression act as a regularizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7686f5b4-87b9-4e83-b1b4-1561b18c9ea4",
   "metadata": {},
   "source": [
    "Yes, sparsification compression can act as a form of regularization in machine learning models. Sparsification refers to the process of converting certain weights or parameters in a model to zero, effectively creating a sparse representation. This process can have a regularizing effect on the model's learning process and can help prevent overfitting.\n",
    "\n",
    "Here's how sparsification compression can act as a regularizer:\n",
    "\n",
    "1. **Reduced Model Complexity**: Sparsification reduces the number of active parameters in the model, leading to a simpler model representation. This can help prevent the model from capturing noise in the training data and focusing on the most relevant features.\n",
    "\n",
    "2. **Prevention of Overfitting**: A sparse model is less likely to overfit the training data because it has fewer degrees of freedom. Overfitting occurs when a model becomes too complex and fits noise in the training data, leading to poor generalization to unseen data. Sparsification helps mitigate this by limiting the model's capacity to overfit.\n",
    "\n",
    "3. **Improved Generalization**: Regularization techniques like sparsification often lead to improved generalization performance. By encouraging the model to focus on the most informative features, the model becomes more robust and performs better on new, unseen data.\n",
    "\n",
    "4. **Interpretability**: Sparse models are often more interpretable because they highlight the most influential features. This can provide insights into which features are driving the model's decisions, making it easier to understand and debug.\n",
    "\n",
    "5. **Efficiency**: Sparse models are computationally more efficient, as they involve fewer computations during inference. This efficiency can be beneficial for deploying models to resource-constrained environments.\n",
    "\n",
    "It's important to note that while sparsification compression can provide regularization benefits, the degree of regularization depends on the extent of sparsity and the sparsification method used. Various techniques, such as L1 regularization (lasso), dropout, or techniques specific to neural network pruning, can be used to induce sparsity and act as regularization in different contexts. However, the exact regularization effect might vary based on the specific problem, dataset, and architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1b9eda-4a44-4997-91c5-2d07b50ec8a3",
   "metadata": {},
   "source": [
    "## 7.5 Are there other projections (e.g., convex cone, symmetry group-specific transforms) that you can use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ec8142-41bd-40e1-8f3e-d15d8fb513c0",
   "metadata": {},
   "source": [
    "Yes, there are several other types of projections and transforms that can be used in various mathematical and computational contexts. These projections are often used to achieve specific properties, structures, or constraints on data or mathematical objects. Here are a few examples:\n",
    "\n",
    "1. **Convex Cone Projection**: Convex cones are sets of vectors that are closed under linear combinations with non-negative coefficients. Projecting onto a convex cone involves finding the point in the cone that is closest to a given vector. This kind of projection is often used in optimization problems where the solution must satisfy certain constraints.\n",
    "\n",
    "2. **Symmetry Group-Specific Transforms**: In some applications, you might want to transform data or objects to respect specific symmetries. For example, in crystallography, Fourier transforms are used to reveal the symmetry of a crystal lattice. In image processing, you might use transforms that respect rotational or translational symmetries.\n",
    "\n",
    "3. **Orthogonal Projection**: Orthogonal projection involves finding the closest point in a subspace to a given vector. This type of projection is commonly used in linear algebra and optimization, where you might want to find the best approximation of a vector within a subspace.\n",
    "\n",
    "4. **Quantization**: Quantization is a projection-like operation used to map continuous values to a discrete set of values. It's often used in signal processing and data compression to reduce the number of possible values while minimizing information loss.\n",
    "\n",
    "5. **Manifold Embedding**: Manifold learning techniques aim to embed high-dimensional data into lower-dimensional spaces while preserving certain properties or structures. Techniques like Isomap, Locally Linear Embedding (LLE), and t-Distributed Stochastic Neighbor Embedding (t-SNE) are examples of such manifold embedding methods.\n",
    "\n",
    "6. **Orthogonal Procrustes Problem**: In linear algebra, the Orthogonal Procrustes Problem involves finding an orthogonal transformation (rotation and reflection) that best aligns two sets of points. It's often used in computer graphics, shape analysis, and alignment tasks.\n",
    "\n",
    "These are just a few examples of the many types of projections and transforms used in various fields. The choice of projection or transform depends on the problem at hand and the specific properties or constraints you want to achieve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:d2l]",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
