{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d81a4359-d6fb-4756-b75c-8bc56289848d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import sys\n",
    "sys.path.append('/home/jovyan/work/d2l_solutions/notebooks/exercises/d2l_utils/')\n",
    "import d2l\n",
    "\n",
    "\n",
    "def conv_block(num_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.LazyBatchNorm2d(), nn.ReLU(),\n",
    "        nn.LazyConv2d(num_channels, kernel_size=3, padding=1))\n",
    "\n",
    "def transition_block(num_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.LazyBatchNorm2d(), nn.ReLU(),\n",
    "        nn.LazyConv2d(num_channels, kernel_size=1),\n",
    "        nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, num_convs, num_channels):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        layer = []\n",
    "        for i in range(num_convs):\n",
    "            layer.append(conv_block(num_channels))\n",
    "        self.net = nn.Sequential(*layer)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for blk in self.net:\n",
    "            Y = blk(X)\n",
    "            # Concatenate input and output of each block along the channels\n",
    "            X = torch.cat((X, Y), dim=1)\n",
    "        return X\n",
    "    \n",
    "class DenseNet(d2l.Classifier):\n",
    "    def b1(self):\n",
    "        return nn.Sequential(\n",
    "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.LazyBatchNorm2d(), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    def __init__(self, num_channels=64, growth_rate=32, arch=(4, 4, 4, 4),\n",
    "                 lr=0.1, num_classes=10):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(self.b1())\n",
    "        for i, num_convs in enumerate(arch):\n",
    "            self.net.add_module(f'dense_blk{i+1}', DenseBlock(num_convs,\n",
    "                                                              growth_rate))\n",
    "            # The number of output channels in the previous dense block\n",
    "            num_channels += num_convs * growth_rate\n",
    "            # A transition layer that halves the number of channels is added\n",
    "            # between the dense blocks\n",
    "            if i != len(arch) - 1:\n",
    "                num_channels //= 2\n",
    "                self.net.add_module(f'tran_blk{i+1}', transition_block(\n",
    "                    num_channels))\n",
    "        self.net.add_module('last', nn.Sequential(\n",
    "            nn.LazyBatchNorm2d(), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
    "            nn.LazyLinear(num_classes)))\n",
    "        self.net.apply(d2l.init_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f3d2a8-0b65-435b-9a3c-3b2e6f5f79f3",
   "metadata": {},
   "source": [
    "# 1. Why do we use average pooling rather than max-pooling in the transition layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4419d1bb-cda9-4a21-96a7-b6f7000ee6af",
   "metadata": {},
   "source": [
    "In DenseNet architectures, transition layers are used to reduce the spatial dimensions (width and height) of feature maps while also reducing the number of feature maps (channels) before passing them to the next dense block. The choice between average pooling and max-pooling in transition layers depends on the design goals and the desired properties of the network. In DenseNet, average pooling is often preferred over max-pooling for several reasons:\n",
    "\n",
    "1. **Feature Retention**: Average pooling computes the average value of the elements in a pooling region. This retains more information about the features compared to max-pooling, which only selects the maximum value. In DenseNet, where information from all previous layers is concatenated together, average pooling helps in maintaining a more comprehensive representation of the features.\n",
    "\n",
    "2. **Smoothing Effect**: Average pooling has a smoothing effect on the output feature maps. This can help in reducing the risk of overfitting by preventing the network from becoming too sensitive to specific details in the data.\n",
    "\n",
    "3. **Stability**: Average pooling is less sensitive to outliers compared to max-pooling. This can make the network more robust to noise or variations in the input data.\n",
    "\n",
    "4. **Translation Invariance**: Average pooling provides a certain degree of translation invariance by taking into account the overall distribution of values in the pooling region. This can be beneficial in scenarios where small translations of the input should not significantly affect the output.\n",
    "\n",
    "5. **Information Sharing**: Average pooling promotes information sharing among neighboring pixels or units. This can help in capturing global patterns and structures present in the input data.\n",
    "\n",
    "While average pooling is preferred in transition layers, max-pooling can still have its own advantages in certain contexts. For example, in architectures like convolutional neural networks (CNNs) that prioritize capturing local features and enhancing feature maps, max-pooling can be effective. However, in DenseNet's context, where the emphasis is on maintaining rich information flow and reducing the risk of information loss, average pooling aligns better with the architecture's principles.\n",
    "\n",
    "Ultimately, the choice between average pooling and max-pooling depends on the specific goals of the network, the characteristics of the data, and the overall design philosophy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f878475-5b4d-4e95-9f61-2f632dc4c824",
   "metadata": {},
   "source": [
    "# 2. One of the advantages mentioned in the DenseNet paper is that its model parameters are smaller than those of ResNet. Why is this the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3229269-d43e-452b-8db7-35b0425b98e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11523338"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "data = d2l.FashionMNIST(batch_size=32, resize=(224, 224))\n",
    "arch18 = [(2,[(64,3,1)]*2,None),(2,[(128,3,1)]*2,128),(2,[(256,3,1)]*2,256),(2,[(512,3,1)]*2,512)]\n",
    "resnet18 = d2l.ResNet(arch=arch18, lr=0.01)\n",
    "resnet18.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
    "count_parameters(resnet18)\n",
    "# resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd8a4dc1-ffbc-43f8-a2f2-beb5fd8fc325",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "758226"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DenseNet(lr=0.01)\n",
    "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28122a78-6c52-4a42-8de0-645d1d0667bd",
   "metadata": {},
   "source": [
    "The reason why DenseNet has smaller model parameters than ResNet is because DenseNet uses **dense connections** between layers, which means that each layer receives the feature maps of all preceding layers as input and passes its own feature maps to all subsequent layers. This way, the number of channels (filters) in each layer can be reduced, since the layer can reuse the features from previous layers. ResNet, on the other hand, uses **residual connections**, which means that each layer only receives the output of the previous layer and adds it to its own output. This requires more channels in each layer to learn new features, since the layer cannot access the features from earlier layers. According to the DenseNet paper¹, a 121-layer DenseNet has 7.98 million parameters, while a 152-layer ResNet has 60.19 million parameters. This is a significant difference in model size and complexity.\n",
    "\n",
    "- (1) [1608.06993] Densely Connected Convolutional Networks - arXiv.org. https://arxiv.org/abs/1608.06993.\n",
    "- (2) DenseNet Explained | Papers With Code. https://paperswithcode.com/method/densenet.\n",
    "- (3) CVPR2017最佳论文DenseNet（一）原理分析 - 知乎 - 知乎专栏. https://zhuanlan.zhihu.com/p/93825208.\n",
    "- (4) Densely Connected Convolutional Networks - arXiv.org. https://arxiv.org/pdf/1608.06993.pdf.\n",
    "- (5) [1512.03385] Deep Residual Learning for Image Recognition - arXiv.org. https://arxiv.org/abs/1512.03385.\n",
    "- (6) ResNet Explained | Papers With Code. https://paperswithcode.com/method/resnet.\n",
    "- (7) arXiv:2110.00476v1 [cs.CV] 1 Oct 2021. https://arxiv.org/pdf/2110.00476.pdf.\n",
    "- (8) undefined. https://doi.org/10.48550/arXiv.1608.06993.\n",
    "- (9) undefined. https://doi.org/10.48550/arXiv.1512.03385."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eb9463-1ac7-46ca-b736-cd1f32922164",
   "metadata": {},
   "source": [
    "# 3. One problem for which DenseNet has been criticized is its high memory consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e024d971-a075-4eb9-8df8-d3526c45db52",
   "metadata": {},
   "source": [
    "## 3.1 Is this really the case? Try to change the input shape to $224\\times 224$ to compare the actual GPU memory consumption empirically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4c4bb3-4a25-42a9-b4b6-e33f3b7bdc0b",
   "metadata": {},
   "source": [
    "## 3.2 Can you think of an alternative means of reducing the memory consumption? How would you need to change the framework?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf265e43-e19f-4a7f-8fbd-415a110e5a82",
   "metadata": {},
   "source": [
    "Reducing memory consumption in a DenseNet architecture can be achieved through various strategies. One approach is to introduce sparsity into the model, which reduces the number of active connections and parameters. Here's how you might change the framework to achieve this:\n",
    "\n",
    "**1. Sparse Connectivity in Dense Blocks:**\n",
    "Instead of having fully connected dense blocks, you can introduce sparse connectivity patterns. This means that not every layer connects to every other layer in the dense block. You can achieve this by randomly selecting a subset of previous layers' feature maps to concatenate with the current layer. This reduces the number of connections and memory consumption.\n",
    "\n",
    "**2. Channel Pruning:**\n",
    "Apply channel pruning techniques to the dense blocks. You can identify less important channels and remove them from the concatenation operation. This effectively reduces the number of active channels and saves memory.\n",
    "\n",
    "**3. Regularization and Compression:**\n",
    "Introduce regularization techniques like L1 regularization during training to encourage certain weights to become exactly zero. Additionally, you can explore model compression methods like knowledge distillation or quantization to reduce the memory footprint of the model.\n",
    "\n",
    "**4. Low-Rank Approximations:**\n",
    "Perform low-rank matrix factorization on the weight matrices in the dense blocks. This technique approximates the weight matrices with lower-dimensional factors, leading to reduced memory usage.\n",
    "\n",
    "**5. Dynamic Allocation:**\n",
    "Allocate memory dynamically during inference to only store the necessary feature maps. This technique avoids allocating memory for feature maps that are no longer needed.\n",
    "\n",
    "**6. Sparsity-Inducing Activation Functions:**\n",
    "Use activation functions that naturally induce sparsity, such as the ReLU6 function, which caps activations at a maximum value and can lead to some neurons becoming inactive.\n",
    "\n",
    "**7. Adaptive Dense Blocks:**\n",
    "Design adaptive dense blocks that dynamically adjust their connectivity patterns based on the data distribution. For example, you can use attention mechanisms to determine which previous feature maps to concatenate based on their importance.\n",
    "\n",
    "Implementing these changes would require modifications to the architecture, training procedure, and potentially custom layers or modifications to existing layers. It's important to note that these techniques might involve a trade-off between memory reduction and model performance. It's recommended to experiment and fine-tune these strategies on your specific problem domain to find the right balance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74854d69-300d-4521-92c5-a5125886286e",
   "metadata": {},
   "source": [
    "# 4. Implement the various DenseNet versions presented in Table 1 of the DenseNet paper (Huang et al., 2017).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e3e07-d251-46bd-8570-12bb6ed14a02",
   "metadata": {},
   "source": [
    "# 5. Design an MLP-based model by applying the DenseNet idea. Apply it to the housing price prediction task in Section 5.7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024a9d81-6ef1-4eee-85ea-610cb3d09953",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c814befe-3586-4f80-85d9-f0e18c6f76cd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87785809-5609-460c-811d-55905fed78e6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23aa3415-4264-450a-8d4b-e6a88a3d25de",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:d2l]",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
