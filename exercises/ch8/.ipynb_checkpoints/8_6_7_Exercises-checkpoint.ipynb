{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1388e1cf-2977-4026-a9e3-62fd8791d6b5",
   "metadata": {},
   "source": [
    "# 1. What are the major differences between the Inception block in Fig. 8.4.1 and the residual block? How do they compare in terms of computation, accuracy, and the classes of functions they can describe?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded14ffa-5ac3-45c5-a5b6-eefcc5f1d6de",
   "metadata": {},
   "source": [
    "Inception blocks and residual blocks are two distinct architectural components commonly used in deep neural networks for various tasks, including image classification. Let's compare the major differences between the Inception block (also known as GoogLeNet inception module) and the residual block (used in ResNet architectures):\n",
    "\n",
    "**Inception Block (GoogLeNet Inception Module)**:\n",
    "The Inception block, introduced in the GoogLeNet architecture, is designed to capture features at multiple scales by using parallel convolutional layers of different sizes and pooling operations. It aims to create a rich hierarchy of features by combining information from different receptive fields. The major characteristics of the Inception block are:\n",
    "\n",
    "1. **Parallel Convolutions**: The Inception block contains multiple convolutional layers of different kernel sizes (e.g., 1x1, 3x3, 5x5). These parallel convolutions capture features of different scales and capture both fine and coarse details.\n",
    "\n",
    "2. **Pooling Operations**: The Inception block also includes pooling operations, such as max-pooling, which helps reduce spatial dimensions and capture translational invariance.\n",
    "\n",
    "3. **Concatenation**: The outputs of the parallel convolutions and pooling operations are concatenated along the channel dimension. This allows the network to capture a diverse set of features.\n",
    "\n",
    "**Residual Block**:\n",
    "The residual block, introduced in the ResNet architecture, is designed to address the vanishing gradient problem and enable the training of very deep networks. It introduces skip connections (also known as shortcut connections) that pass the input of a layer directly to a later layer. The major characteristics of the residual block are:\n",
    "\n",
    "1. **Skip Connections**: The residual block uses a skip connection that adds the original input (identity) to the output of the convolutional layers. This creates a \"residual\" or a \"shortcut\" path for information to flow directly through the network.\n",
    "\n",
    "2. **Identity Mapping**: The idea behind the residual block is that the model can learn to adjust the weights of the convolutional layers to make them represent the residual (the difference between the input and the output). This helps mitigate vanishing gradient issues.\n",
    "\n",
    "3. **Batch Normalization**: Residual blocks often include batch normalization after each convolutional layer. This helps stabilize and accelerate training.\n",
    "\n",
    "4. **Two-Path Learning**: The residual block essentially learns a residual transformation, which can be viewed as a combination of \"what to add\" (learned by convolutional layers) and \"what to keep\" (passed through the skip connection).\n",
    "\n",
    "In summary, the major difference between the Inception block and the residual block lies in their architectural goals and design principles. The Inception block focuses on capturing features at different scales using parallel operations, while the residual block focuses on enabling the training of very deep networks by introducing skip connections that facilitate the learning of residual transformations. Both architectural components have been instrumental in advancing the capabilities of deep neural networks for various tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b10a7c-cff4-4715-a38d-2a79d1b1857e",
   "metadata": {},
   "source": [
    "# 2. Refer to Table 1 in the ResNet paper (He et al., 2016) to implement different variants of the network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a9ee34b-3f03-4373-af95-87b247f4ecb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import sys\n",
    "sys.path.append('/home/jovyan/work/d2l_solutions/notebooks/exercises/d2l_utils/')\n",
    "import d2l\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, convs, conv_1x1_channel, strides=1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i,conv in enumerate(convs):\n",
    "            num_channels, kernel_size, padding = conv\n",
    "            conv_strides = 1 if i != 0 else strides\n",
    "            layers.append(nn.LazyConv2d(num_channels, kernel_size=3, padding=1, stride=conv_strides))\n",
    "            layers.append(nn.LazyBatchNorm2d())\n",
    "            layers.append(nn.ReLU())\n",
    "        self.net = nn.Sequential(*layers[:-1])\n",
    "        self.conv = None\n",
    "        if conv_1x1_channel:\n",
    "            self.conv = nn.LazyConv2d(conv_1x1_channel, kernel_size=1, stride=strides)\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        Y = self.net(X)\n",
    "        if self.conv:\n",
    "            X = self.conv(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)\n",
    "        \n",
    "class ResNet(d2l.Classifier):\n",
    "    def block(self, num_residuals, convs, conv_1x1_channel, first_block=False):\n",
    "        blk = []\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                blk.append(Residual(convs, conv_1x1_channel,strides=2))\n",
    "            else:\n",
    "                blk.append(Residual(convs, conv_1x1_channel))\n",
    "        return nn.Sequential(*blk)\n",
    "    \n",
    "    def __init__(self, arch, lr=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.LazyBatchNorm2d(), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "        for i, b in enumerate(arch):\n",
    "            self.net.add_module(f'b{i+2}', self.block(*b, first_block=(i==0)))\n",
    "        self.net.add_module('last', nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
    "            nn.LazyLinear(num_classes)))\n",
    "        self.net.apply(d2l.init_cnn)\n",
    "        \n",
    "def experiment(data, model):\n",
    "    model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
    "    trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "    trainer.fit(model, data)\n",
    "    X,y = next(iter(data.get_dataloader(False)))\n",
    "    X = X.to('cuda')\n",
    "    y = y.to('cuda')\n",
    "    y_hat = model(X)\n",
    "    return model.accuracy(y_hat,y).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "349c87ea-9b6b-4cd5-8744-3d5f5d8c04ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = d2l.FashionMNIST(batch_size=64, resize=(224, 224))\n",
    "arch18 = [(2,[(64,3,1)]*2,None),(2,[(128,3,1)]*2,None),(2,[(256,3,1)]*2,None),(2,[(512,3,1)]*2,None)]\n",
    "resnet18 = ResNet(arch=arch18, lr=0.01)\n",
    "# experiment(data, resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55f23818-2825-442a-8e53-44c1e5c69dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "arch34 = [(3,[(64,3,1)]*2,None),(4,[(128,3,1)]*2,None),(6,[(256,3,1)]*2,None),(3,[(512,3,1)]*2,None)]\n",
    "resnet34 = ResNet(arch=arch34, lr=0.01)\n",
    "# experiment(data, resnet34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1ab5149-8351-42f9-a0dd-47fdc7b2aeab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "arch50 = [(3,[(64,1,0),(64,3,1)],256),(4,[(128,1,0),(128,3,1)],512),(6,[(256,1,0),(256,3,1)],1024),(3,[(512,1,0),(512,3,1)],2048)]\n",
    "resnet50 = ResNet(arch=arch50, lr=0.01)\n",
    "# experiment(data, resnet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b44d7b29-26ab-43c6-8b3e-df413bafd7a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "arch101 = [(3,[(64,1,0),(64,3,1)],256),(4,[(128,1,0),(128,3,1)],512),(23,[(256,1,0),(256,3,1)],1024),(3,[(512,1,0),(512,3,1)],2048)]\n",
    "resnet101 = ResNet(arch=arch101, lr=0.01)\n",
    "# experiment(data, resnet101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d50ad70-df75-416f-b673-7149b8074f2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "arch152 = [(3,[(64,1,0),(64,3,1)],256),(8,[(128,1,0),(128,3,1)],512),(36,[(256,1,0),(256,3,1)],1024),(3,[(512,1,0),(512,3,1)],2048)]\n",
    "resnet152 = ResNet(arch=arch152, lr=0.01)\n",
    "# experiment(data, resnet152)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc603c7b-da0c-4fb1-9ebc-4d74a73cca81",
   "metadata": {},
   "source": [
    "# 3. For deeper networks, ResNet introduces a “bottleneck” architecture to reduce model complexity. Try to implement it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feac9db-5dd9-4919-8115-458b2faf261d",
   "metadata": {},
   "source": [
    "# 4. In subsequent versions of ResNet, the authors changed the “convolution, batch normalization, and activation” structure to the “batch normalization, activation, and convolution” structure. Make this improvement yourself. See Figure 1 in He et al. (2016) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c8fa53-7d9f-43aa-bed3-23232f0f7cda",
   "metadata": {},
   "source": [
    "# 5. Why can’t we just increase the complexity of functions without bound, even if the function classes are nested?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f2aaf8-a2fa-4e42-84da-b6629e41f61b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97c718e5-75fd-4012-856e-4af6ae3d269c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bdb9bc1-5746-4e9c-bae9-d423c206eef4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b24465ef-1258-4d28-8abd-4678c7e13911",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84de0066-1259-4ae3-a386-2b6b85efd254",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df94d23b-d100-4877-82b2-50a70735aff1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:d2l]",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
