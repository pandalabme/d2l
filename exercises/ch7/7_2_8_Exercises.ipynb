{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e092f1f8-8b39-4859-9fe5-744fc0a942e7",
   "metadata": {},
   "source": [
    "# 1. Construct an image X with diagonal edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26884bae-ff3b-47a6-be5d-dc1c0ca36b2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def corr2d(X,K):\n",
    "    h,w = K.shape\n",
    "    Y = torch.zeros(X.shape[0]-h+1, X.shape[1]-w+1)\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i+h, j:j+w]*K).sum()\n",
    "    return Y\n",
    "\n",
    "K = torch.tensor([[1.0,-1.0]])\n",
    "X = torch.eye(6,8)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebecd2d5-b4dc-461f-bd73-a69003993f24",
   "metadata": {},
   "source": [
    "## 1.1 What happens if you apply the kernel K in this section to it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b71bb54-14fd-40a9-b80b-7c1fd47f546f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [-1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0., -1.,  1.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0., -1.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., -1.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0., -1.,  1.,  0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0906b0b8-246c-44d5-b757-89d11fecd908",
   "metadata": {},
   "source": [
    "## 1.2 What happens if you transpose X?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b480fd36-3a9a-4b39-bac1-0e0737c819ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  0.,  0.,  0.,  0.],\n",
       "        [-1.,  1.,  0.,  0.,  0.],\n",
       "        [ 0., -1.,  1.,  0.,  0.],\n",
       "        [ 0.,  0., -1.,  1.,  0.],\n",
       "        [ 0.,  0.,  0., -1.,  1.],\n",
       "        [ 0.,  0.,  0.,  0., -1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X.T, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed7718b-5571-4671-8edc-2986f3b96590",
   "metadata": {},
   "source": [
    "## 1.3 What happens if you transpose K?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c25f175c-8607-415c-a90c-73c6e9010266",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  1., -1.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  1., -1.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1., -1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  1., -1.,  0.,  0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X, K.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f456cd-9892-4208-ba81-d235523d0e31",
   "metadata": {},
   "source": [
    "# 2. Design some kernels manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee31f43b-47a2-4cfd-b1d5-3e194b64689a",
   "metadata": {},
   "source": [
    "## 2.1 Given a directional vector $\\vec{v}=(v_1,v_2)$, derive an edge-detection kernel that detects edges orthogonal to $\\vec{v}$, i.e., edges in the direction $(v_2,-v_1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "ab5c4e6b-14a3-4e7c-8f11-c9c6c0329224",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "pad(input, pad, mode=\"constant\", value=None) -> Tensor\n",
       "\n",
       "Pads tensor.\n",
       "\n",
       "Padding size:\n",
       "    The padding size by which to pad some dimensions of :attr:`input`\n",
       "    are described starting from the last dimension and moving forward.\n",
       "    :math:`\\left\\lfloor\\frac{\\text{len(pad)}}{2}\\right\\rfloor` dimensions\n",
       "    of ``input`` will be padded.\n",
       "    For example, to pad only the last dimension of the input tensor, then\n",
       "    :attr:`pad` has the form\n",
       "    :math:`(\\text{padding\\_left}, \\text{padding\\_right})`;\n",
       "    to pad the last 2 dimensions of the input tensor, then use\n",
       "    :math:`(\\text{padding\\_left}, \\text{padding\\_right},`\n",
       "    :math:`\\text{padding\\_top}, \\text{padding\\_bottom})`;\n",
       "    to pad the last 3 dimensions, use\n",
       "    :math:`(\\text{padding\\_left}, \\text{padding\\_right},`\n",
       "    :math:`\\text{padding\\_top}, \\text{padding\\_bottom}`\n",
       "    :math:`\\text{padding\\_front}, \\text{padding\\_back})`.\n",
       "\n",
       "Padding mode:\n",
       "    See :class:`torch.nn.ConstantPad2d`, :class:`torch.nn.ReflectionPad2d`, and\n",
       "    :class:`torch.nn.ReplicationPad2d` for concrete examples on how each of the\n",
       "    padding modes works. Constant padding is implemented for arbitrary dimensions.\n",
       "    Replicate and reflection padding are implemented for padding the last 3\n",
       "    dimensions of a 4D or 5D input tensor, the last 2 dimensions of a 3D\n",
       "    or 4D input tensor, or the last dimension of a 2D or 3D input tensor.\n",
       "\n",
       "Note:\n",
       "    When using the CUDA backend, this operation may induce nondeterministic\n",
       "    behaviour in its backward pass that is not easily switched off.\n",
       "    Please see the notes on :doc:`/notes/randomness` for background.\n",
       "\n",
       "Args:\n",
       "    input (Tensor): N-dimensional tensor\n",
       "    pad (tuple): m-elements tuple, where\n",
       "        :math:`\\frac{m}{2} \\leq` input dimensions and :math:`m` is even.\n",
       "    mode: ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.\n",
       "        Default: ``'constant'``\n",
       "    value: fill value for ``'constant'`` padding. Default: ``0``\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> t4d = torch.empty(3, 3, 4, 2)\n",
       "    >>> p1d = (1, 1) # pad last dim by 1 on each side\n",
       "    >>> out = F.pad(t4d, p1d, \"constant\", 0)  # effectively zero padding\n",
       "    >>> print(out.size())\n",
       "    torch.Size([3, 3, 4, 4])\n",
       "    >>> p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)\n",
       "    >>> out = F.pad(t4d, p2d, \"constant\", 0)\n",
       "    >>> print(out.size())\n",
       "    torch.Size([3, 3, 8, 4])\n",
       "    >>> t4d = torch.empty(3, 3, 4, 2)\n",
       "    >>> p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)\n",
       "    >>> out = F.pad(t4d, p3d, \"constant\", 0)\n",
       "    >>> print(out.size())\n",
       "    torch.Size([3, 9, 7, 3])\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "F.pad??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "7473bceb-9735-4f26-a4c1-24467491d588",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.zeros(3, 3)\n",
    "F.pad(b,(2,2,2,2), \"constant\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "ecd0519f-3cb7-4ca6-b061-55271e43f86e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as tF\n",
    "def stat_theta(r):\n",
    "    v = [1,0]\n",
    "    theta = math.acos(F.cosine_similarity(torch.tensor(r).type(torch.float32),torch.tensor(v).type(torch.float32),dim=0))/math.pi*180\n",
    "    return theta\n",
    "\n",
    "def gen_K(v):\n",
    "    v = torch.tensor(v, dtype=torch.float32)  # Replace v1 and v2 with your values\n",
    "    u = v / torch.norm(v)\n",
    "    # Create the edge-detection kernel along the direction (v2, -v1)\n",
    "    K = torch.tensor([[-u[1], u[0]],[-u[0], -u[1]]], dtype=torch.float32)\n",
    "    return K\n",
    "\n",
    "def test(r,a):\n",
    "    theta = stat_theta(r)\n",
    "    K = gen_K(r)\n",
    "    print(K)\n",
    "    b = tF.rotate(a.reshape(1,1,a.shape[0],-1,),angle=theta).reshape(a.shape[0],-1)\n",
    "    print(b)\n",
    "    print(corr2d(b,K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "959dd02a-59a4-4d96-8f04-6e15585bd75b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.,  1.],\n",
      "        [-1., -0.]])\n",
      "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.]])\n",
      "tensor([[ 0., -1.,  0.,  0.,  0.,  1.,  0.],\n",
      "        [ 0., -1.,  0.,  0.,  0.,  1.,  0.],\n",
      "        [ 0., -1.,  0.,  0.,  0.,  1.,  0.],\n",
      "        [ 0., -1.,  0.,  0.,  0.,  1.,  0.],\n",
      "        [ 0., -1.,  0.,  0.,  0.,  1.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((6, 8))\n",
    "a[:, 2:6] = 0\n",
    "test([1,0],a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "92764c3f-3782-43f7-b3bc-99c3e8d57e12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.,  0.],\n",
      "        [-0., -1.]])\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 0.]])\n",
      "tensor([[ 0., -1., -1., -1., -1., -1., -1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1., -1., -1., -1., -1., -1.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "test([0,1],a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "c2ebc6bb-03cb-4c7b-95f8-3fca73faf68a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7071,  0.7071],\n",
      "        [-0.7071, -0.7071]])\n",
      "tensor([[0., 0., 0., 0., 1., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 1., 0., 0., 0., 0.]])\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.7071, -0.7071, -1.4142, -2.1213],\n",
      "        [-0.7071,  0.0000,  0.0000,  0.0000,  0.7071, -0.7071, -1.4142],\n",
      "        [-2.1213, -0.7071,  0.0000,  0.0000,  0.0000,  0.7071, -0.7071],\n",
      "        [-1.4142, -2.1213, -0.7071,  0.0000,  0.0000,  0.0000,  0.7071],\n",
      "        [-0.7071, -1.4142, -2.1213, -0.7071,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "test([1,1],a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349f8837-f468-4e46-a9ea-3e4309ed185b",
   "metadata": {},
   "source": [
    "## 2.2 Derive a finite difference operator for the second derivative. What is the minimum size of the convolutional kernel associated with it? Which structures in images respond most strongly to it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0f9b4f-be20-4b19-b42a-23c91afb8894",
   "metadata": {},
   "source": [
    "The second derivative of a continuous function can be approximated using a finite difference operator. One common way to do this is to use the central difference formula, which is given by:\n",
    "\n",
    "$$ \\frac{\\partial^2 f}{\\partial x^2} \\approx \\frac{f(x+h) - 2f(x) + f(x-h)}{h^2} $$\n",
    "\n",
    "Where $h$ is a small step size.\n",
    "\n",
    "To create a convolutional kernel associated with the second derivative, we can discretize the above formula and put it into a kernel format. The kernel would look like:\n",
    "$$\\text{kernel} = \\begin{bmatrix} 1 & -2 & 1 \\end{bmatrix}$$\n",
    "This kernel captures the second derivative along the horizontal direction. It's worth noting that the central difference formula can be applied in both horizontal and vertical directions separately to capture the second derivative along each direction.\n",
    "\n",
    "The minimum size of the convolutional kernel associated with the second derivative is $3 \\times 1$ or $1 \\times 3$. This size captures the essence of the central difference formula for the second derivative.\n",
    "\n",
    "Structures in images that have rapid intensity changes or sharp transitions will respond most strongly to this second derivative kernel. These structures include edges, corners, and other high-frequency features. The second derivative kernel enhances areas in the image where the intensity changes abruptly, making it a useful tool for edge detection and feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "5174669d-5409-4bb6-83b6-5598362e8500",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.,  1.,  0.,  0.,  1., -1.],\n",
       "        [-1.,  1.,  0.,  0.,  1., -1.],\n",
       "        [-1.,  1.,  0.,  0.,  1., -1.],\n",
       "        [-1.,  1.,  0.,  0.,  1., -1.],\n",
       "        [-1.,  1.,  0.,  0.,  1., -1.],\n",
       "        [-1.,  1.,  0.,  0.,  1., -1.]])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = torch.tensor([[1,-2,1]])\n",
    "print(a)\n",
    "corr2d(a,K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81484b65-ff3a-43e2-aa70-e5dbe11607eb",
   "metadata": {},
   "source": [
    "## 2.3 How would you design a blur kernel? Why might you want to use such a kernel?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddff753-df4d-420d-a145-f76f16dab949",
   "metadata": {},
   "source": [
    "Designing a blur kernel involves creating a convolutional kernel that, when applied to an image, reduces the high-frequency components in the image, resulting in a smoother and more blurred appearance. A commonly used blur kernel is the Gaussian kernel, which is derived from the Gaussian distribution. The Gaussian kernel has the property of spreading out the pixel values around the central pixel, creating a gradual transition between neighboring pixels.\n",
    "\n",
    "To design a Gaussian blur kernel, you typically follow these steps:\n",
    "\n",
    "1. Choose the size of the kernel: The size of the kernel determines the extent of blurring. A larger kernel size will result in more pronounced blurring.\n",
    "\n",
    "2. Determine the standard deviation (\\(\\sigma\\)): The standard deviation controls the spread of the Gaussian distribution. A larger \\(\\sigma\\) will result in a wider spread and more smoothing.\n",
    "\n",
    "3. Compute the Gaussian values: For each pixel in the kernel, compute the Gaussian value based on its distance from the center. The Gaussian values are then normalized to ensure that they sum up to 1.\n",
    "\n",
    "Here's an example of how you can create a 2D Gaussian blur kernel using Python and NumPy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "be44e440-43ff-46ac-9220-668e35b92f34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blur kernel sum:1.0\n",
      "tensor([[0.0751, 0.1238, 0.0751],\n",
      "        [0.1238, 0.2042, 0.1238],\n",
      "        [0.0751, 0.1238, 0.0751]], dtype=torch.float64)\n",
      "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.7259, 0.2741, 0.0000, 0.0000, 0.2741, 0.7259],\n",
       "        [0.7259, 0.2741, 0.0000, 0.0000, 0.2741, 0.7259],\n",
       "        [0.7259, 0.2741, 0.0000, 0.0000, 0.2741, 0.7259],\n",
       "        [0.7259, 0.2741, 0.0000, 0.0000, 0.2741, 0.7259]])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "def gaussian_kernel(size, sigma):\n",
    "    kernel = np.fromfunction(\n",
    "        lambda x, y: (1/(2*np.pi*sigma**2)) * np.exp(-((x-size//2)**2 + (y-size//2)**2) / (2*sigma**2)),\n",
    "        (size, size)\n",
    "    )\n",
    "    kernel /= np.sum(kernel)\n",
    "    return kernel\n",
    "\n",
    "kernel_size = 3\n",
    "sigma = 1.0\n",
    "blur_kernel = torch.tensor(gaussian_kernel(kernel_size, sigma))\n",
    "\n",
    "print(f\"Blur kernel sum:{blur_kernel.sum()}\")\n",
    "print(blur_kernel)\n",
    "print(a)\n",
    "corr2d(a,blur_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9eff06-e74c-4c62-b632-438f5d9e9c0e",
   "metadata": {},
   "source": [
    "Why might you want to use a blur kernel?\n",
    "\n",
    "1. **Noise Reduction**: Blurring can help reduce noise and unwanted artifacts in an image. High-frequency noise is often smoothed out, resulting in a cleaner appearance.\n",
    "\n",
    "2. **Image Smoothing**: Blurring is commonly used to smooth out textures and fine details in an image, creating a more cohesive and aesthetically pleasing result.\n",
    "\n",
    "3. **Edge Preservation**: While blurring reduces high-frequency details, certain blur techniques can preserve important edges while still providing a smoother overall appearance.\n",
    "\n",
    "4. **Preprocessing**: Blurring can be used as a preprocessing step for various computer vision tasks such as object detection and recognition, where the focus is on features rather than fine textures.\n",
    "\n",
    "5. **Privacy Protection**: Blurring or pixelating specific regions of an image can be used for privacy protection by making sensitive information less recognizable.\n",
    "\n",
    "6. **Artistic Effects**: Blurring can also be used creatively to achieve artistic effects or simulate depth of field in photography.\n",
    "\n",
    "Overall, blur kernels serve as a versatile tool in image processing with applications ranging from noise reduction to artistic manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebb9154-cd94-4554-b121-346ef78b60d5",
   "metadata": {},
   "source": [
    "## 2.4 What is the minimum size of a kernel to obtain a derivative of order $d$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3680d583-01dd-457d-89e9-05d8ec468ae9",
   "metadata": {},
   "source": [
    "need a kernel of size $2d+1$ along the direction in which we're calculating the derivative.\n",
    "\n",
    "One possible way to get the kernel size of k-order derivative in 1D is to use the finite difference approximation, which estimates the derivative of a function at a point by using the values of the function at nearby points. ยน For example, if we use the central difference formula to approximate the derivative, then we need a kernel of size 2k+1 to obtain a derivative of order k. This is because the central difference formula uses k points on each side of the center point to estimate the derivative. For instance, the first-order derivative can be approximated by using a kernel of size 3: $$\\frac{\\partial f}{\\partial x}(x)\\approx \\frac{f(x+1)-f(x-1)}{2}$$ The second-order derivative can be approximated by using a kernel of size 5: $$\\frac{\\partial^2 f}{\\partial x^2}(x)\\approx \\frac{f(x+2)-2f(x)+f(x-2)}{4}$$ And so on. However, if we use other types of kernels, such as Sobel or Laplace kernels, then we may need different sizes to obtain a derivative of order k. For example, the Sobel kernel can approximate the first-order derivative by using a kernel of size 3, but it cannot approximate the second-order derivative by using a single kernel. Instead, we need to apply the Sobel kernel twice or use another kernel, such as the Laplace kernel, which can approximate the second-order derivative by using a kernel of size 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5352cde1-11e5-4a5b-8ad2-066fcb425c95",
   "metadata": {},
   "source": [
    "# 3. When you try to automatically find the gradient for the Conv2D class we created, what kind of error message do you see?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce80ed2-6954-47ce-b001-9154dab5cdb5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39f41747-dbbf-4273-9d18-c2b37020afb4",
   "metadata": {},
   "source": [
    "# 4. How do you represent a cross-correlation operation as a matrix multiplication by changing the input and kernel tensors?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088f2be2-f9d5-40c1-8b55-9be31c0b0cf3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43b6ec03-50ec-4301-8025-e8a837dbe30c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eab9f66e-1437-4e25-b218-7aa5dd79a9d9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cf172d0-f13d-423c-80b8-aa621e86a30a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19348e9c-17ff-433c-a3ee-d010ec3180c1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e210f22-f69c-4dc2-bf94-6ec8e49b6947",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6219be62-71bd-4179-999f-9d4876d5fdc7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:d2l]",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
