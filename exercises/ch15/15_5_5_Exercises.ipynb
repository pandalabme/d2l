{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71aab2fe-785a-454b-9d1b-60da814913c3",
   "metadata": {},
   "source": [
    "# 1. If words $w_i$ and $w_j$ co-occur in the same context window, how can we use their distance in the text sequence to redesign the method for calculating the conditional probability $p_{ij}$? Hint: see Section 4.2 of the GloVe paper (Pennington et al., 2014)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e391c8-872a-4574-b0a7-82fb65492632",
   "metadata": {},
   "source": [
    "According to Section 4.2 of the GloVe paperÂ¹, if two words w_i and w_j co-occur in the same context window, we can use their distance d in the text sequence to redesign the method for calculating the conditional probability p_i(j). Specifically, we can assume that p_i(j) is inversely proportional to d, that is, p_i(j) = 1/d. This way, the closer the words are, the higher the conditional probability is, and the farther the words are, the lower the conditional probability is. This matches our intuition, because the closer the words are, the more likely they express related semantic information, and the farther the words are, the more likely they express unrelated semantic information.\n",
    "\n",
    "Using this method, we can write the logarithm of the conditional probability as:\n",
    "\n",
    "$$\n",
    "\\log p_i(j) = \\log \\frac{1}{d} = -\\log d\n",
    "$$\n",
    "\n",
    "Then, we can use this logarithm of the conditional probability as the objective function of the GloVe model, that is:\n",
    "\n",
    "$$\n",
    "J = \\sum_{i,j=1}^V f(x_{ij}) (\\log d + w_i^T \\tilde{w}_j + b_i + \\tilde{b}_j)^2\n",
    "$$\n",
    "\n",
    "where V is the size of the vocabulary, x_{ij} is the co-occurrence count of words w_i and w_j, f is a weighting function, w_i and \\tilde{w}_j are the word vectors of words w_i and w_j, b_i and \\tilde{b}_j are the bias terms of words w_i and w_j.\n",
    "\n",
    "> (1) GloV:https://nlp.stanford.edu/pubs/glove.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4382f49e-ed0a-4880-bf64-8c2d6fde965f",
   "metadata": {},
   "source": [
    "# 2. For any word, are its center word bias and context word bias mathematically equivalent in GloVe? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a5978a-6ee6-4f95-a13f-a968d01b7b76",
   "metadata": {},
   "source": [
    "According to the GloVe paper, the center word bias and the context word bias of any word are mathematically equivalent in the GloVe model. This is because the objective function of the GloVe model is symmetric with respect to the center word and the context word, and the optimal solution does not depend on the initialization of the word vectors and the bias terms. Therefore, the center word vector and the context word bias of any word can be interchanged without affecting the objective function value. However, in practice, due to different initialization values, the same word may still get different values in these two vectors after training. GloVe sums them up as the output vector."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:d2l]",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
