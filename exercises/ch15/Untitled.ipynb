{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "127bd377-c478-425a-8c91-74ae5a56034a",
   "metadata": {},
   "source": [
    "# 1. How can we sample noise words in negative sampling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2b9f9a-ffd0-4e25-8aca-71ca84420684",
   "metadata": {},
   "source": [
    "Negative sampling is a technique used in word embedding models, such as Word2Vec, to train word vectors efficiently. In negative sampling, you select a few \"negative\" words (i.e., words that are not the target word) to update the model's parameters along with the \"positive\" word (the actual target word). These negative samples help the model distinguish between the target word and other words in the vocabulary. The selection of negative samples is done probabilistically based on word frequency.\n",
    "\n",
    "Here's a high-level overview of how you can sample noise words (negative samples) in negative sampling:\n",
    "\n",
    "1. **Prepare a Word Frequency Distribution**: Calculate the frequency of each word in your training corpus. You can count the number of times each word appears in the corpus.\n",
    "\n",
    "2. **Calculate Probabilities**: Calculate the probabilities for each word to be selected as a negative sample. The probability of selecting a word as a negative sample is often proportional to its frequency. Common choices include unigram or unsmoothed probabilities.\n",
    "\n",
    "3. **Create a Noise Distribution**: Convert the probabilities into a noise distribution. This can be achieved by normalizing the probabilities so that they sum to 1, effectively creating a probability distribution over the entire vocabulary.\n",
    "\n",
    "4. **Sampling**: To sample a noise word during training, you can use techniques like the following:\n",
    "   \n",
    "   - **Uniform Sampling**: Generate a random number between 0 and 1 and select the word whose cumulative probability surpasses this random number. This approach ensures that words with higher probabilities are more likely to be selected.\n",
    "\n",
    "   - **Alias Sampling**: Alias sampling is a more efficient method for sampling based on probabilities. It precomputes alias tables to speed up the sampling process. Alias tables can be created based on the noise distribution.\n",
    "\n",
    "   - **Negative Sampling Table**: Another efficient approach is to create a negative sampling table. This table contains word indices, with each index repeated according to its probability. You can then randomly select an index from this table.\n",
    "\n",
    "5. **Update Model**: Once you have selected the negative samples, use them along with the positive sample (the actual target word) to update the model's parameters (e.g., word embeddings) through gradient descent.\n",
    "\n",
    "6. **Repeat**: Repeat this process for each training example and over multiple training iterations to optimize your word vectors.\n",
    "\n",
    "The choice of the number of negative samples (i.e., how many negative words to sample for each positive word) is typically a hyperparameter that you can tune. Too few negative samples may not provide enough contrastive information, while too many may slow down training.\n",
    "\n",
    "In practice, negative sampling has been widely used in word embedding models like Word2Vec to efficiently train word vectors, making them suitable for large-scale natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b05895c-b26a-489d-a553-4778f05c9aa7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise Samples: ['banana', 'date', 'apple', 'cherry', 'banana']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Example word frequencies (you can replace this with your actual data)\n",
    "word_frequencies = {\n",
    "    \"apple\": 10,\n",
    "    \"banana\": 5,\n",
    "    \"cherry\": 3,\n",
    "    \"date\": 2,\n",
    "    \"elderberry\": 1\n",
    "}\n",
    "\n",
    "# Step 1: Calculate the total frequency\n",
    "total_frequency = sum(word_frequencies.values())\n",
    "\n",
    "# Step 2: Calculate the probabilities\n",
    "word_probabilities = {word: freq / total_frequency for word, freq in word_frequencies.items()}\n",
    "\n",
    "# Step 3: Create a noise distribution\n",
    "def create_noise_distribution(probabilities):\n",
    "    # Create a list of words and their cumulative probabilities\n",
    "    cumulative_probabilities = []\n",
    "    cumulative_prob = 0\n",
    "    for word, prob in probabilities.items():\n",
    "        cumulative_prob += prob\n",
    "        cumulative_probabilities.append((word, cumulative_prob))\n",
    "    \n",
    "    # Return the list of words and cumulative probabilities\n",
    "    return cumulative_probabilities\n",
    "\n",
    "noise_distribution = create_noise_distribution(word_probabilities)\n",
    "\n",
    "# Step 4: Sampling from the noise distribution\n",
    "def sample_from_noise_distribution(noise_distribution):\n",
    "    # Generate a random number between 0 and 1\n",
    "    random_number = random.random()\n",
    "    \n",
    "    # Find the word whose cumulative probability surpasses the random number\n",
    "    for word, cumulative_prob in noise_distribution:\n",
    "        if random_number <= cumulative_prob:\n",
    "            return word\n",
    "\n",
    "# Sample some noise words\n",
    "num_samples = 5\n",
    "noise_samples = [sample_from_noise_distribution(noise_distribution) for _ in range(num_samples)]\n",
    "\n",
    "print(\"Noise Samples:\", noise_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77573edd-cc7b-441d-b9e0-82ccb852821e",
   "metadata": {},
   "source": [
    "# 2. Verify that $\\sum_{w\\in V}{P(w|w_c)}=1$ (15.2.9) holds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12df74a-4a93-4a5f-b30a-99a274395953",
   "metadata": {},
   "source": [
    "To verify that $\\sum_{w\\in V}{P(w|w_c)}=1$ in Hierarchical Softmax holds, we need to use the properties of the Huffman tree and the sigmoid function. \n",
    "\n",
    "First, we note that the Huffman tree is a full binary tree, which means that every non-leaf node has exactly two children. Therefore, for any non-leaf node $p_j^w$, the sum of the probabilities of its two children is equal to one, i.e.,\n",
    "\n",
    "$$\n",
    "p(0|\\mathbf{x}_w,\\theta_{j-1}^w) + p(1|\\mathbf{x}_w,\\theta_{j-1}^w) = 1\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}_w$ is the input vector and $\\theta_{j-1}^w$ is the parameter vector for node $p_j^w$. This follows from the definition of the sigmoid function:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "\n",
    "and its property:\n",
    "\n",
    "$$\n",
    "\\sigma(-x) = 1 - \\sigma(x)\n",
    "$$\n",
    "\n",
    "Second, we note that the probability of a word $w$ given the input vector $\\mathbf{x}_w$ is the product of the probabilities along the path from the root to the leaf node corresponding to $w$, i.e.,\n",
    "\n",
    "$$\n",
    "P(w|\\mathbf{x}_w) = \\prod_{j=2}^{l^w} p(d_j^w|\\mathbf{x}_w,\\theta_{j-1}^w)\n",
    "$$\n",
    "\n",
    "where $l^w$ is the length of the path, $d_j^w$ is the binary code of node $p_j^w$, and $p(d_j^w|\\mathbf{x}_w,\\theta_{j-1}^w)$ is either $\\sigma(\\mathbf{x}_w^\\top\\theta_{j-1}^w)$ or $1-\\sigma(\\mathbf{x}_w^\\top\\theta_{j-1}^w)$ depending on whether $d_j^w$ is 0 or 1.\n",
    "\n",
    "Now, we can prove that $\\sum_{w\\in V}{P(w|\\mathbf{x}_w)}=1$ by induction on the depth of the Huffman tree. \n",
    "\n",
    "Base case: If the depth of the tree is 1, then there are only two words in the vocabulary, and their probabilities are:\n",
    "\n",
    "$$\n",
    "P(w_1|\\mathbf{x}_{w_1}) = \\sigma(\\mathbf{x}_{w_1}^\\top\\theta_0^{w_1}) $$\n",
    "$$\n",
    "P(w_2|\\mathbf{x}_{w_2}) = 1 - \\sigma(\\mathbf{x}_{w_2}^\\top\\theta_0^{w_2})\n",
    "$$\n",
    "\n",
    "where $\\theta_0^{w_1}$ and $\\theta_0^{w_2}$ are the parameter vectors for the root node. Since $\\mathbf{x}_{w_1}$ and $\\mathbf{x}_{w_2}$ are arbitrary vectors, we can assume without loss of generality that $\\mathbf{x}_{w_1} = \\mathbf{x}_{w_2} = \\mathbf{x}$. Then, we have:\n",
    "\n",
    "$$\n",
    "\\sum_{w\\in V}{P(w|\\mathbf{x})} = \\sigma(\\mathbf{x}^\\top\\theta_0^{w_1}) + 1 - \\sigma(\\mathbf{x}^\\top\\theta_0^{w_2}) \\\\\n",
    "= \\sigma(\\mathbf{x}^\\top\\theta_0^{w_1}) + \\sigma(-\\mathbf{x}^\\top\\theta_0^{w_2}) \\\\\n",
    "= 1\n",
    "$$\n",
    "\n",
    "where we used the property of the sigmoid function in the last step.\n",
    "\n",
    "Inductive step: Suppose that for any Huffman tree with depth less than or equal to $k$, we have $\\sum_{w\\in V}{P(w|\\mathbf{x}_w)}=1$. Now, consider a Huffman tree with depth $k+1$. We can divide the vocabulary into two subsets, $V_L$ and $V_R$, corresponding to the left and right subtrees of the root node. Then, we have:\n",
    "\n",
    "$$\n",
    "\\sum_{w\\in V}{P(w|\\mathbf{x}_w)} = \\sum_{w\\in V_L}{P(w|\\mathbf{x}_w)} + \\sum_{w\\in V_R}{P(w|\\mathbf{x}_w)}\n",
    "$$\n",
    "\n",
    "By applying the definition of Hierarchical Softmax, we can rewrite each term as:\n",
    "\n",
    "$$\n",
    "\\sum_{w\\in V_L}{P(w|\\mathbf{x}_w)} = p(0|\\mathbf{x},\\theta_0) \\sum_{w\\in V_L}{P(w|\\mathbf{x}_w,p_0^w)} \\\\\n",
    "\\sum_{w\\in V_R}{P(w|\\mathbf{x}_w)} = p(1|\\mathbf{x},\\theta_0) \\sum_{w\\in V_R}{P(w|\\mathbf{x}_w,p_0^w)}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}$ is the input vector, $\\theta_0$ is the parameter vector for the root node, and $p_0^w$ is the root node itself. Note that $P(w|\\mathbf{x}_w,p_0^w)$ is the conditional probability of word $w$ given the input vector $\\mathbf{x}_w$ and the root node $p_0^w$, which is equivalent to the probability of word $w$ given the input vector $\\mathbf{x}_w$ in the subtree rooted at $p_0^w$. Since the depth of each subtree is less than or equal to $k$, we can apply the induction hypothesis and obtain:\n",
    "\n",
    "$$\n",
    "\\sum_{w\\in V_L}{P(w|\\mathbf{x}_w,p_0^w)} = 1 \\\\\n",
    "\\sum_{w\\in V_R}{P(w|\\mathbf{x}_w,p_0^w)} = 1\n",
    "$$\n",
    "\n",
    "Therefore, we have:\n",
    "\n",
    "$$\n",
    "\\sum_{w\\in V}{P(w|\\mathbf{x}_w)} = p(0|\\mathbf{x},\\theta_0) + p(1|\\mathbf{x},\\theta_0) \\\\\n",
    "= 1\n",
    "$$\n",
    "\n",
    "where we used the property of the Huffman tree in the last step.\n",
    "\n",
    "Hence, by induction, we have proved that $\\sum_{w\\in V}{P(w|\\mathbf{x}_w)}=1$ in Hierarchical Softmax holds for any Huffman tree with any depth.\n",
    "\n",
    "For more information about Hierarchical Softmax, you can refer to [Hierarchical Softmax Explained](^1^), [Hierarchical Softmax（层次Softmax）](^2^), or [hierarchical softmaxについて](^3^).\n",
    "\n",
    "- (1) Hierarchical Softmax Explained | Papers With Code. https://paperswithcode.com/method/hierarchical-softmax.\n",
    "- (2) Hierarchical Softmax（层次Softmax） - 知乎. https://zhuanlan.zhihu.com/p/56139075.\n",
    "- (3) hierarchical softmaxについて｜gota_morishita - note（ノート）. https://note.com/gota_morishita/n/nb9c9f126783d."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a07efd-b1c0-4046-8152-dcff55578926",
   "metadata": {},
   "source": [
    "# 3. How to train the continuous bag of words model using negative sampling and hierarchical softmax, respectively?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fe0680-efe7-4b22-b52f-303baac04687",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:d2l]",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
