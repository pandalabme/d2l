{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73e440ab-8b83-40a8-bb8f-a7ea44738cc7",
   "metadata": {},
   "source": [
    "# 1. Train a deeper Transformer in the experiments. How does it affect the training speed and the translation performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f65974-98ab-4203-9976-06ed39305295",
   "metadata": {},
   "source": [
    "# 2. Is it a good idea to replace scaled dot product attention with additive attention in the Transformer? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df258ffd-f3fa-40c6-89ba-38e0a8379697",
   "metadata": {},
   "source": [
    "# 3. For language modeling, should we use the Transformer encoder, decoder, or both? How would you design this method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2d2652-49f8-499b-a177-1ebe897f39e8",
   "metadata": {},
   "source": [
    "# 4. What challenges can Transformers face if input sequences are very long? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3e5b66-a95c-4b1b-8eb1-d649fcbe0392",
   "metadata": {},
   "source": [
    "# 5. How would you improve the computational and memory efficiency of Transformers? Hint: you may refer to the survey paper by Tay et al. (2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ced8b5e-f6f5-4d56-9424-32cf33fc530f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d228e861-7a11-4205-af64-e010fc940732",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05189120-897d-4439-98c5-0dfe63fd1c41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import warnings\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "sys.path.append('/home/jovyan/work/d2l_solutions/notebooks/exercises/d2l_utils/')\n",
    "import d2l\n",
    "from torchsummary import summary\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class PositionWiseFFN(nn.Module):  #@save\n",
    "    \"\"\"The positionwise feed-forward network.\"\"\"\n",
    "    def __init__(self, ffn_num_hiddens, ffn_num_outputs):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.LazyLinear(ffn_num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.LazyLinear(ffn_num_outputs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dense2(self.relu(self.dense1(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c45f27f9-3dd0-48bb-9008-1bd99d257b11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3717,  0.3617,  0.5549,  0.0816, -0.1657, -0.2510, -0.0093, -0.0123],\n",
       "        [ 0.3717,  0.3617,  0.5549,  0.0816, -0.1657, -0.2510, -0.0093, -0.0123],\n",
       "        [ 0.3717,  0.3617,  0.5549,  0.0816, -0.1657, -0.2510, -0.0093, -0.0123]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn = PositionWiseFFN(4, 8)\n",
    "ffn.eval()\n",
    "ffn(torch.ones((2, 3, 4)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45c85141-a114-46ad-ada7-17dc4340fc52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn.dense1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af541c76-6b97-4120-ba6f-ca23982f1638",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3717,  0.3617,  0.5549,  0.0816, -0.1657, -0.2510, -0.0093, -0.0123],\n",
       "        [ 0.3717,  0.3617,  0.5549,  0.0816, -0.1657, -0.2510, -0.0093, -0.0123],\n",
       "        [ 0.3717,  0.3617,  0.5549,  0.0816, -0.1657, -0.2510, -0.0093, -0.0123]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn(torch.ones((2, 3, 4)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a583e8e2-cbff-49f3-baff-0c8a85fc2aa9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3717,  0.3617,  0.5549,  0.0816, -0.1657, -0.2510, -0.0093, -0.0123]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn(torch.ones((2, 1, 4)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2202aaa-44d6-4a4a-82e0-25a453efd688",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3717,  0.3617,  0.5549,  0.0816, -0.1657, -0.2510, -0.0093, -0.0123]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn(torch.ones((2,3, 1, 4)))[0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:d2l]",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
