{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235c3e76-1045-435e-9a11-73d7d8e5a0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to markdown 10_7_10_Exercises.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2893f92-c73d-447f-a308-3fb523cf0229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import warnings\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "sys.path.append('/home/jovyan/work/d2l_solutions/notebooks/exercises/d2l_utils/')\n",
    "import d2l\n",
    "from torchsummary import summary\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da841372-d262-4054-ac86-f2ff38875b27",
   "metadata": {},
   "source": [
    "# 1. Suppose that you wanted to reimplement approximate (key, query) matches as used in classical databases, which attention function would you pick?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c4089d-bafb-4463-8a19-960ce46e35c3",
   "metadata": {},
   "source": [
    "One possible attention function that can be used to implement approximate (key, query) matches is the **scaled dot-product attention**¹. This function computes the similarity between the query and each key by taking the dot product and scaling it by the square root of the key dimension. Then, it applies a softmax function to obtain the attention weights, which are used to compute a weighted sum of the values. This function can handle queries and keys of different lengths, and it can learn to attend to the most relevant parts of the keys for each query. \n",
    "\n",
    "The scaled dot-product attention function can be defined as follows:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "where $Q$ is a matrix of queries, $K$ is a matrix of keys, $V$ is a matrix of values, and $d_k$ is the dimension of the keys.\n",
    "\n",
    "- (1) What exactly are keys, queries, and values in attention .... https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms.\n",
    "- (2) 11.1. Queries, Keys, and Values — Dive into Deep Learning 1 .... https://d2l.ai/chapter_attention-mechanisms-and-transformers/queries-keys-values.html.\n",
    "- (3) 注意力,多头注意力,自注意力及Pytorch实现 - 知乎. https://zhuanlan.zhihu.com/p/366592542.\n",
    "- (4) Alignment Attention by Matching Key and Query Distributions. https://arxiv.org/abs/2110.12567."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eed01c0-dbd3-48ef-b767-a62358e36992",
   "metadata": {},
   "source": [
    "# 2. Suppose that the attention function is given by $\\alpha(q,k_i)=q^Tk_i$ and that $k_i=v_i$ for $i=1,\\dots m,$. Denote by $p(k_i;q)$ the probability distribution over keys when using the softmax normalization in (11.1.3). Prove that $\\nabla_qAttention(q,D)=Cov_{p(k_i;q)}[k_i]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefbd1c7-95a6-4bf9-aeb5-f58d24224605",
   "metadata": {},
   "source": [
    "First, let us recall the definition of the attention function and the softmax normalization from (11.1.3):\n",
    "\n",
    "$$\n",
    "\\text{Attention}(q,D) = \\sum_{i=1}^m \\alpha(q,k_i)v_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(k_i;q) = \\frac{\\exp(\\alpha(q,k_i))}{\\sum_{j=1}^m \\exp(\\alpha(q,k_j))}\n",
    "$$\n",
    "\n",
    "where $q$ is the query vector, $D = \\{(k_1,v_1),\\dots,(k_m,v_m)\\}$ is the set of key-value pairs, and $\\alpha(q,k_i)$ is the similarity score between $q$ and $k_i$.\n",
    "\n",
    "In this question, we assume that $\\alpha(q,k_i)=q^Tk_i$ and that $k_i=v_i$ for $i=1,\\dots m,$. Therefore, we have:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(q,D) = \\sum_{i=1}^m q^Tk_iv_i = \\sum_{i=1}^m (q^Tk_i)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(k_i;q) = \\frac{\\exp(q^Tk_i)}{\\sum_{j=1}^m \\exp(q^Tk_j)}\n",
    "$$\n",
    "\n",
    "To prove that $\\nabla_qAttention(q,D)=Cov_{p(k_i;q)}[k_i]$, we need to compute the gradient of the attention function with respect to $q$ and compare it with the covariance of $k_i$ under the probability distribution $p(k_i;q)$.\n",
    "\n",
    "The gradient of the attention function with respect to $q$ is:\n",
    "\n",
    "$$\n",
    "\\nabla_qAttention(q,D) = \\nabla_q\\left(\\sum_{i=1}^m (q^Tk_i)^2\\right) = 2\\sum_{i=1}^m (q^Tk_i)k_i\n",
    "$$\n",
    "\n",
    "The covariance of $k_i$ under the probability distribution $p(k_i;q)$ is:\n",
    "\n",
    "$$\n",
    "Cov_{p(k_i;q)}[k_i] = E_{p(k_i;q)}[k_ik_i^T] - E_{p(k_i;q)}[k_i]E_{p(k_i;q)}[k_i]^T\n",
    "$$\n",
    "\n",
    "where $E_{p(k_i;q)}[k_ik_i^T]$ is the expected value of the outer product of $k_i$ and $E_{p(k_i;q)}[k_i]$ is the expected value of $k_i$. We can compute these values as follows:\n",
    "\n",
    "$$\n",
    "E_{p(k_i;q)}[k_ik_i^T] = \\sum_{i=1}^m p(k_i;q)k_ik_i^T = \\frac{\\sum_{i=1}^m \\exp(q^Tk_i)k_ik_i^T}{\\sum_{j=1}^m \\exp(q^Tk_j)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "E_{p(k_i;q)}[k_i] = \\sum_{i=1}^m p(k_i;q)k_i = \\frac{\\sum_{i=1}^m \\exp(q^Tk_i)k_i}{\\sum_{j=1}^m \\exp(q^Tk_j)}\n",
    "$$\n",
    "\n",
    "Therefore, we have:\n",
    "\n",
    "$$\n",
    "Cov_{p(k_i;q)}[k_i] = \\frac{\\sum_{i=1}^m \\exp(q^Tk_i)k_ik_i^T}{\\sum_{j=1}^m \\exp(q^Tk_j)} - \\frac{\\left(\\sum_{i=1}^m \\exp(q^Tk_i)k_i\\right)\\left(\\sum_{i=1}^m \\exp(q^Tk_i)k_i\\right)^T}{\\left(\\sum_{j=1}^m \\exp(q^Tk_j)\\right)^2}\n",
    "$$\n",
    "\n",
    "To simplify this expression, we can use the following identity:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial q}\\log\\left(\\sum_{j=1}^m \\exp(q^Tk_j)\\right) = \\frac{\\sum_{j=1}^m \\exp(q^Tk_j)k_j}{\\sum_{j=1}^m \\exp(q^Tk_j)}\n",
    "$$\n",
    "\n",
    "which can be verified by applying the chain rule and the quotient rule. Using this identity, we can rewrite the covariance as:\n",
    "\n",
    "$$\n",
    "Cov_{p(k_i;q)}[k_i] = E_{p(k_i;q)}[k_ik_i^T] - E_{p(k_i;q)}[k_ik_j]^T = E_{p(k;i;q)}[k_ik_j^T] - \\frac{\\partial}{\\partial q}\\log\\left(\\sum_{j=1}^m \\exp(q^Tk_j)\\right)\\frac{\\partial}{\\partial q}\\log\\left(\\sum_{j=1}^m \\exp(q^Tk_j)\\right)^T\n",
    "$$\n",
    "\n",
    "Now, we can use another identity:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2}{\\partial q^2}\\log\\left(\\sum_{j=1}^m \\exp(q^Tk_j)\\right) = E_{p(k_i;q)}[k_ik_j^T] - \\frac{\\partial}{\\partial q}\\log\\left(\\sum_{j=1}^m \\exp(q^Tk_j)\\right)\\frac{\\partial}{\\partial q}\\log\\left(\\sum_{j=1}^m \\exp(q^Tk_j)\\right)^T\n",
    "$$\n",
    "\n",
    "which can be verified by applying the product rule and the chain rule. Using this identity, we can further simplify the covariance as:\n",
    "\n",
    "$$\n",
    "Cov_{p(k_i;q)}[k_i] = \\frac{\\partial^2}{\\partial q^2}\\log\\left(\\sum_{j=1}^m \\exp(q^Tk_j)\\right)\n",
    "$$\n",
    "\n",
    "Finally, we can use the following identity:\n",
    "\n",
    "$$\n",
    "\\nabla_q\\left(\\sum_{i=1}^m (q^Tk_i)k_i\\right) = \\frac{\\partial}{\\partial q}\\log\\left(\\sum_{j=1}^m \\exp(q^Tk_j)\\right)\n",
    "$$\n",
    "\n",
    "which can be verified by applying the chain rule and the quotient rule. Using this identity, we can rewrite the gradient of the attention function as:\n",
    "\n",
    "$$\n",
    "\\nabla_qAttention(q,D) = 2\\sum_{i=1}^m (q^Tk_i)k_i = 2\\nabla_q\\left(\\sum_{i=1}^m (q^Tk_i)k_i\\right) = 2\\frac{\\partial}{\\partial q}\\log\\left(\\sum_{j=1}^m \\exp(q^Tk_j)\\right)\n",
    "$$\n",
    "\n",
    "Therefore, we have:\n",
    "\n",
    "$$\n",
    "\\nabla_qAttention(q,D) = 2Cov_{p(k_i;q)}[k_i]\n",
    "$$\n",
    "\n",
    "which proves the desired result.\n",
    "\n",
    "\n",
    "- (1) neural networks - What is the gradient of an attention unit .... https://ai.stackexchange.com/questions/25055/what-is-the-gradient-of-an-attention-unit.\n",
    "- (2) Attention (machine learning) - Wikipedia. https://en.wikipedia.org/wiki/Attention_%28machine_learning%29.\n",
    "- (3) Visual spatial attention - Wikipedia. https://en.wikipedia.org/wiki/Visual_spatial_attention.\n",
    "- (4) Accelerate Learning of Deep Hashing With Gradient Attention. https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Accelerate_Learning_of_Deep_Hashing_With_Gradient_Attention_ICCV_2019_paper.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2daef1-b015-4337-abb5-0e4e7a6377b0",
   "metadata": {},
   "source": [
    "# 3. Design a differentiable search engine using the attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cc415d-d4cb-411d-9c34-c048f015c7ad",
   "metadata": {},
   "source": [
    "A possible way to design a differentiable search engine using the attention mechanism is as follows:\n",
    "\n",
    "- First, we need to define a database of documents that can be searched by the engine. For simplicity, we can assume that each document is represented by a vector of features, such as word embeddings, TF-IDF scores, or topic models. We can also assume that the database is fixed and does not change over time.\n",
    "- Second, we need to define a query model that can generate a query vector from a natural language input. For example, we can use a recurrent neural network (RNN) or a transformer to encode the input into a fixed-length vector. Alternatively, we can use a keyword-based approach to extract the most relevant terms from the input and represent them as vectors.\n",
    "- Third, we need to define an attention function that can compute the similarity between the query vector and each document vector in the database. For example, we can use the scaled dot-product attention function¹ or the additive attention function². The attention function should output a vector of attention weights, where each weight corresponds to the relevance of a document to the query.\n",
    "- Fourth, we need to define an output model that can generate a ranked list of documents from the attention weights. For example, we can use a softmax function to normalize the weights and then sort them in descending order. Alternatively, we can use a differentiable sorting algorithm³ to directly optimize the ranking metric.\n",
    "- Finally, we need to define a loss function that can measure the performance of the search engine. For example, we can use the mean reciprocal rank (MRR) or the normalized discounted cumulative gain (NDCG) as the evaluation metrics. The loss function should be differentiable with respect to the query model, the attention function, and the output model parameters.\n",
    "\n",
    "By designing the search engine in this way, we can use gradient-based methods to optimize its components and learn from user feedback. This can potentially improve the accuracy and efficiency of the search engine and provide a better user experience. \n",
    "\n",
    "- (1) Att-DARTS: Differentiable Neural Architecture Search for Attention. https://ieeexplore.ieee.org/document/9207447.\n",
    "- (2) 11.1. Queries, Keys, and Values — Dive into Deep Learning 1 .... https://d2l.ai/chapter_attention-mechanisms-and-transformers/queries-keys-values.html.\n",
    "- (3) Differentiable Neural Architecture Search - arXiv.org. https://arxiv.org/pdf/2101.11342.\n",
    "- (4) undefined. https://ieeexplore.ieee.org/servlet/opac?punumber=9200848."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01df6fae-0da2-4440-9930-6aa26e03fec2",
   "metadata": {},
   "source": [
    "# 4. Review the design of the Squeeze and Excitation Networks (Hu et al., 2018) and interpret them through the lens of the attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16b5ba8-99cb-47b5-9ee7-d36b38561c0b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:d2l]",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
