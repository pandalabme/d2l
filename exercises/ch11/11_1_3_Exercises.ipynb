{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235c3e76-1045-435e-9a11-73d7d8e5a0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to markdown 10_7_10_Exercises.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2893f92-c73d-447f-a308-3fb523cf0229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import warnings\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "sys.path.append('/home/jovyan/work/d2l_solutions/notebooks/exercises/d2l_utils/')\n",
    "import d2l\n",
    "from torchsummary import summary\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da841372-d262-4054-ac86-f2ff38875b27",
   "metadata": {},
   "source": [
    "# 1. Suppose that you wanted to reimplement approximate (key, query) matches as used in classical databases, which attention function would you pick?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c4089d-bafb-4463-8a19-960ce46e35c3",
   "metadata": {},
   "source": [
    "One possible attention function that can be used to implement approximate (key, query) matches is the **scaled dot-product attention**¹. This function computes the similarity between the query and each key by taking the dot product and scaling it by the square root of the key dimension. Then, it applies a softmax function to obtain the attention weights, which are used to compute a weighted sum of the values. This function can handle queries and keys of different lengths, and it can learn to attend to the most relevant parts of the keys for each query. \n",
    "\n",
    "The scaled dot-product attention function can be defined as follows:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "where $Q$ is a matrix of queries, $K$ is a matrix of keys, $V$ is a matrix of values, and $d_k$ is the dimension of the keys.\n",
    "\n",
    "- (1) What exactly are keys, queries, and values in attention .... https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms.\n",
    "- (2) 11.1. Queries, Keys, and Values — Dive into Deep Learning 1 .... https://d2l.ai/chapter_attention-mechanisms-and-transformers/queries-keys-values.html.\n",
    "- (3) 注意力,多头注意力,自注意力及Pytorch实现 - 知乎. https://zhuanlan.zhihu.com/p/366592542.\n",
    "- (4) Alignment Attention by Matching Key and Query Distributions. https://arxiv.org/abs/2110.12567."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eed01c0-dbd3-48ef-b767-a62358e36992",
   "metadata": {},
   "source": [
    "# 2. Suppose that the attention function is given by $\\alpha(q,k_i)=q^Tk_i$ and that $k_i=v_i$ for $i=1,\\dots m,$. Denote by $p(k_i;q)$ the probability distribution over keys when using the softmax normalization in (11.1.3). Prove that $\\nabla_qAttention(q,D)=Cov_{p(k_i;q)}[k_i]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e86e57-55ee-4b68-bd05-9556d94238c6",
   "metadata": {},
   "source": [
    "First, let us recall the definition of the attention function and the softmax normalization from (11.1.3):\n",
    "\n",
    "$$\n",
    "\\text{Attention}(q,D) = \\sum_{i=1}^m v_ip(k_i;q)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(k_i;q) = \\frac{\\exp(\\alpha(q,k_i))}{\\sum_{j=1}^m \\exp(\\alpha(q,k_j))}\n",
    "$$\n",
    "\n",
    "where $q$ is the query vector, $D = \\{(k_1,v_1),\\dots,(k_m,v_m)\\}$ is the set of key-value pairs, and $\\alpha(q,k_i)$ is the similarity score between $q$ and $k_i$. we assume that $\\alpha(q,k_i)=q^Tk_i$\n",
    "$$\n",
    "\\nabla_qAttention(q,D) = \\nabla_q\\sum_{i=1}^m v_ip(k_i;q) = \\sum_{i=1}^m \\nabla_qp(k_i;q)k_i^T\n",
    "=\\sum_{i=1}^m \\nabla_q\\alpha(q,k_i)p(k_i;q)(1-p(k_i;q))k_i^T=\\sum_{i=1}^mk_ip(k_i;q)(1-p(k_i;q))k_i^T$$\n",
    "The covariance of $k_i$ under the probability distribution $p(k_i;q)$ is:\n",
    "\n",
    "$$\n",
    "Cov_{p(k_i;q)}[k_i] = E_{p(k_i;q)}(k_i - E_{p(k_i;q)}[k_i])^2=E_{p(k_i;q)}[(k_i - E_{p(k_i;q)}[k_i])(k_i - E_{p(k_i;q)}[k_i])^T]=E[(k_i-\\sum k_ip_i)(k_i-\\sum k_ip_i)^T]=E(k_i\\sum p_i^Tk_i^T - k_ik_i^T-\\sum k_ip_i\\sum p_i^Tk_i^T+\\sum k_ip_i*k_i^T)=\\sum k_ip_i\\sum p_i^Tk_i^T - \\sum p_ik_ik_i^T-\\sum k_ip_i\\sum p_i^Tk_i^T+\\sum k_ip_i\\sum p_i^Tk_i^T=\\sum_{i=1}^mk_ip(k_i;q)(1-p(k_i;q))k_i^T=\\nabla_qAttention(q,D)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2daef1-b015-4337-abb5-0e4e7a6377b0",
   "metadata": {},
   "source": [
    "# 3. Design a differentiable search engine using the attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cc415d-d4cb-411d-9c34-c048f015c7ad",
   "metadata": {},
   "source": [
    "A possible way to design a differentiable search engine using the attention mechanism is as follows:\n",
    "\n",
    "- First, we need to define a database of documents that can be searched by the engine. For simplicity, we can assume that each document is represented by a vector of features, such as word embeddings, TF-IDF scores, or topic models. We can also assume that the database is fixed and does not change over time.\n",
    "- Second, we need to define a query model that can generate a query vector from a natural language input. For example, we can use a recurrent neural network (RNN) or a transformer to encode the input into a fixed-length vector. Alternatively, we can use a keyword-based approach to extract the most relevant terms from the input and represent them as vectors.\n",
    "- Third, we need to define an attention function that can compute the similarity between the query vector and each document vector in the database. For example, we can use the scaled dot-product attention function¹ or the additive attention function². The attention function should output a vector of attention weights, where each weight corresponds to the relevance of a document to the query.\n",
    "- Fourth, we need to define an output model that can generate a ranked list of documents from the attention weights. For example, we can use a softmax function to normalize the weights and then sort them in descending order. Alternatively, we can use a differentiable sorting algorithm³ to directly optimize the ranking metric.\n",
    "- Finally, we need to define a loss function that can measure the performance of the search engine. For example, we can use the mean reciprocal rank (MRR) or the normalized discounted cumulative gain (NDCG) as the evaluation metrics. The loss function should be differentiable with respect to the query model, the attention function, and the output model parameters.\n",
    "\n",
    "By designing the search engine in this way, we can use gradient-based methods to optimize its components and learn from user feedback. This can potentially improve the accuracy and efficiency of the search engine and provide a better user experience. \n",
    "\n",
    "- (1) Att-DARTS: Differentiable Neural Architecture Search for Attention. https://ieeexplore.ieee.org/document/9207447.\n",
    "- (2) 11.1. Queries, Keys, and Values — Dive into Deep Learning 1 .... https://d2l.ai/chapter_attention-mechanisms-and-transformers/queries-keys-values.html.\n",
    "- (3) Differentiable Neural Architecture Search - arXiv.org. https://arxiv.org/pdf/2101.11342.\n",
    "- (4) undefined. https://ieeexplore.ieee.org/servlet/opac?punumber=9200848."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01df6fae-0da2-4440-9930-6aa26e03fec2",
   "metadata": {},
   "source": [
    "# 4. Review the design of the Squeeze and Excitation Networks (Hu et al., 2018) and interpret them through the lens of the attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16b5ba8-99cb-47b5-9ee7-d36b38561c0b",
   "metadata": {},
   "source": [
    "Squeeze and Excitation Networks (SENet) are a type of convolutional neural network that use a novel architectural unit called the Squeeze-and-Excitation (SE) block to enhance the representational power of the network by adaptively recalibrating channel-wise feature responses¹. The SE block can be interpreted as a form of attention mechanism that learns to assign different weights to different channels based on their relevance to the input.\n",
    "\n",
    "The SE block consists of two operations: squeeze and excitation. The squeeze operation applies global average pooling to the input feature map, which reduces the spatial dimension to 1 and outputs a channel descriptor vector. The channel descriptor vector captures the global information of the input and serves as a summary statistic of the input feature map. The excitation operation uses a gating mechanism to generate a set of per-channel modulation weights, which are used to rescale the input feature map. The gating mechanism consists of two fully-connected layers with a non-linearity (ReLU) and a sigmoid activation function, respectively. The sigmoid activation function ensures that the modulation weights are in the range of [0, 1], which can be interpreted as the importance or attention score of each channel.\n",
    "\n",
    "The SE block can be integrated into any existing convolutional neural network architecture by inserting it after each convolutional layer. The SE block does not change the dimension or structure of the input feature map, but only modifies its magnitude by multiplying it with the modulation weights. Therefore, the SE block is compatible with any network design and does not introduce any additional spatial or depth-wise parameters. The only parameters added by the SE block are from the fully-connected layers in the excitation operation, which are relatively small compared to the convolutional layers.\n",
    "\n",
    "The SE block can be seen as a way of enhancing the channel relationship within a convolutional neural network. By using global information to generate channel-wise weights, the SE block can learn to emphasize the most informative channels and suppress the less relevant ones for each input. This can improve the feature quality and diversity of the network, and thus boost its performance on various tasks. According to Hu et al.¹, SENet architectures achieved state-of-the-art results on several image classification benchmarks, such as ImageNet, CIFAR-10, and CIFAR-100. They also showed that SE blocks can improve the performance of existing networks, such as ResNet, Inception, and MobileNet, with minimal additional computational cost.\n",
    "\n",
    "\n",
    "- (1) [1709.01507] Squeeze-and-Excitation Networks - arXiv.org. https://arxiv.org/abs/1709.01507.\n",
    "- (2) Squeeze-and-Excitation Networks - IEEE Xplore. https://ieeexplore.ieee.org/document/8578843.\n",
    "- (3) Squeeze-and-Excitation Networks - CVF Open Access. https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf.\n",
    "- (4) undefined. https://doi.org/10.48550/arXiv.1709.01507.\n",
    "- (5) undefined. https://github.com/hujie-frank/SENet.\n",
    "- (6) undefined. https://ieeexplore.ieee.org/servlet/opac?punumber=8576498."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:d2l]",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
